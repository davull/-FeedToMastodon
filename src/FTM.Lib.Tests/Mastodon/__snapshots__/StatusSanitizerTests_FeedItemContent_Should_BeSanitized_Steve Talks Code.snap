[
  {
    "itemId": "https://stevetalkscode.co.uk/regex-source-generator",
    "raw": "<p>In this post, I describe the new source code generated RegEx that has been introduced with .NET 7</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>.NET 7 is now here, and one of the less talked about improvements is the sprinkling of source-code generation magic over the RegEx class along with other improvements to the functionality provided by RegEx.</p>\n<p>Stephen Toub has a very <a href=\"https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7/\" target=\"_blank\" class=\"externallink\">in depth blog</a> post that describes all the changes that have been made to RegEx in improve the overall performance of regular expression evaluations.</p>\n<p>There is a lot of great information in there, but in this post, I want to focus on the source generator experience.</p>\n<h2 id=\"introducing-the-generatedregexattribute\"><a name=\"Introducing-the-GeneratedRegexAttribute\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#Introducing-the-GeneratedRegexAttribute\">Introducing the GeneratedRegexAttribute</a></h2>\n<p>The approach to moving to a source generated RegEx takes the path that was also used for JSON serialisers in .NET by introducing the new <a href=\"https://learn.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.generatedregexattribute?view=net-7.0\" target=\"_blank\" class=\"externallink\">GeneratedRegex</a>\nattribute that is recognised by the code generator.</p>\n<p><em>(Note, that this attribute was renamed after initially being introduced in the early release candidates, so you may find some early blog posts refer to the older 'RegexGeneratorAttribute' name )</em></p>\n<p>This attribute is applied to a static partial method that returns a RegEx</p>\n<!--?# Gist f7c4fed325f9be06c8033facaea94353 /?-->\n<p>What is also a nice improvement in Visual Studio is that the pattern is recognised as a regular expression and a colour coding is applied to improve readability.</p>\n<!--?# LinkSizedImage \"RegExGenExampleColorCoding.png\" Alt=\"Screen shot XML comments seen in tool tip\" /?-->\n<p>This is thanks to the attribute's pattern parameter being decorated with <a href=\"https://learn.microsoft.com/en-us/dotnet/api/system.diagnostics.codeanalysis.stringsyntaxattribute.regex\" target=\"_blank\" class=\"externallink\">StringSyntaxAttribute.RegEx</a> which allows IDEs\nto apply syntax rules to displaying the string.</p>\n<p>The other parameters are optional, though I would recommend looking at the matchTimeoutMilliseconds if your pattern may be susceptible to a '<a href=\"https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS\" target=\"_blank\" class=\"externallink\">Regex Denial-of-Service</a>'</p>\n<p>In my example, I created a regular expression for matching whether a url presenting in a string is valid. The pattern I have used is not intended as an example of the best way to do this, but is provided as an example of a reasonably complex pattern to\nput the source generator through its paces.</p>\n<p>If I wanted to expose all the functionality of the generated RegEx instance, I could make the method public. However, in most cases, the use of the regular expression is part of some other specific functionality, so I like to make the generated RegEx private\nand expose a method that calls the RegEx's Match method, thus hiding other functionality such as Replace. Also in the example, the outer class MyUrlValidator is static, but this is not a requirement of using the source generated RegEx and it can happily live in a non-static class.</p>\n<p>What IS a requiment for using the generated RegEx is that both the class and the method that is decorated with the GeneratedRegexAttribute are both declared as partial.\nThis is a key factor in using source code generators as the generator will add code to the class and the method during the Rosyln compilation process.</p>\n<h2 id=\"performance\"><a name=\"Performance\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#Performance\">Performance</a></h2>\n<p>Since the first release of .NET, the RegEx class has used an interpreter engine to perform pattern matching. However, before the interpreter engine can be used, the pattern must be validated and converted into code that will be used by the engine to do the matching.\nEach time you create a new instance of the RegEx, it takes time to create this code, so the best practice is usually to create it once and cache it in a static thread-safe variable.</p>\n<p>In previous versions of .NET, there has been the option to use the RegexOptions.Compiled option to utilise the MSIL generated as raw code instead of using the interpreter engine. However, this adds an initial overhead to startup time, so if the pattern matching is not\nin an application's hot-path, there may be minimal benefit to its use. In .NET Framework and .NET 6, there has also been the option to <a href=\"https://learn.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.compiletoassembly?view=net-7.0\" target=\"_blank\" class=\"externallink\">compile to an assembly</a>, but this has a number of caveats to its use.</p>\n<p>With the source generated RegEx, the interpreter is no longer needed as the generated code creates a custom class that inherits from RegEx that is cached as a thread-safe singleton. So, with the new source generated version, you gain all the performance benefits\nthat you previously had to use the compile options for, without the associated drawbacks.</p>\n<p>However, when running benchmarks, the generated RegEx does not necessarily have better or directly comparable performance with the compiled version, so you will need to perform your own benchmarks.\nIn my tests of the URL validator, the source generated version is marginally slower, but we are talking a few nanoseconds. In the screenshot below, benchmarking over one million iterations shows a few milliseconds difference.</p>\n<!--?# LinkSizedImage \"RegExGenBenchmark.png\" Alt=\"Screen shot showing benchmark results.\" /?-->\n<p>I would put the caveat on this that my benchmarking approach may not be correct and may need some refinement as <a href=\"https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7/#source-generation\" target=\"_blank\" class=\"externallink\">the section about source generation in Stephen Toub's post</a>\nindicates that performance <em>should</em> be better. The code can be found at <a href=\"https://github.com/stevetalkscode/sourcegenregex\" target=\"_blank\" class=\"externallink\">https://github.com/stevetalkscode/sourcegenregex</a></p>\n<p>However, whilst performance is important, there are other advantages to using the source generated version.</p>\n<h2 id=\"other-advantages\"><a name=\"Other-Advantages\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#Other-Advantages\">Other Advantages</a></h2>\n<p>Performance is not the only advantage of using the new source generated RegEx. There are several other advantages to using the new source code generated version of the regular expression.</p>\n<p>The first is that you can see the source code that performs the pattern matching within Visual Studio. Remember, this is not MSIL (as is generated by the older compile option), but is C# that is much more readable.</p>\n<!--?# LinkSizedImage \"RegExGenViewCode.png\" Alt=\"Screen shot viewing the generated source code.\" /?-->\n<p>The next benefit is the XML comment that is generated for the class that explains how the pattern matching will work as a step-by-step instruction in English and can be seen as a tooltip in an IDE.</p>\n<!--?# LinkSizedImage \"RegExGenXmlComments.png\" Alt=\"Screen shot XML comments seen in tool tip\" /?-->\n<p>A Rosyln analyser is now also provided to help you identify where existing RegEx instances can be replaced with a source generated alternative.</p>\n<!--?# LinkSizedImage \"RegExGenAnalyserWarning.png\" Alt=\"Screen shot of analyser warning that RegEx can be converted to a source generated alternative.\" /?-->\n<p>Not only will it let you know about the change to refactor, but also provides a fix to do the refactoring for you.</p>\n<!--?# LinkSizedImage \"RegExGenAnalyserConvert.png\" Alt=\"Screen shot of analyser fix for converting RegEx to a source generated alternative.\" /?-->\n<h2 id=\"conclusion\"><a name=\"Conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#Conclusion\">Conclusion</a></h2>\n<p>I have been a big fan of source generators since their introduction in .NET 5 and it is good to see Microsoft continuing to bring their benefits into the base library.</p>\n<p>As mentioned at the start of this post, Stephen Toub has a very <a href=\"https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7/\" target=\"_blank\" class=\"externallink\">in depth blog</a> post goes into a lot more detail that is worth a read\nif you want to find out more.</p>\n",
    "sanitized": "In this post, I describe the new source code generated RegEx that has been introduced with .NET 7\nBackground\n.NET 7 is now here, and one of the less talked about improvements is the sprinkling of source-code generation magic over the RegEx class along with other improvements to the functionality provided by RegEx.\nStephen Toub has a very in depth blog post that describes all the changes that have been made to RegEx in improve the overall performance of regular expression evaluations.\nThere is a lot of great information in there, but in this post, I want to focus on the source generator experience.\nIntroducing the GeneratedRegexAttribute\nThe approach to moving to a source generated RegEx takes the path that was also used for JSON serialisers in .NET by introducing the new GeneratedRegex\nattribute that is recognised by the code generator.\n(Note, that this attribute was renamed after initially being introduced in the early release candidates, so you may find some early blog posts refer to the older 'RegexGeneratorAttribute' name )\nThis attribute is applied to a static partial method that returns a RegEx\n\nWhat is also a nice improvement in Visual Studio is that the pattern is recognised as a regular expression and a colour coding is applied to improve readability.\n\nThis is thanks to the attribute's pattern parameter being decorated with StringSyntaxAttribute.RegEx which allows IDEs\nto apply syntax rules to displaying the string.\nThe other parameters are optional, though I would recommend looking at the matchTimeoutMilliseconds if your pattern may be susceptible to a 'Regex Denial-of-Service'\nIn my example, I created a regular expression for matching whether a url presenting in a string is valid. The pattern I have used is not intended as an example of the best way to do this, but is provided as an example of a reasonably complex pattern to\nput the source generator through its paces.\nIf I wanted to expose all the functionality of the generated RegEx instance, I could make the method public. However, in most cases, the use of the regular expression is part of some other specific functionality, so I like to make the generated RegEx private\nand expose a method that calls the RegEx's Match method, thus hiding other functionality such as Replace. Also in the example, the outer class MyUrlValidator is static, but this is not a requirement of using the source generated RegEx and it can happily live in a non-static class.\nWhat IS a requiment for using the generated RegEx is that both the class and the method that is decorated with the GeneratedRegexAttribute are both declared as partial.\nThis is a key factor in using source code generators as the generator will add code to the class and the method during the Rosyln compilation process.\nPerformance\nSince the first release of .NET, the RegEx class has used an interpreter engine to perform pattern matching. However, before the interpreter engine can be used, the pattern must be validated and converted into code that will be used by the engine to do the matching.\nEach time you create a new instance of the RegEx, it takes time to create this code, so the best practice is usually to create it once and cache it in a static thread-safe variable.\nIn previous versions of .NET, there has been the option to use the RegexOptions.Compiled option to utilise the MSIL generated as raw code instead of using the interpreter engine. However, this adds an initial overhead to startup time, so if the pattern matching is not\nin an application's hot-path, there may be minimal benefit to its use. In .NET Framework and .NET 6, there has also been the option to compile to an assembly, but this has a number of caveats to its use.\nWith the source generated RegEx, the interpreter is no longer needed as the generated code creates a custom class that inherits from RegEx that is cached as a thread-safe singleton. So, with the new source generated version, you gain all the performance benefits\nthat you previously had to use the compile options for, without the associated drawbacks.\nHowever, when running benchmarks, the generated RegEx does not necessarily have better or directly comparable performance with the compiled version, so you will need to perform your own benchmarks.\nIn my tests of the URL validator, the source generated version is marginally slower, but we are talking a few nanoseconds. In the screenshot below, benchmarking over one million iterations shows a few milliseconds difference.\n\nI would put the caveat on this that my benchmarking approach may not be correct and may need some refinement as the section about source generation in Stephen Toub's post\nindicates that performance should be better. The code can be found at https://github.com/stevetalkscode/sourcegenregex\nHowever, whilst performance is important, there are other advantages to using the source generated version.\nOther Advantages\nPerformance is not the only advantage of using the new source generated RegEx. There are several other advantages to using the new source code generated version of the regular expression.\nThe first is that you can see the source code that performs the pattern matching within Visual Studio. Remember, this is not MSIL (as is generated by the older compile option), but is C# that is much more readable.\n\nThe next benefit is the XML comment that is generated for the class that explains how the pattern matching will work as a step-by-step instruction in English and can be seen as a tooltip in an IDE.\n\nA Rosyln analyser is now also provided to help you identify where existing RegEx instances can be replaced with a source generated alternative.\n\nNot only will it let you know about the change to refactor, but also provides a fix to do the refactoring for you.\n\nConclusion\nI have been a big fan of source generators since their introduction in .NET 5 and it is good to see Microsoft continuing to bring their benefits into the base library.\nAs mentioned at the start of this post, Stephen Toub has a very in depth blog post goes into a lot more detail that is worth a read\nif you want to find out more."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/speccyimageloader",
    "raw": "<p>In this post, I divert from my usual .NET posts and have a bit of retro fun showing how I recreated a ZX Spectrum loading screen for my Source Generation Game talk</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>Back in October 2021, I presented a new talk at DDD East Midlands about my journey with source code generators leading up to the introduction of .NET\nsource code generators being introduced with .NET 5.</p>\n<p>In the months since then, I have refined the talk to introduce the enhancements in .NET 6 and also the potential dangers of using generators found on NuGet (as highlighted\nin my blog post <a href=\"https://stevetalkscode.co.uk/sourcegeneratorattacks\" class=\"externallink\">Could .NET Source Generator Attacks Be A Danger To Your Code?</a>).</p>\n<p>The talk starts with the ZX Spectrum, my first computer where I wrote <a href=\"https://worldofspectrum.org/archive/magazines/your-sinclair/42/0/1989/6/0#80\" target=\"_blank\" class=\"externallink\">my first code generator</a> and moves through the VB years and then onto .NET with the many different ways\nthat we have used various tools and libraries to generate code.</p>\n<p>The name of the talk \"Steve Collins and the [Source Code] Generation Game\" is a play on the name of a popular BBC TV show <a href=\"https://en.wikipedia.org/wiki/The_Generation_Game\" target=\"_blank\" class=\"externallink\">'The Generation Game'</a> that ran\nfrom the early 1970s that continued on and off into the early 2000s.</p>\n<p>I thought it would be a bit of fun to have a logo for the opening slide that would be heavily influenced by one of the logos used by the TV programme in its opening titles.\nThe one that I felt was suitably \"retro\" was from the title sequence <a href=\"https://www.ravensbourne.ac.uk/bbc-motion-graphics-archive/generation-game-1974\" target=\"_blank\" class=\"externallink\">in this video</a></p>\n<div style=\"display: flex\">\n    <!--?# LinkSizedImage \"brucegengame.png\" Alt=\"Logo from 1970's The Generation Game\"/?--> \n    <!--?# LinkSizedImage \"stevegengame.png\" Alt=\"Logo from The Source Code Generation Game\"/?-->\n</div>\n<p>Whilst I could not find a matching font, I think that the final result captured the feel of the original but with a slightly more '80s vibe than the original early '70s one.</p>\n<h2 id=\"speccyfying-the-image\"><a name=\"addingspeccyloader\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#addingspeccyloader\">\"Speccyfying\" The Image</a></h2>\n<p>In preparing for the talk's latest outing at DDD South West, and given that the start of the talk is\nabout writing a source code generator on the ZX Spectrum, I thought that it might\nbe fun to emulate the logo loading on a Spectrum, the results can be seen here.</p>\n<!--?# YouTubeDiv b7k8_k2wKI4 \"Introduction to Source Generators\" /?-->\n<p>Some people have expressed an interest in how I created this, so I have written this blog as a step by step guide.</p>\n<h3 id=\"step-1-converting-the-image\"><a name=\"convertimage\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#convertimage\">Step 1 - Converting The Image</a></h3>\n<p>The first step is to convert the image into a ZX Spectrum image format.</p>\n<p>To do this, I used the <a href=\"http://www.fruitcake.plus.com/Sinclair/Spectrum/Spectra/SpectraInterface_Software_ImageConverter.htm\" target=\"_blank\" class=\"externallink\">Spectra Image Converter</a>.</p>\n<p>This application can take images in various formats and do the appropriate conversion. Whilst it supports many output resolutions, for the true original 48K Spectrum experience with colour-clash, I set it up using the following settings</p>\n<ul>\n<li>Attribute Size : 8 x 8</li>\n<li>Colours : 8 (though you may want to experiment with 15 to include 'Bright' colours)</li>\n<li>Encoding Size : Single Byte</li>\n<li>Output File Format : Tape</li>\n</ul>\n<p>In the Spectra application there is an option to create a 'Display Program' but this does not achive what I was expecting (as it loads the image into memory at address 32768, not the screen address of 16384), so I have left this option unchecked in the user interface.</p>\n<p><em>(For a detailed explanation of the ZX Spectrum pixel and attribute layout, there is a great explanation at <a href=\"http://www.overtakenbyevents.com/lets-talk-about-the-zx-specrum-screen-layout/\" target=\"_blank\" class=\"externallink\">David Black's Overtaken by Events</a> site.)</em></p>\n<!--?# LinkSizedImage \"spectraconvert.png\" Alt=\"Using the Spectra Image Convert\"/?--> \n<p>You may want to refine your original image and play around with this until you get a final image that you are happy with.</p>\n<p>Once you have the image ready, it is time to save the output as a TAP file.</p>\n<h3 id=\"step-2-prepare-the-fuse-emulator\"><a name=\"preparefuse\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#preparefuse\">Step 2 - Prepare the Fuse Emulator</a></h3>\n<p>With a TAP file ready, the next step is to load the image using a ZX Spectrum emulator.</p>\n<p>Various emulators are available, but the one that I have been using is the <a href=\"http://fuse-emulator.sourceforge.net/\" target=\"_blank\" class=\"externallink\">Fuse Emulator</a>.</p>\n<p>The Fuse emulator supports options for fast loading from a virtual tape, but for what we want to achive, we want to turn these options off, as shown in the screen below (Accessed via the menus: Options &gt; Media)</p>\n<div style=\"display: flex; justify-content: center;\">\n<!--?# LinkSizedImage \"fusemediaoptions.png\" Size=\"50%\" Alt=\"Setting Media Options in Fuse Emulator\"/?--> \n</div>\n<h3 id=\"step-3-write-the-loader-program\"><a name=\"writeloader\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#writeloader\">Step 3 - Write the Loader Program</a></h3>\n<p>With this done, we need to write a short program to load the image into the screen memory.</p>\n<!--?# Gist 8ae440ba9bf7b9a8f6487359705e7b0b /?-->\n<p>Have created a TAP file of this program that can be loaded into FUSE <a href=\"https://github.com/stevetalkscode/zxloader/blob/main/screenloader.tap\" target=\"_blank\" class=\"externallink\">on GitHub</a></p>\n<p>In the program shown above, I have used a white background and 'paper' (colour 7) with black 'ink' (colour 0) before the screen starts loading and the set a black border a the end. Depending on the palette of your image you may want to use different colours and the BRIGHT command to suit your needs.</p>\n<h3 id=\"step-4-run-the-loader-program\"><a name=\"runtheloader\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#runtheloader\">Step 4 - Run the Loader Program</a></h3>\n<p>With the program entered (or loaded from the TAP file), we can now get ready to record the loader. To do this, I used Camtasia to record the FUSE screen.</p>\n<p>In FUSE, use the Media menu to navigate to the Tape option, them the Open option (or press F7) to select the TAP file that you saved from Spectra.</p>\n<p>The enter the RUN command on the Spectrum being emulated.</p>\n<div style=\"display: flex; justify-content: center;\">\n<!--?# LinkSizedImage \"runspeccyloader.png\" Size=\"75%\" Alt=\"RUN the loader program and press PLAY\"/?--> \n</div>\n<p>This will not actually do anything yet as it is waiting for you to press PLAY on the virtual tape deck.</p>\n<p>If you have not already started your screen recorder to capture the FUSE screen, do so now</p>\n<p>To play the virtual tape, go back to the Tape option on the Media menu and click on Play (or press F8 if you do not want to have to edit the menu selections from your screen recording).</p>\n<p>At this point, assuming you have set the Media options not to use fast loading, you will hear the unforgetable tones of the tape header followed by the squeek of data to show the program name you save the image as. This is then followed by the image loading one line at a time and finally the colour attributes at the end.</p>\n<h2 id=\"have-a-play\">Have a Play!</h2>\n<p>If you have a hankering to re-live those long forgotton 8-bit days when loading screens were provided to relieve the boredom of waiting serveral minutes for a game to load, why not have a go at recreating your own loader screen. Hopefully the process I have described above will be of use to you.</p>\n",
    "sanitized": "In this post, I divert from my usual .NET posts and have a bit of retro fun showing how I recreated a ZX Spectrum loading screen for my Source Generation Game talk\nBackground\nBack in October 2021, I presented a new talk at DDD East Midlands about my journey with source code generators leading up to the introduction of .NET\nsource code generators being introduced with .NET 5.\nIn the months since then, I have refined the talk to introduce the enhancements in .NET 6 and also the potential dangers of using generators found on NuGet (as highlighted\nin my blog post Could .NET Source Generator Attacks Be A Danger To Your Code?).\nThe talk starts with the ZX Spectrum, my first computer where I wrote my first code generator and moves through the VB years and then onto .NET with the many different ways\nthat we have used various tools and libraries to generate code.\nThe name of the talk \"Steve Collins and the [Source Code] Generation Game\" is a play on the name of a popular BBC TV show 'The Generation Game' that ran\nfrom the early 1970s that continued on and off into the early 2000s.\nI thought it would be a bit of fun to have a logo for the opening slide that would be heavily influenced by one of the logos used by the TV programme in its opening titles.\nThe one that I felt was suitably \"retro\" was from the title sequence in this video\n\n     \n    \n\nWhilst I could not find a matching font, I think that the final result captured the feel of the original but with a slightly more '80s vibe than the original early '70s one.\n\"Speccyfying\" The Image\nIn preparing for the talk's latest outing at DDD South West, and given that the start of the talk is\nabout writing a source code generator on the ZX Spectrum, I thought that it might\nbe fun to emulate the logo loading on a Spectrum, the results can be seen here.\n\nSome people have expressed an interest in how I created this, so I have written this blog as a step by step guide.\nStep 1 - Converting The Image\nThe first step is to convert the image into a ZX Spectrum image format.\nTo do this, I used the Spectra Image Converter.\nThis application can take images in various formats and do the appropriate conversion. Whilst it supports many output resolutions, for the true original 48K Spectrum experience with colour-clash, I set it up using the following settings\n\nAttribute Size : 8 x 8\nColours : 8 (though you may want to experiment with 15 to include 'Bright' colours)\nEncoding Size : Single Byte\nOutput File Format : Tape\n\nIn the Spectra application there is an option to create a 'Display Program' but this does not achive what I was expecting (as it loads the image into memory at address 32768, not the screen address of 16384), so I have left this option unchecked in the user interface.\n(For a detailed explanation of the ZX Spectrum pixel and attribute layout, there is a great explanation at David Black's Overtaken by Events site.)\n \nYou may want to refine your original image and play around with this until you get a final image that you are happy with.\nOnce you have the image ready, it is time to save the output as a TAP file.\nStep 2 - Prepare the Fuse Emulator\nWith a TAP file ready, the next step is to load the image using a ZX Spectrum emulator.\nVarious emulators are available, but the one that I have been using is the Fuse Emulator.\nThe Fuse emulator supports options for fast loading from a virtual tape, but for what we want to achive, we want to turn these options off, as shown in the screen below (Accessed via the menus: Options > Media)\n\n \n\nStep 3 - Write the Loader Program\nWith this done, we need to write a short program to load the image into the screen memory.\n\nHave created a TAP file of this program that can be loaded into FUSE on GitHub\nIn the program shown above, I have used a white background and 'paper' (colour 7) with black 'ink' (colour 0) before the screen starts loading and the set a black border a the end. Depending on the palette of your image you may want to use different colours and the BRIGHT command to suit your needs.\nStep 4 - Run the Loader Program\nWith the program entered (or loaded from the TAP file), we can now get ready to record the loader. To do this, I used Camtasia to record the FUSE screen.\nIn FUSE, use the Media menu to navigate to the Tape option, them the Open option (or press F7) to select the TAP file that you saved from Spectra.\nThe enter the RUN command on the Spectrum being emulated.\n\n \n\nThis will not actually do anything yet as it is waiting for you to press PLAY on the virtual tape deck.\nIf you have not already started your screen recorder to capture the FUSE screen, do so now\nTo play the virtual tape, go back to the Tape option on the Media menu and click on Play (or press F8 if you do not want to have to edit the menu selections from your screen recording).\nAt this point, assuming you have set the Media options not to use fast loading, you will hear the unforgetable tones of the tape header followed by the squeek of data to show the program name you save the image as. This is then followed by the image loading one line at a time and finally the colour attributes at the end.\nHave a Play!\nIf you have a hankering to re-live those long forgotton 8-bit days when loading screens were provided to relieve the boredom of waiting serveral minutes for a game to load, why not have a go at recreating your own loader screen. Hopefully the process I have described above will be of use to you."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/sourcegeneratorattacks",
    "raw": "<div class=\"hide-on-print\">\n\t<!--?# LinkSizedImage \"Do_You_TrustSource_Generators.png\" Alt=\"Banner with message 'Do you trust externally written C Sharp Source Generators?'\"/?--> \n</div>\n<p>In this post, I highlight the potential dangers of trusting third party .NET Source Generators and show ways to try and spot Supply Chain Attacks trying to inject malicious code into your code base.</p>\n<div style=\"display:block; background-color:#F0F0F0; border: 1px solid black;padding:0.6em;\">\n<p>This post is part of <a href=\"https://dusted.codes/\">Dustin Moris Gorski's</a> .NET Advent Calendar. Go check out the other posts at <a href=\"https://dotnet.christmas/\" target=\"_blank\" class=\"externallink\">https://dotnet.christmas/</a></p>\n</div>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>Since they were introduced in .NET 5, I have been a big fan of C# source&nbsp; code generators. They are a powerful tool that (a) help avoid the need to write a lot of boiler plate code and (b) also help improve performance of your code by allowing for code to be generated at compile time for tasks that you would previously may have resorted to using reflection to perform.</p>\n<p>If you are not familiar with source code generators, there are lots of places you can find out about them. Here are a few presentations on You Tube that you may find useful</p>\n<p><strong>Introduction to Source Generators</strong></p>\n<!--?# YouTubeDiv cB66gOHConw \"Introduction to Source Generators\" /?-->\n<p><strong>Using Source Generators for Fun (and Maybe Profit)</strong></p>\n<!--?# YouTubeDiv 4DVV7FXukC8 \"Using Source Generators for Fun (and Maybe Profit)\" /?-->\n<p><strong>Exploring Source Generators</strong></p>\n<!--?# YouTubeDiv Rgdj8X6i9sA \"Exploring Source Generators\" /?-->\n<p>These videos provide a lot of information as to why source generators are a great addition to .NET.&nbsp; However ...</p>\n<h2 id=\"with-great-power-comes-great-responsibility\"><a name=\"great-power-responsibility\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#great-power-responsibility\">With Great Power Comes Great Responsibility</a></h2>\n<p>I recently became aware of the potential dangers of using source code generators after reading two excellent blog posts.</p>\n<p>The first is&nbsp;<a href=\"https://www.veracode.com/blog/secure-development/net-5-source-generators-and-supply-chain-attacks\" target=\"_blank\" class=\"externallink\">Mateusz Krzeszowiec's VeraCode Blog</a>&nbsp;which provides an overview of supply chain attacks and how source code generators can be used to generate potentially harmful code that gets baked into your software</p>\n<p>The second is &nbsp;<a href=\"https://blog.maartenballiauw.be/post/2021/05/05/building-a-supply-chain-attack-with-dotnet-nuget-dns-source-generators-and-more.html\" target=\"_blank\" class=\"externallink\">Maarten Balliauw 's Blog Post</a> that also describes the problem, but also shows how attributes can be used to try and hide the code from being inspected.</p>\n<p>I highly recommend going and reading both of these before continuing, but if you want the TL;DR, here is my take on what these two blogs eloquently describe in great detail.</p>\n<h2 id=\"supply-chain-attacks\"><a name=\"supply-chain-attacks\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#supply-chain-attacks\">Supply Chain Attacks</a></h2>\n<p>As mentioned in these blogs, a <a href=\"https://en.wikipedia.org/wiki/Supply_chain_attack\" target=\"_blank\" class=\"externallink\">supply chain attack</a> uses what appears to be a harmless component or build process to generate code that is added as part of the Continuous Integration build pipeline. This injected code can later be used to scrape data from users of that software. The most notable recent attack was the <a href=\"https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach#SolarWinds_exploit_2\">SolarWinds exploit</a>.</p>\n<p>With source generators, you are potentially open to attack on two fronts.</p>\n<p>The first is that the generator has the ability to inspect your code via the syntax tree and/or the semantic tree. Whilst unlikely, there is the risk that the analysis stage could keep a lookout for common coding patterns where usernames, passwords, connection details etc. could be scraped and logged.</p>\n<p>However, the second is the more dangerous and that is to generate malicious code that 'dials home' information from users of your software. Maarten's blog shows how this is done and what to look for in the generated code.</p>\n<h2 id=\"defending-yourself-against-attacks\"><a name=\"defending-yourself-against-attacks\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#defending-yourself-against-attacks\">Defending Yourself Against Attacks</a></h2>\n<p>The NuGet team has a <a href=\"https://docs.microsoft.com/en-us/nuget/concepts/security-best-practices#knowing-what-is-in-your-environment\" target=\"_blank\" class=\"externallink\">page</a> that describes what to look for when assessing whether to use a NuGet package in general. However, given the risk of a rogue source generator creating malicious code, the packages need closer scrutiny.</p>\n<p>In summary, here are a few things to consider before using a source generator from NuGet</p>\n<ul>\n<li>Is the package from a reputable source? If the package is from a commercial vendor, this is fairly easy to verify. However, for open source projects, this is a bit harder, which leads to the next check</li>\n<li>Is the source code available? If the package is open source, the source code will usually be available on GitHub. For source generators, it is always worth looking over the source code and checking what the generator is doing.</li>\n<li>If the source code is not available, it may be worth considering using a tool such as <a href=\"https://www.jetbrains.com/decompiler/\" target=\"_blank\" class=\"externallink\">JetBrains DotPeek</a> to decompile the code. Two things to consider with this, though. The first is that this may be a breach of the license terms, so you need to tread carefully. The second is that the code may have been obfuscated, so will be harder to read and work out what it is doing.</li>\n<li>For open source projects, can you be sure that the NuGet package published has been built from the code in the GitHub repository. Subject to licensing, you may want to consider forking the repo and creating your own build and manage within your own NuGet feed</li>\n<li>Consider using tools to vet packages or static source code analysers to check on generated code.</li>\n</ul>\n<h2 id=\"ensuring-generated-code-is-visible\"><a name=\"ensuring-generated-code-is-visible\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#ensuring-generated-code-is-visible\">Ensuring Generated Code Is Visible</a></h2>\n<p>Whilst ideally, you will have done the due diligence described above, one of the safeguards you can put in place is to make some changes to your csproj file to ensure that the generated code is visible, so that it can be viewed and tracked in source control.</p>\n<h3 id=\"writing-compiler-generated-code-into-files\"><a name=\"writing-compiler-generated-code-into-files\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#writing-compiler-generated-code-into-files\">Writing Compiler Generated Code into Files</a></h3>\n<p>By default, generated source code does not get written to files that you can see as the code is part of the Roslyn compile process. This makes it easy for bad actors providing code generators to hide under the radar.</p>\n<p>To address this, there are two properties you can add to your project file that will make the generated code visible and trackable in source control</p>\n<!--?# LinkSizedImage \"ProjectFileForVisibleFiles.png\" Alt=\"Image showing the project file before editing\" /?--> \n<p>The first is the <span class=\"code-item\">EmitCompilerGeneratedFiles</span> property. This will write out any generated code files to the file system. It should be noted, that the contents of these files are effectively just a log of what has been added to the compilation process. The files themselves do not get used as part of the compilation.</p>\n<p>By default, these files with be written to the obj folder in a path structure of</p>\n<p>obj\\BuildProfile\\Platform\\generated\\generator assembly\\generator namespace+name</p>\n<p>Within this path there will be one or many files depending on how the source code generator has split the code into virtual files using the 'hintname' parameter to the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.codeanalysis.generatorexecutioncontext.addsource#Microsoft_CodeAnalysis_GeneratorExecutionContext_AddSource_System_String_System_String_\" target=\"_blank\" class=\"externallink\">AddSource method</a> on the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.codeanalysis.isourcegenerator.execute\" target=\"_blank\" class=\"externallink\">GeneratorExecutionContext at the end of the generator's Execute method</a>.</p>\n<p>Whilst this gives some visibility, having the files in the obj folder is not really of help as this is usually excluded from source control.</p>\n<p>This is where the second project property comes into play.</p>\n<p>The <span class=\"code-item\">CompilerGeneratedFilesOutputPath</span> property allows you to specify a path. This will usually be relative to the consuming project's path. I typically use a path of 'GeneratedFiles'.</p>\n<p>Beneath this path, the structure is <span class=\"code-item\">\\generator assembly\\generator namespace+name</span>, again with one or more files depending on how the generated divides the generated code.</p>\n<p>As with the files within the obj folder, these files do not take part in the actual compilation. However, this is where things get a bit tricky as by default, any *.cs files within the project folder structure get included. This causes the compiler to generate a whole load of errors stating that there is duplicate definitions of code in the files that have been output.</p>\n<p>To get around this, an extra section needs to be added to the project file.</p>\n<!--?# LinkSizedImage \"ProjectFileForVisibleFiles2.png\" Alt=\"Image showing the use of the Remove element within the Compile Element in the project file/?--> \n<p>Adding the this section excludes the generated files from compilation as they are there purely so that we can see the output in source control (as the actual generated code is already in the compilation pipeline in memory).</p>\n<pre><code>&lt;ItemGroup&gt; \n   &lt;Compile Remove=\"$(CompilerGeneratedFilesOutputPath)\\**\" /&gt; \n&lt;/ItemGroup&gt;\n</code></pre>\n<p>Now that we have the files in the project file structure, it can be added to source control and any changes tracked. This then means that the files form part of the code review process and can be checked for anything 'dodgy' going on in the code</p>\n<h3 id=\"view-generated-files-in-visual-studio\"><a name=\"view-generated-files-in-vs\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#view-generated-files-in-vs\">View Generated Files in Visual Studio</a></h3>\n<p>Since VS2019 16.10 and now in VS2022, the Solution Explorer window can now drill down into generated files by expanding each source code generator listed under the <em>Analyzers</em> node in the Solution Explorer window.</p>\n<hr>\n<p>You may be interested in my previous <a href=\"https://stevetalkscode.co.uk/debug-source-generators-with-vs2019-1610\">blog post</a> where I show how to set up source generator debugging in Visual Studio 2019. Whilst a couple of screens differ in VS2022 (due to the new debugging profile window), the guide works as well</p>\n<hr>\n<p>This feature does not require the <span class=\"code-item\">EmitCompilerGeneratedFiles</span> to be enabled as this uses the in-memory compiler generated code (which *should* be the same as the emitted files.</p>\n<p>In the screen shot below, I enabled the 'Show All Files' button and expanded out both the compiler generated output in the <em>Analyzers</em> node and also the emitted output files in the <em>GeneratedFiles</em> folder</p>\n<!--?# LinkSizedImage \"VS_Source_Gen_Explorer.png\" Alt=\"VS Solution Explorer showing generated files/?--> \n<p>One thing to also consider is that a sneaky attack may generate different code depending on whether the build is using a Debug or Release profile (as developers usually build locally in debug mode, whilst the CI build engine will be using release)</p>\n<h2 id=\"demo-code\"><a name=\"demo-code\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#demo-code\">Demo Code</a></h2>\n<p>I have made the solution shown in the above image available in a GitHub repo at <a href=\"https://github.com/stevetalkscode/sourcegeneratorattacks\" target=\"_blank\" class=\"externallink\">https:<wbr>//github.com<wbr>/stevetalkscode<wbr>/sourcegeneratorattacks</a></p>\n<p>The solution illustrates the techniques described above using two source code generators.</p>\n<p>The first is a simple generator that creates a class that has some of the danger signs to look out for based on Marten's blog.</p>\n<p>The second is a demonstration of the <a href=\"https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/\" target=\"_blank\" class=\"externallink\">new System.Text.Json code generation</a> that has been introduced with .NET 6 for generating serialisation and deserialisation code that would previously have been handled by reflection (and is still the default behaviour unless explicitly used as shown in the demo).</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>The above discussion may put you off using source generators. This should not be the case as this feature of .NET is incredibly powerful and is starting to be used by Microsoft in .NET 6 with the improvements made to <a href=\"https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/\" target=\"_blank\" class=\"externallink\">Json serialisation</a>, <a href=\"https://docs.microsoft.com/en-us/aspnet/core/release-notes/aspnetcore-6.0?view=aspnetcore-6.0#razor-compiler-updated-to-use-source-generators\" target=\"_blank\" class=\"externallink\">razor page compilation</a> and <a href=\"https://docs.microsoft.com/en-us/dotnet/core/extensions/logger-message-generator\" target=\"_blank\" class=\"externallink\">logging</a>.</p>\n<p>But if you are using source generators that you (or your team) have not written, you need to be aware of the potential dangers of not verifying where the generator has come from and what it is doing to your code.</p>\n<p>\"Let's be careful out there!\"</p>\n<div class=\"hide-on-print\">\n<hr>\n<h2 id=\"while-you-are-here\"><a name=\"dddempromo\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#dddempromo\">While You Are Here ...</a></h2>\n<p>If you have enjoyed this blog post, you may be interested in my talk that I presented at DDD East Midlands where I discuss my experiences with source code generation of various shades over the past 30 years.</p>\n<!--?# YouTubeDiv 3IuDAp9xV3w \"Soure Code Generation Game at DDD East Midlands\" /?-->\n</div>\n",
    "sanitized": "In this post, I highlight the potential dangers of trusting third party .NET Source Generators and show ways to try and spot Supply Chain Attacks trying to inject malicious code into your code base.\n\nThis post is part of Dustin Moris Gorski's .NET Advent Calendar. Go check out the other posts at https://dotnet.christmas/\n\nBackground\nSince they were introduced in .NET 5, I have been a big fan of C# source  code generators. They are a powerful tool that (a) help avoid the need to write a lot of boiler plate code and (b) also help improve performance of your code by allowing for code to be generated at compile time for tasks that you would previously may have resorted to using reflection to perform.\nIf you are not familiar with source code generators, there are lots of places you can find out about them. Here are a few presentations on You Tube that you may find useful\nIntroduction to Source Generators\n\nUsing Source Generators for Fun (and Maybe Profit)\n\nExploring Source Generators\n\nThese videos provide a lot of information as to why source generators are a great addition to .NET.  However ...\nWith Great Power Comes Great Responsibility\nI recently became aware of the potential dangers of using source code generators after reading two excellent blog posts.\nThe first is Mateusz Krzeszowiec's VeraCode Blog which provides an overview of supply chain attacks and how source code generators can be used to generate potentially harmful code that gets baked into your software\nThe second is  Maarten Balliauw 's Blog Post that also describes the problem, but also shows how attributes can be used to try and hide the code from being inspected.\nI highly recommend going and reading both of these before continuing, but if you want the TL;DR, here is my take on what these two blogs eloquently describe in great detail.\nSupply Chain Attacks\nAs mentioned in these blogs, a supply chain attack uses what appears to be a harmless component or build process to generate code that is added as part of the Continuous Integration build pipeline. This injected code can later be used to scrape data from users of that software. The most notable recent attack was the SolarWinds exploit.\nWith source generators, you are potentially open to attack on two fronts.\nThe first is that the generator has the ability to inspect your code via the syntax tree and/or the semantic tree. Whilst unlikely, there is the risk that the analysis stage could keep a lookout for common coding patterns where usernames, passwords, connection details etc. could be scraped and logged.\nHowever, the second is the more dangerous and that is to generate malicious code that 'dials home' information from users of your software. Maarten's blog shows how this is done and what to look for in the generated code.\nDefending Yourself Against Attacks\nThe NuGet team has a page that describes what to look for when assessing whether to use a NuGet package in general. However, given the risk of a rogue source generator creating malicious code, the packages need closer scrutiny.\nIn summary, here are a few things to consider before using a source generator from NuGet\n\nIs the package from a reputable source? If the package is from a commercial vendor, this is fairly easy to verify. However, for open source projects, this is a bit harder, which leads to the next check\nIs the source code available? If the package is open source, the source code will usually be available on GitHub. For source generators, it is always worth looking over the source code and checking what the generator is doing.\nIf the source code is not available, it may be worth considering using a tool such as JetBrains DotPeek to decompile the code. Two things to consider with this, though. The first is that this may be a breach of the license terms, so you need to tread carefully. The second is that the code may have been obfuscated, so will be harder to read and work out what it is doing.\nFor open source projects, can you be sure that the NuGet package published has been built from the code in the GitHub repository. Subject to licensing, you may want to consider forking the repo and creating your own build and manage within your own NuGet feed\nConsider using tools to vet packages or static source code analysers to check on generated code.\n\nEnsuring Generated Code Is Visible\nWhilst ideally, you will have done the due diligence described above, one of the safeguards you can put in place is to make some changes to your csproj file to ensure that the generated code is visible, so that it can be viewed and tracked in source control.\nWriting Compiler Generated Code into Files\nBy default, generated source code does not get written to files that you can see as the code is part of the Roslyn compile process. This makes it easy for bad actors providing code generators to hide under the radar.\nTo address this, there are two properties you can add to your project file that will make the generated code visible and trackable in source control\n \nThe first is the EmitCompilerGeneratedFiles property. This will write out any generated code files to the file system. It should be noted, that the contents of these files are effectively just a log of what has been added to the compilation process. The files themselves do not get used as part of the compilation.\nBy default, these files with be written to the obj folder in a path structure of\nobj\\BuildProfile\\Platform\\generated\\generator assembly\\generator namespace+name\nWithin this path there will be one or many files depending on how the source code generator has split the code into virtual files using the 'hintname' parameter to the AddSource method on the GeneratorExecutionContext at the end of the generator's Execute method.\nWhilst this gives some visibility, having the files in the obj folder is not really of help as this is usually excluded from source control.\nThis is where the second project property comes into play.\nThe CompilerGeneratedFilesOutputPath property allows you to specify a path. This will usually be relative to the consuming project's path. I typically use a path of 'GeneratedFiles'.\nBeneath this path, the structure is \\generator assembly\\generator namespace+name, again with one or more files depending on how the generated divides the generated code.\nAs with the files within the obj folder, these files do not take part in the actual compilation. However, this is where things get a bit tricky as by default, any *.cs files within the project folder structure get included. This causes the compiler to generate a whole load of errors stating that there is duplicate definitions of code in the files that have been output.\nTo get around this, an extra section needs to be added to the project file.\n \nAdding the this section excludes the generated files from compilation as they are there purely so that we can see the output in source control (as the actual generated code is already in the compilation pipeline in memory).\n<ItemGroup> \n   <Compile Remove=\"$(CompilerGeneratedFilesOutputPath)\\**\" /> \n</ItemGroup>\n\nNow that we have the files in the project file structure, it can be added to source control and any changes tracked. This then means that the files form part of the code review process and can be checked for anything 'dodgy' going on in the code\nView Generated Files in Visual Studio\nSince VS2019 16.10 and now in VS2022, the Solution Explorer window can now drill down into generated files by expanding each source code generator listed under the Analyzers node in the Solution Explorer window.\n\nYou may be interested in my previous blog post where I show how to set up source generator debugging in Visual Studio 2019. Whilst a couple of screens differ in VS2022 (due to the new debugging profile window), the guide works as well\n\nThis feature does not require the EmitCompilerGeneratedFiles to be enabled as this uses the in-memory compiler generated code (which *should* be the same as the emitted files.\nIn the screen shot below, I enabled the 'Show All Files' button and expanded out both the compiler generated output in the Analyzers node and also the emitted output files in the GeneratedFiles folder\n \nOne thing to also consider is that a sneaky attack may generate different code depending on whether the build is using a Debug or Release profile (as developers usually build locally in debug mode, whilst the CI build engine will be using release)\nDemo Code\nI have made the solution shown in the above image available in a GitHub repo at https://github.com/stevetalkscode/sourcegeneratorattacks\nThe solution illustrates the techniques described above using two source code generators.\nThe first is a simple generator that creates a class that has some of the danger signs to look out for based on Marten's blog.\nThe second is a demonstration of the new System.Text.Json code generation that has been introduced with .NET 6 for generating serialisation and deserialisation code that would previously have been handled by reflection (and is still the default behaviour unless explicitly used as shown in the demo).\nConclusion\nThe above discussion may put you off using source generators. This should not be the case as this feature of .NET is incredibly powerful and is starting to be used by Microsoft in .NET 6 with the improvements made to Json serialisation, razor page compilation and logging.\nBut if you are using source generators that you (or your team) have not written, you need to be aware of the potential dangers of not verifying where the generator has come from and what it is doing to your code.\n\"Let's be careful out there!\"\n\n\nWhile You Are Here ...\nIf you have enjoyed this blog post, you may be interested in my talk that I presented at DDD East Midlands where I discuss my experiences with source code generation of various shades over the past 30 years."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/debug-source-generators-with-vs2019-1610",
    "raw": "<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>I'm a big fan of source generators that were added in C#9, but debugging has been a bit of a pain so far, involving forcing breakpoints in code and attaching the debugger during compilation.</p>\n<p>With the RTM release of 16.10 Visual Studio 2019, things now get a bit easier with the introduction the new <span class=\"code-item\">IsRoslynComponent</span> element in the csproj file and a new debugger launch option of Roslyn Component.</p>\n<p>At time of writing, this has not had much publicity, other than a short paragraph in the <a href=\"https://docs.microsoft.com/en-us/visualstudio/releases/2019/release-notes#NETProductivity\" target=\"_blank\" class=\"externallink\">release notes</a> and a short demo in <a href=\"https://youtu.be/PTSSDrY1oDA?t=1197\">an episode of Visual Studio Toolbox</a>.</p>\n<p>Therefore, to give some visibility to this new functionality, I have written this short step-by-step guide.</p>\n<p>If you are new to source generators, there are lots of great resources out there, but a good starting point is this video from the ON.NET show</p>\n<!--?# YouTubeDiv cB66gOHConw \"ON.NET Show\" /?-->\n<h3 id=\"before-starting\">Before Starting</h3>\n<p>In order to take advantage of the new debugging features in VS2019, you will need to make sure that you have the .NET Compiler Platform SDK component installed as part of Visual Studio.</p>\n<p>This is shown as one of the components installed with the Visual Studio extension development workload.</p>\n<!--?# LinkSizedImage \"VSInstaller.png\" Alt=\"Visual Studio Installer workload\" /?-->\n<h2 id=\"step-by-step-guide\"><a name=\"step-by-step-guide\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#step-by-step-guide\">Step By Step Guide</a></h2>\n<p>For this guide, I am assuming that you already have a source generator that you have written, but if not,&nbsp;I have put the <a href=\"https://github.com/stevetalkscode/DebugSourceGenWithVs2019\" target=\"_blank\" class=\"externallink\">source code for this guide on GitHub if you want to download a working example</a>.</p>\n<h3 id=\"step-1-add-the-isroslyncomponent-property-to-your-source-generator-project\">Step 1 - Add the IsRoslynComponent Property to Your Source Generator Project</h3>\n<p>The first step in making use of the new debugging functionality is to add a new entry to your project.</p>\n<p>The property to add is <span class=\"code-item\">IsRoslynComponent</span>&nbsp;and the value should be set to <span class=\"code-item\">true</span></p>\n<p>You will need to do this in the text editor as there is no user interface to add this.</p>\n<!--?# LinkSizedImage \"IsRoslynComponent.png\" Alt=\"Screen shot of setting the IsRoslynComponent property in a project file\" /?-->\n<p>Whist in the project file, make sure that you have the latest versions of the core NuGet packages that you require / are useful to work with source generators.</p>\n<p>The screen shot below shows the versions that are active at time of writing this post</p>\n<!--?# LinkSizedImage \"SourceGenPackageVersions.png\" Alt=\"Source Generator Package Versions screen shot from project file\" /?-->\n<h3 id=\"step-2-add-a-reference-to-your-source-generator-project-to-a-consuming-project\">Step 2 - Add a Reference to Your Source Generator Project to a Consuming Project</h3>\n<p>For the purpose of this guide, I am using a project reference to consume the source generator.</p>\n<p>When referencing a source generator, there are two additional attributes that need to be set to flag that this is an analyser that does not need to be distributed with the final assembly. These are</p>\n<ul>\n<li><span class=\"code-item\">OutputItemType = \"Analyzer\"</span></li>\n<li><span class=\"code-item\">ReferenceOutputAssembly=\"false\"</span></li>\n</ul>\n<!--?# LinkSizedImage \"SourceGenRefProject.png\" Alt=\"Referencing the Source Generator project\" /?-->\n<h3 id=\"step-3-build-all-projects-and-view-source-code\">Step 3 - Build All Projects and View Source Code</h3>\n<p>To keep this guide short, I am assuming that both the source generator project and the consuming project have no errors and will build.</p>\n<p>Assuming that this is the case, there are two new features in VS2019 for source generators that are useful for viewing the generated code.</p>\n<p>The first is in the Dependencies node of the consuming project where you will now find an entry in the Analyzers node of the tree for your source generator project. Under this, you can now see the generated files (<em>Item 1</em> in the screen shot below)</p>\n<!--?# LinkSizedImage \"debug.png\" Alt=\"Screen shot of VS2019 Source Generated Code in Solution Explorer view\" /?-->\n<p>There are two other things to notice in this screen shot.</p>\n<p><em>Item 2</em> is the warning that this is a code generated file and cannot be edited (the code editor will not allow you to type into the window even if you try).</p>\n<p><em>Item 3</em> highlights the editor showing where the file is located. Note that by default, this is in your local temp directory as specified by the environmental variable.</p>\n<p>Now that the source code can be viewed in the code window, you can set a breakpoint in the generated code as you would with your own code.</p>\n<p>The second new feature is that the code window treats the file as a regular code file, so you can do things like 'Find All References' from within the generated file. Conversely, you can use 'Goto Definition' from any usages of the generated members to get to the source generated file in the editor.</p>\n<h3 id=\"step-4-prepare-to-debug-the-source-generator\">Step 4 - Prepare to Debug The Source Generator</h3>\n<p>This is where the 'magic' in VS2019 now kicks in!</p>\n<p>First, in Solution Explorer, navigate to the source generator project and navigate to the properties dialog, then into the Debug tab.</p>\n<!--?# LinkSizedImage \"SourceGenProperties.png\" Alt=\"Screen shot of navigating to the Properties dialog from Solution Explorer\" /?-->\n<!--?# LinkSizedImage \"DebugRoslynComponentDialog.png\" Alt=\"Screen shot of setting Roslyn Component as the launch setting for debugging a source generator project\" /?-->\n<p>At this point, when you open the Launch drop down, you will notice at the top there is now a Roslyn Component option which has now been enabled thanks to the <span class=\"code-item\">IsRoslynComponent</span> entry in your project file.</p>\n<p>Having clicked on this, you can set which project you want to use to trigger the source generator (in our case, the consuming project) in the drop down in the panel. Then set use the context menu on the the source generator project to set it to be the start up project to use when you hit F5 to start debugging.</p>\n<!--?# LinkSizedImage \"SourceDebugSetStartupProject.png\" Alt=\"Screen shot of setting Roslyn Component as the launch setting for debugging a source generator project\" /?-->\n<h3 id=\"step-5-start-debugging\">Step 5 - Start Debugging</h3>\n<p>In your source generator project, find an appropriate line to place a breakpoint in your code, inside either the <span class=\"code-item\">Inititialize</span> or&nbsp;<span class=\"code-item\">Execute</span> methods of your generator class.</p>\n<p>You are now ready to hit F5 to start debugging!</p>\n<p>What you will notice is that a console window pops up with the C# compiler being started. This is because you are now debugging within stage of the compiler that is using your code to generate the source code to be used elsewhere in the build.</p>\n<!--?# LinkSizedImage \"debugsourcegenroslynconsole.png\" Alt=\"Screen shot of debugging as source generator with compiler opened in console\" /?-->\n<p>Now, you can step through your source generator code as you would with any other code in the IDE.</p>\n<h3 id=\"step-6-changing-code-requires-a-restart-of-visual-studio\">Step 6 - Changing Code Requires a Restart of Visual Studio</h3>\n<p>If as part of the debugging process you find you need to change your code. When you recompile and start debugging again, you may find that your changes have not taken effect from within your Visual Studio session.</p>\n<p>This is due to the way that Visual Studio caches analyzers and source code generators. In the 16.10 release, the flushing of the cache has not yet been fully addressed and therefore, you will still need to restart Visual Studio to see your code changes within the generator take effect.</p>\n<p>If you find that you need to make changes iteratively to debug a problem, you may want to go back to including conditional statements in your code to launch the debugger and use the dotnet build command line to get the compiler to trigger the source code generation in order to debug the problem.</p>\n<p>If you do need to do this, I take the following approach to avoid debugger code slipping into the release build.</p>\n<p>(1) In the Configuration Manager, create a new build configuration based on the Debug configuration called <span class=\"code-item\">DebugGenerator</span></p>\n<p>(2) In the project's build properties, create a conditional compilation symbol called <span class=\"code-item\">DEBUGGENERATOR</span></p>\n<p>(3) In the <span class=\"code-item\">Initialize</span> method of the source generator to be debugged, add the following code</p>\n<!--?# Gist 45d1755611d3f463bf30ca53ab5af7ec /?-->\n<p>(4) Instead of using Visual Studio to trigger a build, open a command line instance and use the following to start the build</p>\n<p><span class=\"code-item\">dotnet build -c DebugGenerator --no-incremental</span></p>\n<p>This will force the build to use the configuration that will trigger the debugger to launch. Usually when the compiler reaches the line to launch the debugger, you will be prompted to select an instance of Visual Studio. At this point, select the instance that you have just been editing.</p>\n<div style=\"margin:0 30% 0 30%\">\n\t<!--?# LinkSizedImage \"SourceGenSelectDebugInstance.png\" Alt=\"Image of Just-in-Time Debugger Selector dialog\" /?-->\n</div>\n<p>After a short period of loading symbols, Visual Studio will break on the Debugger.Launch line. You can now set a breakpoint anywhere in your source generator project (if not already set) and use F5 to run to that line.</p>\n<p><em>Note, I have used the <span class=\"code-item\">--no-incremental</span> switch to force a rebuild so that the debugger launch is triggered even if the code has not changed.</em></p>\n<h2 id=\"a-gotcha\"><a name=\"gotcha\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#gotcha\">A Gotcha!</a></h2>\n<p>When I started playing with this new functionality, I loaded up an existing source generator that had been written by a colleague and found that the option to select Roslyn Component was not available, but worked when I created a new source generator project</p>\n<p>After a few hours of trial and error by editing the project file, I found that the existing source generator had a reference to the Microsoft.Net.Compilers.Toolset NuGet package. Taking this out and restarting Visual Studio triggered the new functionality of VS to kick in.</p>\n<p>If you look at the <a href=\"https://www.nuget.org/packages/Microsoft.Net.Compilers.Toolset\" target=\"_blank\" class=\"externallink\">description of the package</a>, it becomes clear where the problem arises. In short, it comes with its own set of compilers instead of using the default compiler. The current 'live' version is 3.9.0 which does not appear to support the <span class=\"code-item\">IsRoslynComponent</span> element.&nbsp;The version required to work is still in pre-release - 3.10.0-3.final.</p>\n<p>If you hit this snag, it is worth investigating why the package has been used and whether it can be removed given that</p>\n<ul>\n<li>It is not intended for general consumption</li>\n<li>It is only intended as a short-term fix when the out-of-the-box compiler is crashing and awaiting a fix from Microsoft</li>\n</ul>\n<p>More details on why it should not be used can be found in this <a href=\"https://stackoverflow.com/a/60623906\" target=\"_blank\" class=\"externallink\">Stack Overflow answer from Jared Parsons</a>.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>Whilst not perfect due to the caching problem, the 16.10 release of Visual Studio has added some rich new functionality to help with writing and debugging source generators.</p>\n<!--?# Gist 45d1755611d3f463bf30ca53ab5af7ec /?-->\n",
    "sanitized": "Background\nI'm a big fan of source generators that were added in C#9, but debugging has been a bit of a pain so far, involving forcing breakpoints in code and attaching the debugger during compilation.\nWith the RTM release of 16.10 Visual Studio 2019, things now get a bit easier with the introduction the new IsRoslynComponent element in the csproj file and a new debugger launch option of Roslyn Component.\nAt time of writing, this has not had much publicity, other than a short paragraph in the release notes and a short demo in an episode of Visual Studio Toolbox.\nTherefore, to give some visibility to this new functionality, I have written this short step-by-step guide.\nIf you are new to source generators, there are lots of great resources out there, but a good starting point is this video from the ON.NET show\n\nBefore Starting\nIn order to take advantage of the new debugging features in VS2019, you will need to make sure that you have the .NET Compiler Platform SDK component installed as part of Visual Studio.\nThis is shown as one of the components installed with the Visual Studio extension development workload.\n\nStep By Step Guide\nFor this guide, I am assuming that you already have a source generator that you have written, but if not, I have put the source code for this guide on GitHub if you want to download a working example.\nStep 1 - Add the IsRoslynComponent Property to Your Source Generator Project\nThe first step in making use of the new debugging functionality is to add a new entry to your project.\nThe property to add is IsRoslynComponent and the value should be set to true\nYou will need to do this in the text editor as there is no user interface to add this.\n\nWhist in the project file, make sure that you have the latest versions of the core NuGet packages that you require / are useful to work with source generators.\nThe screen shot below shows the versions that are active at time of writing this post\n\nStep 2 - Add a Reference to Your Source Generator Project to a Consuming Project\nFor the purpose of this guide, I am using a project reference to consume the source generator.\nWhen referencing a source generator, there are two additional attributes that need to be set to flag that this is an analyser that does not need to be distributed with the final assembly. These are\n\nOutputItemType = \"Analyzer\"\nReferenceOutputAssembly=\"false\"\n\n\nStep 3 - Build All Projects and View Source Code\nTo keep this guide short, I am assuming that both the source generator project and the consuming project have no errors and will build.\nAssuming that this is the case, there are two new features in VS2019 for source generators that are useful for viewing the generated code.\nThe first is in the Dependencies node of the consuming project where you will now find an entry in the Analyzers node of the tree for your source generator project. Under this, you can now see the generated files (Item 1 in the screen shot below)\n\nThere are two other things to notice in this screen shot.\nItem 2 is the warning that this is a code generated file and cannot be edited (the code editor will not allow you to type into the window even if you try).\nItem 3 highlights the editor showing where the file is located. Note that by default, this is in your local temp directory as specified by the environmental variable.\nNow that the source code can be viewed in the code window, you can set a breakpoint in the generated code as you would with your own code.\nThe second new feature is that the code window treats the file as a regular code file, so you can do things like 'Find All References' from within the generated file. Conversely, you can use 'Goto Definition' from any usages of the generated members to get to the source generated file in the editor.\nStep 4 - Prepare to Debug The Source Generator\nThis is where the 'magic' in VS2019 now kicks in!\nFirst, in Solution Explorer, navigate to the source generator project and navigate to the properties dialog, then into the Debug tab.\n\n\nAt this point, when you open the Launch drop down, you will notice at the top there is now a Roslyn Component option which has now been enabled thanks to the IsRoslynComponent entry in your project file.\nHaving clicked on this, you can set which project you want to use to trigger the source generator (in our case, the consuming project) in the drop down in the panel. Then set use the context menu on the the source generator project to set it to be the start up project to use when you hit F5 to start debugging.\n\nStep 5 - Start Debugging\nIn your source generator project, find an appropriate line to place a breakpoint in your code, inside either the Inititialize or Execute methods of your generator class.\nYou are now ready to hit F5 to start debugging!\nWhat you will notice is that a console window pops up with the C# compiler being started. This is because you are now debugging within stage of the compiler that is using your code to generate the source code to be used elsewhere in the build.\n\nNow, you can step through your source generator code as you would with any other code in the IDE.\nStep 6 - Changing Code Requires a Restart of Visual Studio\nIf as part of the debugging process you find you need to change your code. When you recompile and start debugging again, you may find that your changes have not taken effect from within your Visual Studio session.\nThis is due to the way that Visual Studio caches analyzers and source code generators. In the 16.10 release, the flushing of the cache has not yet been fully addressed and therefore, you will still need to restart Visual Studio to see your code changes within the generator take effect.\nIf you find that you need to make changes iteratively to debug a problem, you may want to go back to including conditional statements in your code to launch the debugger and use the dotnet build command line to get the compiler to trigger the source code generation in order to debug the problem.\nIf you do need to do this, I take the following approach to avoid debugger code slipping into the release build.\n(1) In the Configuration Manager, create a new build configuration based on the Debug configuration called DebugGenerator\n(2) In the project's build properties, create a conditional compilation symbol called DEBUGGENERATOR\n(3) In the Initialize method of the source generator to be debugged, add the following code\n\n(4) Instead of using Visual Studio to trigger a build, open a command line instance and use the following to start the build\ndotnet build -c DebugGenerator --no-incremental\nThis will force the build to use the configuration that will trigger the debugger to launch. Usually when the compiler reaches the line to launch the debugger, you will be prompted to select an instance of Visual Studio. At this point, select the instance that you have just been editing.\n\n\t\n\nAfter a short period of loading symbols, Visual Studio will break on the Debugger.Launch line. You can now set a breakpoint anywhere in your source generator project (if not already set) and use F5 to run to that line.\nNote, I have used the --no-incremental switch to force a rebuild so that the debugger launch is triggered even if the code has not changed.\nA Gotcha!\nWhen I started playing with this new functionality, I loaded up an existing source generator that had been written by a colleague and found that the option to select Roslyn Component was not available, but worked when I created a new source generator project\nAfter a few hours of trial and error by editing the project file, I found that the existing source generator had a reference to the Microsoft.Net.Compilers.Toolset NuGet package. Taking this out and restarting Visual Studio triggered the new functionality of VS to kick in.\nIf you look at the description of the package, it becomes clear where the problem arises. In short, it comes with its own set of compilers instead of using the default compiler. The current 'live' version is 3.9.0 which does not appear to support the IsRoslynComponent element. The version required to work is still in pre-release - 3.10.0-3.final.\nIf you hit this snag, it is worth investigating why the package has been used and whether it can be removed given that\n\nIt is not intended for general consumption\nIt is only intended as a short-term fix when the out-of-the-box compiler is crashing and awaiting a fix from Microsoft\n\nMore details on why it should not be used can be found in this Stack Overflow answer from Jared Parsons.\nConclusion\nWhilst not perfect due to the caching problem, the 16.10 release of Visual Studio has added some rich new functionality to help with writing and debugging source generators."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/jetbrains-di-webinar",
    "raw": "<p>Last week, I had the honour of presenting a live webinar for JetBrains where I gave a presentation about using Dependency Injection in .NET Core and .NET 5.</p>\n<p>You can find out more details about what is included in the talk on <a href=\"https://blog.jetbrains.com/dotnet/2021/04/09/net-5-dependency-injection-webinar-recording/\" target=\"_blank\" class=\"externallink\">the JetBrains Blog,</a> or go direct to the recording on <a href=\"https://youtu.be/0x2KW-dJDQU\">YouTube.</a></p>\n<div class=\"youtube-wrapper\" >\n<iframe class=\"youtube-iframe\" width=\"2000\" height=\"1113\" src=\"https://www.youtube.com/embed/0x2KW-dJDQU?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n<p>A big thank you to JetBrains and to <a href=\"https://twitter.com/maartenballiauw\" target=\"_blank\" class=\"externallink\">Maarten Balliauw</a> for hosting.</p>\n",
    "sanitized": "Last week, I had the honour of presenting a live webinar for JetBrains where I gave a presentation about using Dependency Injection in .NET Core and .NET 5.\nYou can find out more details about what is included in the talk on the JetBrains Blog, or go direct to the recording on YouTube.\n\n\n\nA big thank you to JetBrains and to Maarten Balliauw for hosting."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/openapireference-commands",
    "raw": "<p>This is not one of my usual blogs, but an aide-mémoire for myself that may be of use to other people who are using the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/web-api/microsoft.dotnet-openapi\" target=\"_blank\" class=\"externallink\">OpenApiReference tooling</a> in their C# projects to generate C# client code for HTTP APIs from Swagger/OpenApi definitions.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>I have been using <a href=\"https://twitter.com/ricosuter\" target=\"_blank\" class=\"externallink\">Rico Suter's</a> brilliant <a href=\"https://github.com/RicoSuter/NSwag\" target=\"_blank\" class=\"externallink\">NSwag</a> for some time now to generate client code for working with HTTP API endpoints. Initially I was using the <a href=\"https://github.com/RicoSuter/NSwag\" target=\"_blank\" class=\"externallink\">NSwag Studio</a> application to create the C# code and placing the output into my project, but then I later found I could add the code generation into build process using the <a href=\"https://github.com/RicoSuter/NSwag\" target=\"_blank\" class=\"externallink\">NSwag MSBuild task</a>.</p>\n<p>Recently though, I watched the <a href=\"https://dotnet.microsoft.com/live/community-standup\" target=\"_blank\" class=\"externallink\">ASP.NET Community Standup</a> with <a href=\"https://twitter.com/jongalloway\" target=\"_blank\" class=\"externallink\">Jon Galloway</a> and <a href=\"https://twitter.com/bradygaster\" target=\"_blank\" class=\"externallink\">Brady Gaster</a>. In the <a href=\"https://youtu.be/Mpf0fCO6NrU?list=PLdo4fOcmZ0oX-DBuRG4u58ZTAJgBAeQ-t&amp;t=4037\" target=\"_blank\" class=\"externallink\">last half hour</a> or so, they discuss the Connected Services functionality in Visual Studio 2019 that sets up code generation of HTTP API endpoint clients for you.</p>\n<div class=\"youtube-wrapper\">\n<iframe class=\"youtube-iframe\" width=\"2000\" height=\"1113\" src=\"https://www.youtube.com/embed/Mpf0fCO6NrU?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe>\n</div>\n<p>This feature had passed me by and watching the video got my curiosity going as to whether the build chain that I have been using for the last couple of years could be simplified. Especially given that, behind the scenes, it is using NSwag to do the code generation.</p>\n<h3 id=\"using-connected-services\"><a name=\"using-connected-services\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#using-connected-services\">Using Connected Services</a></h3>\n<p>Having watched the video above, I recommend reading Jon Galloway's post <a href=\"https://devblogs.microsoft.com/aspnet/generating-http-api-clients-using-visual-studio-connected-services/\" target=\"_blank\" class=\"externallink\">Generating HTTP API clients using Visual Studio Connected Services</a> As that post covers the introduction to using Connected Services, I won't repeat the basics again here.</p>\n<p>It is also worth reading the other blog posts in the series written by Brady Gaster:</p>\n<ul>\n<li><a href=\"https://devblogs.microsoft.com/aspnet/creating-discoverable-http-apis-with-asp-net-core-5-web-api/\" target=\"_blank\" class=\"externallink\">Creating Discoverable HTTP APIs with ASP.NET Core 5 Web API</a></li>\n<li><a href=\"https://devblogs.microsoft.com/aspnet/open-source-http-api-packages-and-tools/?WT.mc_id=dotnet-13135-bradyg\" target=\"_blank\" class=\"externallink\">Open-source HTTP API packages and tools</a></li>\n<li><a href=\"https://devblogs.microsoft.com/aspnet/app-building-with-azure-api-management-functions-power-apps-and-logic-apps/\" target=\"_blank\" class=\"externallink\">App Building with Azure API Management, Functions, Power Apps, and Logic Apps</a></li>\n</ul>\n<p><em>One of the new things I learnt from the video and blog posts is to make sure that your OpenApi definitions in the source API include an OperationId (which you can set by overloads of&nbsp; the HttpGet, HttpPost (etc) attributes on your Action method) to help the code generator assign a 'sensible' names to the calling method in the code generated client.</em></p>\n<h3 id=\"purpose-of-this-post\"><a name=\"purpose-of-this-post\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#purpose-of-this-post\">Purpose of This Post</a></h3>\n<p>Having started with using the Visual Studio dialogs to set up the Connected Service, the default options may not necessarily match with how you want the generated code to work in your particular project.</p>\n<p>Having had a few years' experience of using NSwag to generate code, I started to dig deeper into how to get the full customisation I have been used to from using the \"full\" NSwag experience but within the more friendly build experience of using the OpenApiReference project element.</p>\n<h3 id=\"one-gotcha-to-be-aware-of\"><a name=\"one-gotcha-to-be-aware-of\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#one-gotcha-to-be-aware-of\">One Gotcha To Be Aware Of!</a></h3>\n<p>If you use the Connected Services dialog in Visual Studio to create the connected service, you will hit a problem if you have used a Directory.Packages.props file to manage your NuGet packages centrally across your solution. The Connnected Services wizard (as at time of writing) tries to specific versions of NuGet packages.</p>\n<p>This is part of a wider problem in Visual Studio (as at time of writing) where the NuGet Package Manager interaction clashes with the restrictions applied in Directory.Packages.props. However, this may be addressed in future versions as of the NuGet tooling and Visual Studio per <a href=\"https://github.com/NuGet/Home/wiki/Centrally-managing-NuGet-package-versions\" target=\"_blank\" class=\"externallink\">this Wiki post</a>.</p>\n<p>If you are not familiar with using Directory.Packages.props, have a look at this <a href=\"https://stu.dev/managing-package-versions-centrally/\" target=\"_blank\" class=\"externallink\">blog post from Stuart Lang</a></p>\n<h2 id=\"manually-updating-the-openapireference-entry-in-your-project\"><a name=\"manually-updating-openapireference-in-project\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#manually-updating-openapireference-in-project\">Manually Updating the OpenApiReference Entry in your Project</a></h2>\n<p>There isn't much documentation around on how to make adjustments to the OpenApiReference element that gets added to your csproj file, so hopefully this post will fill in some of the gaps until documentation is added to the Microsoft Docs site.</p>\n<p>I have based this part of the post on looking through the source code at <a href=\"https://github.com/dotnet/aspnetcore/tree/main/src/Tools/Extensions.ApiDescription.Client\" target=\"_blank\" class=\"externallink\">https://github.com/dotnet/aspnetcore/tree/main/src/Tools/Extensions.ApiDescription.Client</a> and therefore some of my conclusions may be wrong, so proceed with caution if making changes to your project.</p>\n<p>The main source of information is the <a href=\"https://github.com/dotnet/aspnetcore/blob/main/src/Tools/Extensions.ApiDescription.Client/src/build/Microsoft.Extensions.ApiDescription.Client.props\" target=\"_blank\" class=\"externallink\">Microsoft.Extensions.ApiDescription.Client.props</a> file which defines the XML schema and includes comments that I have used here.</p>\n<h2 id=\"the-openapireference-and-openapiprojectreference-elements\"><a name=\"openapireference-openapiprojectreference-elements\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapireference-openapiprojectreference-elements\">The OpenApiReference and OpenApiProjectReference Elements</a></h2>\n<p>These two elements can be added one or multiple times within an ItemGroup in your csproj file.</p>\n<p>The main focus of this section is the OpenApiReference element that adds code generation to the current project for a specific OpenApi JSON definition.</p>\n<p>The OpenApiProjectReference allows external project references to be added as well. More on this below,</p>\n<p>The following attributes and sub-elements are the main areas of interest within the OpenApiReference element.</p>\n<p>The <a href=\"https://github.com/dotnet/aspnetcore/blob/main/src/Tools/Extensions.ApiDescription.Client/src/build/Microsoft.Extensions.ApiDescription.Client.props\" target=\"_blank\" class=\"externallink\">props file</a> makes references to other properties that live outside of the element that you can override within your csproj file.</p>\n<p>As I haven't used the TypeScript generator, I have focussed my commentary on the NSwagCSharp code generator.</p>\n<h3 id=\"include-attribute-required\"><a name=\"include-attribute-is-required\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#include-attribute-is-required\">Include Attribute (Required)</a></h3>\n<p>The contents of the Include attribute will depend on which element you are in.</p>\n<p>For <strong>OpenApiReference</strong> this will be the path to the OpenApi/Swagger json file that will be the source the code generator will use.</p>\n<p>For <strong>OpenApiProjectReference</strong> this will be the path to another project that is being referenced.</p>\n<h3 id=\"classname-element-optional\"><a name=\"classname-element\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#classname-element\">ClassName Element (Optional)</a></h3>\n<p>This is the name to give the class that will be generated. If not specified, the class will default to the name given in the OutputPath parameter (see below)</p>\n<h3 id=\"codegenerator-element-required\"><a name=\"codegenerator-element\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#codegenerator-element\">CodeGenerator Element (Required)</a></h3>\n<p>The default value is 'NSwagCSharp'. This points to the NSwag C# client generator, more details of which below.</p>\n<p>At time of writing, only C# and TypeScript are supported, and the value here must end with either \"CSharp\" or \"TypeScript\". Builds will invoke a MSBuild target named \"Generate(CodeGenerator)\" to do actual code generation. More on this below.</p>\n<h3 id=\"namespace-element-optional\"><a name=\"namespace-element\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#namespace-element\">Namespace Element (Optional)</a></h3>\n<p>This is the namespace to assign the generated class to. If not specified, the RootNamespace entry from your project will be used to put the class within your project's namespace. You may choose to be more specific with the NSwag specific commands below.</p>\n<h3 id=\"options-element-optional\"><a name=\"options-element\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#options-element\">Options Element (Optional)</a></h3>\n<p>These are the customisation instructions that will be passed to the code generator as command line options.&nbsp;See Customising The Generated Code with NSwag Commands below for details about usage with the NSwagCSharp generator</p>\n<p>One of the problems I have been having with this element is that the contents are passed to the command line of the NSwagCSharp as-is and therefore you cannot include line breaks to make it more readable.</p>\n<p>It would be nice if there was a new element that allows each command option to be listed as an XML sub-element in its own right that the MSBuild target concatenates and parses into the single command line to make editing the csproj file a bit easier.</p>\n<!--?# LinkSizedImage \"OptionsDeclaration.jpg\" Alt=\"Possible Options Declaration\" /?-->\n<h3 id=\"outputpath-optional\"><a name=\"outputpath\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#outputpath\">OutputPath (Optional)</a></h3>\n<p>This is the path to place generated code into. It is up to the code generator as to whether to interpret the path as a filename or as a directory.</p>\n<p>The Default filename or folder name is %(Filename)Client.[cs|ts].</p>\n<p>Filenames and relative paths (if explicitly set) are combined with $(OpenApiCodeDirectory). Final value is likely to be a path relative to the client project.</p>\n<h3 id=\"globalpropertiestoremove-openapiprojectreference-only-optional\"><a name=\"globalpropertiestoremove\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#globalpropertiestoremove\">GlobalPropertiesToRemove (OpenApiProjectReference Only - Optional)</a></h3>\n<p>This is a semicolon-separated list of global properties to remove in a ProjectReference item created for the OpenApiProjectReference. These properties, along with Configuration, Platform, RuntimeIdentifier and TargetFrameworks, are also removed when invoking the 'OpenApiGetDocuments' target in the referenced project.</p>\n<h2 id=\"other-property-elements\"><a name=\"other-property-elements\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#other-property-elements\">Other Property Elements</a></h2>\n<p>In the section above, there are references to other properties that get set within the props file.</p>\n<p>The properties can be overridden within your csproj file, so for completeness, I have added some commentary here</p>\n<h3 id=\"openapigeneratecodeoptions\"><a name=\"openapigeneratecodeoptions\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapigeneratecodeoptions\">OpenApiGenerateCodeOptions</a></h3>\n<p>The Options element above if not populated defaults to the contents of this element, which in of itself is empty by default.</p>\n<p>As per my comment above for Options, this suffers the same problem of all command values needing to be on the same single line.</p>\n<h3 id=\"openapigeneratecodeonbuild\"><a name=\"openapigeneratecodeonbuild\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapigeneratecodeonbuild\">OpenApiGenerateCodeOnBuild</a></h3>\n<p>If this is set to 'true' (the default), code is generated for the OpenApiReference element and any OpenApiProjectReference items before the BeforeCompile target is invoked.</p>\n<p>However, it may be that you do not want the generated called on every single build as you may have set up a CI pipeline where the target is explicitly invoked (via a command line or as a build target) as a build step before the main build. In that case, the value can be set to 'false'</p>\n<h3 id=\"openapigeneratecodeatdesigntime\"><a name=\"openapigeneratecodeatdesigntime\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapigeneratecodeatdesigntime\">OpenApiGenerateCodeAtDesignTime</a></h3>\n<p>Similar to OpenApiGenerateCodeOnBuild above, but this time determines whether to generate code at design time as well as being part of a full build. This is set to true by default.</p>\n<h3 id=\"openapibuildreferencedprojects\"><a name=\"openapibuildreferencedprojects\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapibuildreferencedprojects\">OpenApiBuildReferencedProjects</a></h3>\n<p>If set to 'true' (the default), any projects referenced in an ItemGroup containing one or many OpenApiProjectReference elements will get built before retrieving that project's OpenAPI documents list (or generating code).</p>\n<p>If set to 'false', you need to ensure the referenced projects are built before the current project within the solution or through other means (such as a build pipeline) but IDEs such as Visual Studio and Rider may get confused about the project dependency graph in this case.</p>\n<h3 id=\"openapicodedirectory\"><a name=\"openapicodedirectory\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#openapicodedirectory\">OpenApiCodeDirectory</a></h3>\n<p>This is the default folder to place the generated code into. The value is interpreted relative to the project folder, unless already an absolute path. This forms part of the default OutputPath within the OpenApiReference above and the OpenApiProjectReference items.</p>\n<p>The default value for this is <strong>BaseIntermediateOutputPath</strong> which is set elsewhere in your csproj file or is implicitly set by the SDK.</p>\n<h2 id=\"customising-the-generated-code-with-nswag-commands\"><a name=\"customising-nswag-commands\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#customising-nswag-commands\">Customising The Generated Code with NSwag Commands</a></h2>\n<p>Here we get to the main reason I have written this post.</p>\n<p>There is a huge amount of customisation that you can do to craft the generated code into a shape that suits you.</p>\n<p>The easiest way to get an understanding of the levels of customisation is to use NSwag Studio to pay around with the various customisation options and see how the options affect the generated code.</p>\n<p>Previously when I have been using the NSwag MSBuild task, I have pointed the task to an NSwag configuration json file saved from the NSwag Studio and let the build process get on with the job of doing the code generation as a pre-build task.</p>\n<p>However, the OpenApiReference task adds a layer of abstraction that means that just using the NSwag configuration file is not an option. Instead, you need to pass the configuration as command line parameters via the <options> element.</options></p>\n<p>This can get a bit hairy for a couple of reasons.</p>\n<ul>\n<li>Firstly, each command has to be added to one single line which can make your csproj file a bit unwieldy to view if you have a whole load of customisations that you want to make (scroll right, scroll right, scroll right again!)</li>\n<li>Secondly, you need to know all the NSwag commands and the associated syntax to pass these to the Options element.</li>\n</ul>\n<h3 id=\"options-syntax\"><a name=\"options-syntax\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#options-syntax\">Options Syntax</a></h3>\n<p>Each option that you want to pass takes the form of a command line parameter which</p>\n<ul>\n<li>starts with a forward slash</li>\n<li>followed by the command</li>\n<li>then a colon and then</li>\n<li>the value to pass to the command</li>\n</ul>\n<p>So, something like this:&nbsp;<strong>/ClientBaseClass:ClientBase</strong></p>\n<p>The format of the value depends on the value type of the command of which there are three common ones</p>\n<ul>\n<li>boolean values are set with true or false. E.g. /GenerateOptionalParameters:true</li>\n<li>string values are set with the string value as-is. E.g. /ClassStyle:Poco</li>\n<li>string arrays are comma delimited lists of string values. E.g. /AdditionalNamespaceUsages:MyNamespace1,MyNamespace2,MyNamespace3</li>\n</ul>\n<p>The following table is a GitHub gist copy from <a href=\"https://github.com/stevetalkscode/NSwagCommands\" target=\"_blank\" class=\"externallink\">the GitHub repository I have set up for this</a> and which I plan to update over time as I get a better understanding of each command and its effect on the generated code.</p>\n<p>At time of writing, many of the descriptions have been lifted from the XML comments in the code from the NSwag repository on GitHub.</p>\n<p><em>(Apologies the format of the imported markdown here is not great. I hope to make this a bit better later when I can find the time. You may want to go direct <a href=\"https://gist.github.com/stevetalkscode/69719465d8271f1e9fa412626fdadfcd\" target=\"_blank\" class=\"externallink\">to the gist directly</a>)</em></p>\n<style>\n    body .gist .gist-file {        \n        color: black;\n        font-size: 0.5em;\n    }\n\n    .markdown-body td\n    {\n        font-size: 0.6em;\n    }\n</style>\n<!--?# Gist 69719465d8271f1e9fa412626fdadfcd /?-->\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>The new tooling makes the code generation build process itself a lot simpler, but there are a few hoops to jump through to customise the code generated.</p>\n<p>I've been very impressed with the tooling and I look forward to seeing how to it progresses in the future.</p>\n<p>I hope that this blog is of help to anyone else who wants to understand more about the customisation of the OpenApiReference tooling and plugs a gap in the lack of documentation currently available.</p>\n",
    "sanitized": "This is not one of my usual blogs, but an aide-mémoire for myself that may be of use to other people who are using the OpenApiReference tooling in their C# projects to generate C# client code for HTTP APIs from Swagger/OpenApi definitions.\nBackground\nI have been using Rico Suter's brilliant NSwag for some time now to generate client code for working with HTTP API endpoints. Initially I was using the NSwag Studio application to create the C# code and placing the output into my project, but then I later found I could add the code generation into build process using the NSwag MSBuild task.\nRecently though, I watched the ASP.NET Community Standup with Jon Galloway and Brady Gaster. In the last half hour or so, they discuss the Connected Services functionality in Visual Studio 2019 that sets up code generation of HTTP API endpoint clients for you.\n\n\n\nThis feature had passed me by and watching the video got my curiosity going as to whether the build chain that I have been using for the last couple of years could be simplified. Especially given that, behind the scenes, it is using NSwag to do the code generation.\nUsing Connected Services\nHaving watched the video above, I recommend reading Jon Galloway's post Generating HTTP API clients using Visual Studio Connected Services As that post covers the introduction to using Connected Services, I won't repeat the basics again here.\nIt is also worth reading the other blog posts in the series written by Brady Gaster:\n\nCreating Discoverable HTTP APIs with ASP.NET Core 5 Web API\nOpen-source HTTP API packages and tools\nApp Building with Azure API Management, Functions, Power Apps, and Logic Apps\n\nOne of the new things I learnt from the video and blog posts is to make sure that your OpenApi definitions in the source API include an OperationId (which you can set by overloads of  the HttpGet, HttpPost (etc) attributes on your Action method) to help the code generator assign a 'sensible' names to the calling method in the code generated client.\nPurpose of This Post\nHaving started with using the Visual Studio dialogs to set up the Connected Service, the default options may not necessarily match with how you want the generated code to work in your particular project.\nHaving had a few years' experience of using NSwag to generate code, I started to dig deeper into how to get the full customisation I have been used to from using the \"full\" NSwag experience but within the more friendly build experience of using the OpenApiReference project element.\nOne Gotcha To Be Aware Of!\nIf you use the Connected Services dialog in Visual Studio to create the connected service, you will hit a problem if you have used a Directory.Packages.props file to manage your NuGet packages centrally across your solution. The Connnected Services wizard (as at time of writing) tries to specific versions of NuGet packages.\nThis is part of a wider problem in Visual Studio (as at time of writing) where the NuGet Package Manager interaction clashes with the restrictions applied in Directory.Packages.props. However, this may be addressed in future versions as of the NuGet tooling and Visual Studio per this Wiki post.\nIf you are not familiar with using Directory.Packages.props, have a look at this blog post from Stuart Lang\nManually Updating the OpenApiReference Entry in your Project\nThere isn't much documentation around on how to make adjustments to the OpenApiReference element that gets added to your csproj file, so hopefully this post will fill in some of the gaps until documentation is added to the Microsoft Docs site.\nI have based this part of the post on looking through the source code at https://github.com/dotnet/aspnetcore/tree/main/src/Tools/Extensions.ApiDescription.Client and therefore some of my conclusions may be wrong, so proceed with caution if making changes to your project.\nThe main source of information is the Microsoft.Extensions.ApiDescription.Client.props file which defines the XML schema and includes comments that I have used here.\nThe OpenApiReference and OpenApiProjectReference Elements\nThese two elements can be added one or multiple times within an ItemGroup in your csproj file.\nThe main focus of this section is the OpenApiReference element that adds code generation to the current project for a specific OpenApi JSON definition.\nThe OpenApiProjectReference allows external project references to be added as well. More on this below,\nThe following attributes and sub-elements are the main areas of interest within the OpenApiReference element.\nThe props file makes references to other properties that live outside of the element that you can override within your csproj file.\nAs I haven't used the TypeScript generator, I have focussed my commentary on the NSwagCSharp code generator.\nInclude Attribute (Required)\nThe contents of the Include attribute will depend on which element you are in.\nFor OpenApiReference this will be the path to the OpenApi/Swagger json file that will be the source the code generator will use.\nFor OpenApiProjectReference this will be the path to another project that is being referenced.\nClassName Element (Optional)\nThis is the name to give the class that will be generated. If not specified, the class will default to the name given in the OutputPath parameter (see below)\nCodeGenerator Element (Required)\nThe default value is 'NSwagCSharp'. This points to the NSwag C# client generator, more details of which below.\nAt time of writing, only C# and TypeScript are supported, and the value here must end with either \"CSharp\" or \"TypeScript\". Builds will invoke a MSBuild target named \"Generate(CodeGenerator)\" to do actual code generation. More on this below.\nNamespace Element (Optional)\nThis is the namespace to assign the generated class to. If not specified, the RootNamespace entry from your project will be used to put the class within your project's namespace. You may choose to be more specific with the NSwag specific commands below.\nOptions Element (Optional)\nThese are the customisation instructions that will be passed to the code generator as command line options. See Customising The Generated Code with NSwag Commands below for details about usage with the NSwagCSharp generator\nOne of the problems I have been having with this element is that the contents are passed to the command line of the NSwagCSharp as-is and therefore you cannot include line breaks to make it more readable.\nIt would be nice if there was a new element that allows each command option to be listed as an XML sub-element in its own right that the MSBuild target concatenates and parses into the single command line to make editing the csproj file a bit easier.\n\nOutputPath (Optional)\nThis is the path to place generated code into. It is up to the code generator as to whether to interpret the path as a filename or as a directory.\nThe Default filename or folder name is %(Filename)Client.[cs|ts].\nFilenames and relative paths (if explicitly set) are combined with $(OpenApiCodeDirectory). Final value is likely to be a path relative to the client project.\nGlobalPropertiesToRemove (OpenApiProjectReference Only - Optional)\nThis is a semicolon-separated list of global properties to remove in a ProjectReference item created for the OpenApiProjectReference. These properties, along with Configuration, Platform, RuntimeIdentifier and TargetFrameworks, are also removed when invoking the 'OpenApiGetDocuments' target in the referenced project.\nOther Property Elements\nIn the section above, there are references to other properties that get set within the props file.\nThe properties can be overridden within your csproj file, so for completeness, I have added some commentary here\nOpenApiGenerateCodeOptions\nThe Options element above if not populated defaults to the contents of this element, which in of itself is empty by default.\nAs per my comment above for Options, this suffers the same problem of all command values needing to be on the same single line.\nOpenApiGenerateCodeOnBuild\nIf this is set to 'true' (the default), code is generated for the OpenApiReference element and any OpenApiProjectReference items before the BeforeCompile target is invoked.\nHowever, it may be that you do not want the generated called on every single build as you may have set up a CI pipeline where the target is explicitly invoked (via a command line or as a build target) as a build step before the main build. In that case, the value can be set to 'false'\nOpenApiGenerateCodeAtDesignTime\nSimilar to OpenApiGenerateCodeOnBuild above, but this time determines whether to generate code at design time as well as being part of a full build. This is set to true by default.\nOpenApiBuildReferencedProjects\nIf set to 'true' (the default), any projects referenced in an ItemGroup containing one or many OpenApiProjectReference elements will get built before retrieving that project's OpenAPI documents list (or generating code).\nIf set to 'false', you need to ensure the referenced projects are built before the current project within the solution or through other means (such as a build pipeline) but IDEs such as Visual Studio and Rider may get confused about the project dependency graph in this case.\nOpenApiCodeDirectory\nThis is the default folder to place the generated code into. The value is interpreted relative to the project folder, unless already an absolute path. This forms part of the default OutputPath within the OpenApiReference above and the OpenApiProjectReference items.\nThe default value for this is BaseIntermediateOutputPath which is set elsewhere in your csproj file or is implicitly set by the SDK.\nCustomising The Generated Code with NSwag Commands\nHere we get to the main reason I have written this post.\nThere is a huge amount of customisation that you can do to craft the generated code into a shape that suits you.\nThe easiest way to get an understanding of the levels of customisation is to use NSwag Studio to pay around with the various customisation options and see how the options affect the generated code.\nPreviously when I have been using the NSwag MSBuild task, I have pointed the task to an NSwag configuration json file saved from the NSwag Studio and let the build process get on with the job of doing the code generation as a pre-build task.\nHowever, the OpenApiReference task adds a layer of abstraction that means that just using the NSwag configuration file is not an option. Instead, you need to pass the configuration as command line parameters via the  element.\nThis can get a bit hairy for a couple of reasons.\n\nFirstly, each command has to be added to one single line which can make your csproj file a bit unwieldy to view if you have a whole load of customisations that you want to make (scroll right, scroll right, scroll right again!)\nSecondly, you need to know all the NSwag commands and the associated syntax to pass these to the Options element.\n\nOptions Syntax\nEach option that you want to pass takes the form of a command line parameter which\n\nstarts with a forward slash\nfollowed by the command\nthen a colon and then\nthe value to pass to the command\n\nSo, something like this: /ClientBaseClass:ClientBase\nThe format of the value depends on the value type of the command of which there are three common ones\n\nboolean values are set with true or false. E.g. /GenerateOptionalParameters:true\nstring values are set with the string value as-is. E.g. /ClassStyle:Poco\nstring arrays are comma delimited lists of string values. E.g. /AdditionalNamespaceUsages:MyNamespace1,MyNamespace2,MyNamespace3\n\nThe following table is a GitHub gist copy from the GitHub repository I have set up for this and which I plan to update over time as I get a better understanding of each command and its effect on the generated code.\nAt time of writing, many of the descriptions have been lifted from the XML comments in the code from the NSwag repository on GitHub.\n(Apologies the format of the imported markdown here is not great. I hope to make this a bit better later when I can find the time. You may want to go direct to the gist directly)\n\n\nConclusion\nThe new tooling makes the code generation build process itself a lot simpler, but there are a few hoops to jump through to customise the code generated.\nI've been very impressed with the tooling and I look forward to seeing how to it progresses in the future.\nI hope that this blog is of help to anyone else who wants to understand more about the customisation of the OpenApiReference tooling and plugs a gap in the lack of documentation currently available."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/middleware-styles",
    "raw": "<p>In this post, I discuss the differences between convention and factory styles of writing middleware in ASP.NET Core along with the differences in how the instances are created and interact with dependency injection.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>If you have been using ASP.NET Core for a while, you are probably familiar with the concept of middleware. If not, I would recommend reading the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware\" target=\"_blank\" class=\"externallink\">Microsoft Docs page</a> that provides an overview of how middleware conceptually works</p>\n<p>In this post, I will be delving deeper into how middleware is added into the request-response pipeline with references to the code in <a href=\"https://source.dot.net/#Microsoft.AspNetCore.Http.Abstractions/Extensions/UseMiddlewareExtensions.cs\" target=\"_blank\" class=\"externallink\">UseMiddlewareExtensions.cs.</a></p>\n<hr>\n<p>The link I have used here is to the excellent <a href=\"https://source.dot.net/\" target=\"_blank\" class=\"externallink\">https://source.dot.net/</a> &nbsp;web site where you can easily search for .NET Core/5 code by type or member name instead of trawling through the ASP.NET Core GitHub repos.</p>\n<!--?# LinkSizedImage \"SourceBrowser.png\" Alt=\"Source Browser screen shot\" /?-->\n<hr>\n<p>Before delving into the mechanics of how the pipeline is built and works, lets start with how the middleware gets registered with your application.</p>\n<h2 id=\"registering-your-own-custom-middleware\"><a name=\"registering-your-own-custom-middleware\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#registering-your-own-custom-middleware\">Registering Your Own Custom Middleware</a></h2>\n<p>When you need to add middleware to your ASP.NET Core application, it is usually done within your Startup class in the Configure method.</p>\n<p>There are three main ways of registering middleware in the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/startup?the-configure-method\" target=\"_blank\" class=\"externallink\">Configure method</a>, namely by using the generic and non-generic UseMiddleware extension methods and lastly the Use method on <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.iapplicationbuilder\">IApplicationBuilder</a>.</p>\n<p>Let's look at each of these in a bit more detail in order of ease of use (which also happens to be a top down order of execution).</p>\n<h3 id=\"usemiddleware\"><a name=\"use-middleware-t-middleware>\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#use-middleware-t-middleware\">UseMiddleware<tmiddleware></tmiddleware></a></h3>\n<p>In most cases, you will be encapsulating your middleware into a class which adheres to either a convention or an interface (more on this in a bit). This allows you to reuse your middleware code if it is in its own class library project.</p>\n<p>The simplest way to register your middleware class within the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/startup?the-configure-method\" target=\"_blank\" class=\"externallink\">Configure method</a> is to use the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.usemiddlewareextensions.usemiddleware?#Microsoft_AspNetCore_Builder_UseMiddlewareExtensions_UseMiddleware__1_Microsoft_AspNetCore_Builder_IApplicationBuilder_System_Object___\" target=\"_blank\" class=\"externallink\">UseMiddleware<tmiddleware></tmiddleware></a> extension method on the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.iapplicationbuilder\" target=\"_blank\" class=\"externallink\">IApplicationBuilder</a> instance that is passed into the Startup's Configure method.</p>\n<p>To call this method, you need to supply a generic parameter <tmiddleware> that is set to the type of your middleware class as shown here.</tmiddleware></p>\n<!--?# Gist 9a9903d6ec7df13c122c07c86bd54a55 /?-->\n<p>The method has an optional parameter (args) for passing in an array of objects that represent parameters that can be passed into the constructor of your class. This can be of use when not all of the constructor parameters can be resolved by dependency injection.</p>\n<p>Behind the scenes, this method makes a call to the non-generic UseMiddleware method. To do this, the generic method gets a Type instance from the generic type parameter and passes it (with the optional args array if present) to the non-generic method which does the hard work.</p>\n<!--?# Gist f7339687ee19e1cac2194dc3ae191a18 /?-->\n<h3 id=\"usemiddleware-non-generic-version\"><a name=\"use-middleware-non-generic version\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#use-middleware-non-generic%20version\">UseMiddleware (Non-Generic Version)</a></h3>\n<p>Most of the time, you will be using the generic version, but it is worth knowing there is a non-generic version if you need to derive the type at runtime.</p>\n<!--?# Gist a8910f046ba34558e3534e5f202dfe0b /?-->\n<p>Let's look at the start of the method to see what is going on</p>\n<!--?# Gist 9a7aa803812b09aa4a767b06b46de2df /?-->\n<p>In this method, the code checks to see if the middleware type passed in as a parameter implements the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.imiddleware\" target=\"_blank\" class=\"externallink\">IMiddleware</a> interface. If it does, the method branches off to use a&nbsp; middleware factory that will use dependency injection to create the instance of the middleware class</p>\n<p>Otherwise, it makes a call to the IApplicationBuilder's Use method by passing in an anonymous function that uses reflection to create the required request delegate (not show here as it is a lot of code - if interested, look <a href=\"https://source.dot.net/#Microsoft.AspNetCore.Http.Abstractions/Extensions/UseMiddlewareExtensions.cs\">here</a>).</p>\n<h3 id=\"the-iapplicationbuilder-use-method\"><a name=\"the-i-applicationbuilder-use-method\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-i-applicationbuilder-use-method\">The IApplicationBuilder Use Method</a></h3>\n<p>The <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.iapplicationbuilder.use?view=aspnetcore-5.0#Microsoft_AspNetCore_Builder_IApplicationBuilder_Use_System_Func_Microsoft_AspNetCore_Http_RequestDelegate_Microsoft_AspNetCore_Http_RequestDelegate__\" target=\"_blank\" class=\"externallink\">IApplicationBuilder.Use</a> method is the 'raw' way of registering middleware. The method takes a single parameter of type</p>\n<p>Func&lt;RequestDelegate, RequestDelegate&gt;</p>\n<p>This delegate may be a method or an anonymous function created by a lambda expression, but must adhere to the delegate signature of</p>\n<p>Task RequestDelegate(HttpContext context)</p>\n<!--?# Gist 81faa118727e689560c78464abb38b0e /?-->\n<p>In this example from the Microsoft Docs web site, we are not actually doing anything, but it gives an example of an implementation where you may want to intercept the pipeline and do something before and/or after the next middleware in the pipeline is executed (by the next.Invoke() in this example).</p>\n<p>What is interesting is seeing how the RequestDelegate is expressed as an anonymous function.</p>\n<p>The context parameter gives us access to an HttpContext instance which in turn gives us access to the request and response objects. However, you must not&nbsp; make any changes to the response if another middleware is to be invoked as once the response has started (by a later middleware in the pipeline), any changes to it will cause an exception.</p>\n<p>If you do want to return a response, your middleware becomes a terminating or a short-circuiting middleware and does not invoke any further middlewares in the pipeline.</p>\n<hr>\n<p>In this post I want to keep the focus on understanding how middleware works with dependency injection lifetimes rather than the mechanics of how the middleware pipeline itself is built and executes each pipeline middleware in turn .</p>\n<p>For a deeper dive into how request delegates interact with each other to form a pipeline, I highly recommend <a href=\"https://twitter.com/stevejgordon\">Steve Gordon's</a>&nbsp; <a href=\"https://www.stevejgordon.co.uk/how-is-the-asp-net-core-middleware-pipeline-built\">Deep Dive - How Is The Asp Net Core Middleware Pipeline Built?&nbsp;</a> which goes deep into the chaining of request delegates.</p>\n<hr>\n<h2 id=\"middleware-styles-in-line-vs-factory-vs-convention\"><a name=\"middleware styles-in-line-vs-factory-vs-convention\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#middleware%20styles-in-line-vs-factory-vs-convention\">Middleware Styles : In-Line vs Factory vs Convention</a></h2>\n<p>Ultimately, all middleware eventually boils down to becoming a request delegate function that gets passed into the Use method on the IApplicationBuilder implementation instance.</p>\n<h3 id=\"in-line-middleware\"><a name=\"in-line-middleware\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#in-line-middleware\">In-Line Middleware</a></h3>\n<p>You can write your middleware directly inside an anonymous function (or method) that gets passed to the Use method via a delegate parameter. The call to the Use method is written inside the Configure method of the Startup class (on via the Configure method on the HostBuilder).</p>\n<p>This is the approach you usually take if want to do something simple in your middleware that has no dependencies&nbsp; that would need to be resolved by the dependency injection container as the delegate does not interact with the container.</p>\n<p>That does not mean that you cannot get the container to resolve instances for you. It just means that you have to explicitly ask the container to resolve a service via the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httpcontext.requestservices\" target=\"_blank\" class=\"externallink\">RequestServices property on the HttpContext</a> instance (that is passed as a parameter into the delegate).</p>\n<p>This moves you into the realms of the service locater anti-pattern, but given that you are usually creating the delegate within the confines of the&nbsp; application startup, this is not so much of a concern as doing it elsewhere in your application landscape.</p>\n<!--?# Gist caf86c44fafa4d9e29747394726f1167 /?-->\n<p>As the code is all written in-line, it can become a bit of a pain to read and debug if it is doing many things or you have multiple Use entries as the Configure method becomes a but unwieldy.</p>\n<p>To avoid this, you could extract the contents of the Use anonymous function to their own methods within the Startup. However this still limits your middleware to the confines of your project.</p>\n<p>in most cases, you will want to make the middleware code self-contained and reusable and take advantage of getting the dependency injection container do the work of resolving instances without having to explicitly call the container (as we've had to in the above example).</p>\n<p>This is where the other two styles of writing middleware come into their own by writing classes that encapsulate the functionality that will be called by the pipeline.</p>\n<h3 id=\"factory-style-middleware\"><a name=\"factory-style-middleware\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#factory-style-middleware\">Factory Style Middleware</a></h3>\n<p>Now I have a confession. I have been using .NET Core for quite a while now and, until recently, this had passed me by.</p>\n<p>This may be because I learnt about middleware with ASP.NET Core 1.1 and factory style middleware was not a 'thing' until ASP.NET Core 2.0,&nbsp; when it was introduced without any fanfare (it was not included in the What's New in ASP.NET Core for either version <a href=\"https://docs.microsoft.com/en-us/aspnet/core/release-notes/aspnetcore-2.0\" target=\"_blank\" class=\"externallink\">2.0</a> or <a href=\"https://docs.microsoft.com/en-us/aspnet/core/release-notes/aspnetcore-2.1\" target=\"_blank\" class=\"externallink\">2.1</a>).</p>\n<p>It was not until I recently read <a href=\"https://khalidabuhakmeh.com/aspnet-core-middleware-registration\" target=\"_blank\" class=\"externallink\">Khalid Abuhakmeh's blog post</a> that then led be to read the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/extensibility\" target=\"_blank\" class=\"externallink\">this Microsoft Docs page</a>&nbsp;that&nbsp;I even became aware of factory style middleware.</p>\n<hr>\n<p>If you are interested in how the the factory style was introduced into ASP.NET Core, have a look at <a href=\"https://github.com/aspnet/HttpAbstractions/issues/754\" target=\"_blank\" class=\"externallink\">this Git issue</a> which shows how it evolved with a lot of commentary from David Fowler</p>\n<hr>\n<p>The introduction of factory style middleware brings the following benefits over convention style middleware:</p>\n<ul>\n<li>Activation per request, so allows injection of scoped services into the constructor</li>\n<li>The explicitly defined InvokeAsync method on the interface moves away from the unstructured Invoke/InvokeAsync methods in convention style middleware that allowed additional parameters to be added</li>\n<li>It is more aligned with the way you write other classes in your application as it is based on the premise of constructor injection over method parameter injection</li>\n<li>Whilst there is a <a href=\"https://source.dot.net/#Microsoft.AspNetCore.Http/MiddlewareFactory.cs,a9293f5a0e80da4f\" target=\"_blank\" class=\"externallink\">default implementation</a> of <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.imiddlewarefactory\" target=\"_blank\" class=\"externallink\">IMiddlewareFactory</a> that gets registered by default by the default <a href=\"https://source.dot.net/#Microsoft.AspNetCore.Hosting/GenericHost/GenericWebHostBuilder.cs,26b824c769468238\" target=\"_blank\" class=\"externallink\">WebHostBuilder</a>, you can write your own implementation of <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.imiddlewarefactory\" target=\"_blank\" class=\"externallink\">IMiddlewareFactory</a> that may be of use if you are using another container such as AutoFac where you may want to resolve instances using functionality that is not present in the out-of-the-box Microsoft implementation of IServiceProvider</li>\n</ul>\n<p>So how do we implement the factory style of middleware ?</p>\n<p>The key to this is by writing your middleware class as an implementation of the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.imiddleware\" target=\"_blank\" class=\"externallink\">IMiddleware</a> interface which has a single method, InvokeAsync.</p>\n<p>The method has two incoming parameters</p>\n<ul>\n<li>an instance of&nbsp; HttpContext&nbsp; this holds all the request/response information</li>\n<li>a RequestDelegate instance that provides the link to the next middleware in the pipeline to call</li>\n</ul>\n<p>Inside the method is where you can do whatever it is that you need to do in your custom middleware. This may be&nbsp; either</p>\n<ul>\n<li>intercepting the request-response chain to do something before and after the next middleware in the chain, such as logging or manipulating the request (or response), or</li>\n<li>acting as terminating middleware that sends a response back (such as the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.staticfiles.staticfilemiddleware\" target=\"_blank\" class=\"externallink\">static file middleware</a>) and therefore does not proceed to the next middleware in the pipeline (unless it cannot handle the request).</li>\n</ul>\n<p>Lastly, if you are not terminating the pipeline in your middleware, you need to ensure that the request delegate is invoked, passing the HttpContext instance to it as a parameter.</p>\n<!--?# Gist caf976b73059f6095e6ca4c5d7f772be /?-->\n<p>In the above example, I have taken the in-line style we used earlier and refactored it into its own factory-style middleware by implementing the IMiddleware interface. By doing this, I could put it into its own project and share between multiple other projects, avoiding the repetition of the in-line style.</p>\n<p>We also benefit from not having to get the instance of ILogger<t> from the context parameter (so avoiding the service locator anti-pattern) and also have a proper type to use with the logger (as the Startup type felt wrong in the in-line style).</t></p>\n<p>To use the factory style middleware, there are two things that need to be done to use it in your application.</p>\n<!--?# Gist 6d559be83d6c67268f8c6058be31da91 /?-->\n<p>The first, as with all middleware (regardless of style), is to register the middleware in the Configure method of your Startup class as discussed above (alternatively, you may want to use the Configure method directly on the host builder)</p>\n<p>The next step is to go back up in to the ConfigureServices method (either in the StartUp class or directly on the HostBuilder) and register the middleware class with the dependency injection container.</p>\n<p>To do this, you need to adhere to two simple rules</p>\n<ol>\n<li>You need to register the middleware with the concrete type as the service, not the IMiddleware interface (or any other interface it may implement)</li>\n<li>Think very carefully lifetime of the registration - for now, we will assume that the lifetime will be scoped (as opposed to transient or singleton) as the purpose of factory middleware is that is is invoked per request.</li>\n</ol>\n<p>The second point is important as you could be in danger of creating a captured dependency if you register your middleware as a singleton, but the service types of the parameters in the constructor have shorter lifetimes.</p>\n<h3 id=\"convention-style-middleware\"><a name=\"convention-style-middleware\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#convention-style-middleware\">Convention Style Middleware</a></h3>\n<p>Convention style middleware is the way you will find most examples are written and indeed how most Microsoft written middleware works (and as per the comments in the Git issue, remains for backward compatibility).</p>\n<p>As it is well documented in many places on the internet (and of course, in the <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/write\">Microsoft Docs - see Writing Custom ASP.NET Core Middleware</a>), I will concentrate on the key differences with factory style middleware here</p>\n<!--?# Gist ca0703e5b58e26ba6fb18997b27902b8 /?-->\n<p>The first obvious difference is that the class does not have to implement a specific interface (E.g. IMiddleware). Instead, you expected to adhere to a convention of implementing one of two methods Invoke or InvokeAsync.</p>\n<ul>\n<li>Both methods have the same signature, the naming choice is up to you, though given that both return a Task, it is usual to append Async to asynchronous method names</li>\n<li>You cannot have both Invoke and InvokeAsync methods - this will cause an exception to be thrown</li>\n<li>The first parameter must be of type HttpContext - if this is not present, an exception will the thrown</li>\n<li>The return type must be a Task</li>\n</ul>\n<p>So given that the RequestDelegate for the next delegate is no longer passed into the InvokeAsync as a parameter, we need to obtain it from somewhere else. As we are adhering to a convention, we should stick with constructor injection and have the next delegate injected here.</p>\n<p>We are still missing our ILogger<loguseragentconventionalmiddleware> instance, but this is where things get a bit more complex.</loguseragentconventionalmiddleware></p>\n<p>When it comes to interactions with dependency injection, the key thing to be aware of is that the instance of our convention-style middleware class is <em>not created by the dependency injection container,</em> even if you register the class in ConfigureServices. Instead, the UseMiddleware extension method uses a combination of reflection and the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.activatorutilities\">ActivatorUtilities</a> class to create the instance - <em>once, and only once!</em> - so it is effectively a singleton.</p>\n<p>The reason for this is that the code that is of interest to the middleware pipeline is the Invoke/InvokeAsync method, as it is a call to this will be wrapped inside the anonymous function that gets passed to the Use method in the IApplicationBuilder instance.&nbsp;In other words, creating the class instance is a stepping stone to creating the delegate and once created, the class constructor is never interacted with again.</p>\n<p>Why does this matter? It comes back to understanding how we obtain dependencies in our custom middleware.</p>\n<p>If we specified dependencies in the constructor that have been registered with shorter lifetime that singleton (transient and scoped), we end up with captured dependencies that are locked until the singleton is released (which in the case of middleware is when the web application shuts down).</p>\n<p>If you require transient or scoped dependencies, these should be added as parameters to the Invoke/InvokeAsync method.</p>\n<p>When the UseMiddleware scans the class through reflection, it does several things such as validating the class adheres to the expected conventions, but the main thing we are interested in is the mapping of our middleware's Invoke or InvokeAsync method to the RequestDelegate signature required by the ApplicationBuilder's Use method.</p>\n<p>The mapping is decided by checking if the signature of the Invoke/InvokeAsync method exactly matches the RequestDelegate signature (e.g. it only requires a single parameter of type HttpContext), it will use the method directly as the RequestDelegate function</p>\n<p>Otherwise, it will create a wrapping function that matches the RequestDelegate signature, but that then uses a the ActivatorUtilities class to&nbsp; resolve the method parameters from the dependency injection container accessed via the ApplicationBuilder's ApplicationServices property.</p>\n<p><a href=\"https://source.dot.net/#Microsoft.AspNetCore.Http.Abstractions/Extensions/UseMiddlewareExtensions.cs,e940dbf3ad65ffe4\" target=\"_blank\" class=\"externallink\">You can view this from the source code here</a></p>\n<p>At this point, once the UseMiddleware has mapped the RequestDelegate that represents our class's middleware invocation method into the ApplicationBuilder, our middleware is baked into the pipeline.</p>\n<h2 id=\"comparison\"><a name=\"comparison\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#comparison\">Comparison</a></h2>\n<p>Whether you use convention or factory based middleware is a design decision to be made based on what your middleware is trying to achieve and what dependencies it needs resolved from the dependency injection container.</p>\n<ul>\n<li><strong>In-line style</strong> is find for 'quick and dirty' middleware that you do not plan to reuse and is either terminating or does very little when intercepting the pipelines with few dependencies</li>\n<li>If there are many scoped or transient dependencies, you may want to consider the <strong>Factory-style approach</strong> for coding simplicity as it aligns with the constructor injection pattern that you are probably more familiar with for getting dependencies from the container and <em>if registered with the correct scope</em>, avoids captured dependencies</li>\n<li>If there are no dependencies, or you know the only dependencies are guaranteed to have&nbsp; a singleton lifetime, you may lean towards <strong>convention-style</strong> middleware as these can be injected into the container when the pipeline is first built and, as there are no additional parameters to the InvokeAsync method, the method can be used as a direct match to the RequestDelegate function that gets used in the pipeline</li>\n<li>If you are already familiar with using&nbsp;<strong>convention-style</strong> middleware and specifying transient and scoped dependencies in the Invoke/InvokeAsync&nbsp; parameter list, there is no pressing need to change to the factory-style approach.</li>\n</ul>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>I hope this post has been of use. If so, please spread the word by linking to it on Twitter and mentioning me <a href=\"https://twitter.com/stevetalkscode\" target=\"_blank\" class=\"externallink\">@stevetalkscode</a>.</p>\n<p>I have created a demo solution at <a href=\"https://github.com/stevetalkscode/middlewarestyles\" target=\"_blank\" class=\"externallink\">https://github.com/stevetalkscode/middlewarestyles</a>&nbsp; which you can download and have a play with the different styles of writing middleware and see the effects of different dependency injection lifetime registrations for the factory style vs the singleton captured in conventional style middleware.</p>\n<p>I plan to revisit this topic in a future post to dig deeper into the how the different styles can affect start up and request performance and also the memory usage (which in turn affects garbage collection) which may sway the decision between using factory or convention style way of writing middleware.</p>\n",
    "sanitized": "In this post, I discuss the differences between convention and factory styles of writing middleware in ASP.NET Core along with the differences in how the instances are created and interact with dependency injection.\nBackground\nIf you have been using ASP.NET Core for a while, you are probably familiar with the concept of middleware. If not, I would recommend reading the Microsoft Docs page that provides an overview of how middleware conceptually works\nIn this post, I will be delving deeper into how middleware is added into the request-response pipeline with references to the code in UseMiddlewareExtensions.cs.\n\nThe link I have used here is to the excellent https://source.dot.net/  web site where you can easily search for .NET Core/5 code by type or member name instead of trawling through the ASP.NET Core GitHub repos.\n\n\nBefore delving into the mechanics of how the pipeline is built and works, lets start with how the middleware gets registered with your application.\nRegistering Your Own Custom Middleware\nWhen you need to add middleware to your ASP.NET Core application, it is usually done within your Startup class in the Configure method.\nThere are three main ways of registering middleware in the Configure method, namely by using the generic and non-generic UseMiddleware extension methods and lastly the Use method on IApplicationBuilder.\nLet's look at each of these in a bit more detail in order of ease of use (which also happens to be a top down order of execution).\nUseMiddleware\nIn most cases, you will be encapsulating your middleware into a class which adheres to either a convention or an interface (more on this in a bit). This allows you to reuse your middleware code if it is in its own class library project.\nThe simplest way to register your middleware class within the Configure method is to use the UseMiddleware extension method on the IApplicationBuilder instance that is passed into the Startup's Configure method.\nTo call this method, you need to supply a generic parameter  that is set to the type of your middleware class as shown here.\n\nThe method has an optional parameter (args) for passing in an array of objects that represent parameters that can be passed into the constructor of your class. This can be of use when not all of the constructor parameters can be resolved by dependency injection.\nBehind the scenes, this method makes a call to the non-generic UseMiddleware method. To do this, the generic method gets a Type instance from the generic type parameter and passes it (with the optional args array if present) to the non-generic method which does the hard work.\n\nUseMiddleware (Non-Generic Version)\nMost of the time, you will be using the generic version, but it is worth knowing there is a non-generic version if you need to derive the type at runtime.\n\nLet's look at the start of the method to see what is going on\n\nIn this method, the code checks to see if the middleware type passed in as a parameter implements the IMiddleware interface. If it does, the method branches off to use a  middleware factory that will use dependency injection to create the instance of the middleware class\nOtherwise, it makes a call to the IApplicationBuilder's Use method by passing in an anonymous function that uses reflection to create the required request delegate (not show here as it is a lot of code - if interested, look here).\nThe IApplicationBuilder Use Method\nThe IApplicationBuilder.Use method is the 'raw' way of registering middleware. The method takes a single parameter of type\nFunc<RequestDelegate, RequestDelegate>\nThis delegate may be a method or an anonymous function created by a lambda expression, but must adhere to the delegate signature of\nTask RequestDelegate(HttpContext context)\n\nIn this example from the Microsoft Docs web site, we are not actually doing anything, but it gives an example of an implementation where you may want to intercept the pipeline and do something before and/or after the next middleware in the pipeline is executed (by the next.Invoke() in this example).\nWhat is interesting is seeing how the RequestDelegate is expressed as an anonymous function.\nThe context parameter gives us access to an HttpContext instance which in turn gives us access to the request and response objects. However, you must not  make any changes to the response if another middleware is to be invoked as once the response has started (by a later middleware in the pipeline), any changes to it will cause an exception.\nIf you do want to return a response, your middleware becomes a terminating or a short-circuiting middleware and does not invoke any further middlewares in the pipeline.\n\nIn this post I want to keep the focus on understanding how middleware works with dependency injection lifetimes rather than the mechanics of how the middleware pipeline itself is built and executes each pipeline middleware in turn .\nFor a deeper dive into how request delegates interact with each other to form a pipeline, I highly recommend Steve Gordon's  Deep Dive - How Is The Asp Net Core Middleware Pipeline Built?  which goes deep into the chaining of request delegates.\n\nMiddleware Styles : In-Line vs Factory vs Convention\nUltimately, all middleware eventually boils down to becoming a request delegate function that gets passed into the Use method on the IApplicationBuilder implementation instance.\nIn-Line Middleware\nYou can write your middleware directly inside an anonymous function (or method) that gets passed to the Use method via a delegate parameter. The call to the Use method is written inside the Configure method of the Startup class (on via the Configure method on the HostBuilder).\nThis is the approach you usually take if want to do something simple in your middleware that has no dependencies  that would need to be resolved by the dependency injection container as the delegate does not interact with the container.\nThat does not mean that you cannot get the container to resolve instances for you. It just means that you have to explicitly ask the container to resolve a service via the RequestServices property on the HttpContext instance (that is passed as a parameter into the delegate).\nThis moves you into the realms of the service locater anti-pattern, but given that you are usually creating the delegate within the confines of the  application startup, this is not so much of a concern as doing it elsewhere in your application landscape.\n\nAs the code is all written in-line, it can become a bit of a pain to read and debug if it is doing many things or you have multiple Use entries as the Configure method becomes a but unwieldy.\nTo avoid this, you could extract the contents of the Use anonymous function to their own methods within the Startup. However this still limits your middleware to the confines of your project.\nin most cases, you will want to make the middleware code self-contained and reusable and take advantage of getting the dependency injection container do the work of resolving instances without having to explicitly call the container (as we've had to in the above example).\nThis is where the other two styles of writing middleware come into their own by writing classes that encapsulate the functionality that will be called by the pipeline.\nFactory Style Middleware\nNow I have a confession. I have been using .NET Core for quite a while now and, until recently, this had passed me by.\nThis may be because I learnt about middleware with ASP.NET Core 1.1 and factory style middleware was not a 'thing' until ASP.NET Core 2.0,  when it was introduced without any fanfare (it was not included in the What's New in ASP.NET Core for either version 2.0 or 2.1).\nIt was not until I recently read Khalid Abuhakmeh's blog post that then led be to read the this Microsoft Docs page that I even became aware of factory style middleware.\n\nIf you are interested in how the the factory style was introduced into ASP.NET Core, have a look at this Git issue which shows how it evolved with a lot of commentary from David Fowler\n\nThe introduction of factory style middleware brings the following benefits over convention style middleware:\n\nActivation per request, so allows injection of scoped services into the constructor\nThe explicitly defined InvokeAsync method on the interface moves away from the unstructured Invoke/InvokeAsync methods in convention style middleware that allowed additional parameters to be added\nIt is more aligned with the way you write other classes in your application as it is based on the premise of constructor injection over method parameter injection\nWhilst there is a default implementation of IMiddlewareFactory that gets registered by default by the default WebHostBuilder, you can write your own implementation of IMiddlewareFactory that may be of use if you are using another container such as AutoFac where you may want to resolve instances using functionality that is not present in the out-of-the-box Microsoft implementation of IServiceProvider\n\nSo how do we implement the factory style of middleware ?\nThe key to this is by writing your middleware class as an implementation of the IMiddleware interface which has a single method, InvokeAsync.\nThe method has two incoming parameters\n\nan instance of  HttpContext  this holds all the request/response information\na RequestDelegate instance that provides the link to the next middleware in the pipeline to call\n\nInside the method is where you can do whatever it is that you need to do in your custom middleware. This may be  either\n\nintercepting the request-response chain to do something before and after the next middleware in the chain, such as logging or manipulating the request (or response), or\nacting as terminating middleware that sends a response back (such as the static file middleware) and therefore does not proceed to the next middleware in the pipeline (unless it cannot handle the request).\n\nLastly, if you are not terminating the pipeline in your middleware, you need to ensure that the request delegate is invoked, passing the HttpContext instance to it as a parameter.\n\nIn the above example, I have taken the in-line style we used earlier and refactored it into its own factory-style middleware by implementing the IMiddleware interface. By doing this, I could put it into its own project and share between multiple other projects, avoiding the repetition of the in-line style.\nWe also benefit from not having to get the instance of ILogger from the context parameter (so avoiding the service locator anti-pattern) and also have a proper type to use with the logger (as the Startup type felt wrong in the in-line style).\nTo use the factory style middleware, there are two things that need to be done to use it in your application.\n\nThe first, as with all middleware (regardless of style), is to register the middleware in the Configure method of your Startup class as discussed above (alternatively, you may want to use the Configure method directly on the host builder)\nThe next step is to go back up in to the ConfigureServices method (either in the StartUp class or directly on the HostBuilder) and register the middleware class with the dependency injection container.\nTo do this, you need to adhere to two simple rules\n\nYou need to register the middleware with the concrete type as the service, not the IMiddleware interface (or any other interface it may implement)\nThink very carefully lifetime of the registration - for now, we will assume that the lifetime will be scoped (as opposed to transient or singleton) as the purpose of factory middleware is that is is invoked per request.\n\nThe second point is important as you could be in danger of creating a captured dependency if you register your middleware as a singleton, but the service types of the parameters in the constructor have shorter lifetimes.\nConvention Style Middleware\nConvention style middleware is the way you will find most examples are written and indeed how most Microsoft written middleware works (and as per the comments in the Git issue, remains for backward compatibility).\nAs it is well documented in many places on the internet (and of course, in the Microsoft Docs - see Writing Custom ASP.NET Core Middleware), I will concentrate on the key differences with factory style middleware here\n\nThe first obvious difference is that the class does not have to implement a specific interface (E.g. IMiddleware). Instead, you expected to adhere to a convention of implementing one of two methods Invoke or InvokeAsync.\n\nBoth methods have the same signature, the naming choice is up to you, though given that both return a Task, it is usual to append Async to asynchronous method names\nYou cannot have both Invoke and InvokeAsync methods - this will cause an exception to be thrown\nThe first parameter must be of type HttpContext - if this is not present, an exception will the thrown\nThe return type must be a Task\n\nSo given that the RequestDelegate for the next delegate is no longer passed into the InvokeAsync as a parameter, we need to obtain it from somewhere else. As we are adhering to a convention, we should stick with constructor injection and have the next delegate injected here.\nWe are still missing our ILogger instance, but this is where things get a bit more complex.\nWhen it comes to interactions with dependency injection, the key thing to be aware of is that the instance of our convention-style middleware class is not created by the dependency injection container, even if you register the class in ConfigureServices. Instead, the UseMiddleware extension method uses a combination of reflection and the ActivatorUtilities class to create the instance - once, and only once! - so it is effectively a singleton.\nThe reason for this is that the code that is of interest to the middleware pipeline is the Invoke/InvokeAsync method, as it is a call to this will be wrapped inside the anonymous function that gets passed to the Use method in the IApplicationBuilder instance. In other words, creating the class instance is a stepping stone to creating the delegate and once created, the class constructor is never interacted with again.\nWhy does this matter? It comes back to understanding how we obtain dependencies in our custom middleware.\nIf we specified dependencies in the constructor that have been registered with shorter lifetime that singleton (transient and scoped), we end up with captured dependencies that are locked until the singleton is released (which in the case of middleware is when the web application shuts down).\nIf you require transient or scoped dependencies, these should be added as parameters to the Invoke/InvokeAsync method.\nWhen the UseMiddleware scans the class through reflection, it does several things such as validating the class adheres to the expected conventions, but the main thing we are interested in is the mapping of our middleware's Invoke or InvokeAsync method to the RequestDelegate signature required by the ApplicationBuilder's Use method.\nThe mapping is decided by checking if the signature of the Invoke/InvokeAsync method exactly matches the RequestDelegate signature (e.g. it only requires a single parameter of type HttpContext), it will use the method directly as the RequestDelegate function\nOtherwise, it will create a wrapping function that matches the RequestDelegate signature, but that then uses a the ActivatorUtilities class to  resolve the method parameters from the dependency injection container accessed via the ApplicationBuilder's ApplicationServices property.\nYou can view this from the source code here\nAt this point, once the UseMiddleware has mapped the RequestDelegate that represents our class's middleware invocation method into the ApplicationBuilder, our middleware is baked into the pipeline.\nComparison\nWhether you use convention or factory based middleware is a design decision to be made based on what your middleware is trying to achieve and what dependencies it needs resolved from the dependency injection container.\n\nIn-line style is find for 'quick and dirty' middleware that you do not plan to reuse and is either terminating or does very little when intercepting the pipelines with few dependencies\nIf there are many scoped or transient dependencies, you may want to consider the Factory-style approach for coding simplicity as it aligns with the constructor injection pattern that you are probably more familiar with for getting dependencies from the container and if registered with the correct scope, avoids captured dependencies\nIf there are no dependencies, or you know the only dependencies are guaranteed to have  a singleton lifetime, you may lean towards convention-style middleware as these can be injected into the container when the pipeline is first built and, as there are no additional parameters to the InvokeAsync method, the method can be used as a direct match to the RequestDelegate function that gets used in the pipeline\nIf you are already familiar with using convention-style middleware and specifying transient and scoped dependencies in the Invoke/InvokeAsync  parameter list, there is no pressing need to change to the factory-style approach.\n\nConclusion\nI hope this post has been of use. If so, please spread the word by linking to it on Twitter and mentioning me @stevetalkscode.\nI have created a demo solution at https://github.com/stevetalkscode/middlewarestyles  which you can download and have a play with the different styles of writing middleware and see the effects of different dependency injection lifetime registrations for the factory style vs the singleton captured in conventional style middleware.\nI plan to revisit this topic in a future post to dig deeper into the how the different styles can affect start up and request performance and also the memory usage (which in turn affects garbage collection) which may sway the decision between using factory or convention style way of writing middleware."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/stronglytypedheaders-part1",
    "raw": "<p>In this first part of a series of occasional posts, I discuss the thinking behind taking&nbsp; string based HTTP Headers and presenting them to your .NET code via dependency injection as strongly typed objects.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>If you have read my previous blog posts or seen my talks, you will be aware that I am a big fan of the configuration binding functionality in .NET Core (now .NET 5) that takes string key/value pairs stored in various ways (such as JSON or environmental variables) and binding them to a typed object that is exposed via the .NET Dependency Injection container.</p>\n<p>Recently, I have been giving some thought to other areas in ASP.NET Core that would benefit from this ability to bind string data into object instances, in particular looking at HTTP request headers.</p>\n<h2 id=\"the-iheaderdictionary-interface\"><a name=\"the-iheaderdictionary-interface\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-iheaderdictionary-interface\">The IHeaderDictionary Interface</a></h2>\n<p>In ASP.NET Core, HTTP request headers are presented as an instance of\nthe <span class=\"code-item\"><a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.iheaderdictionary\" target=\"_blank\">IHeaderDictionary</a>\n</span> interface which is primarily a (dictionary) collection of key/value pairs made up of a string value for the key (the header key)\nand the header values within a\n<span class=\"code-item\"><a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.primitives.stringvalues\" target=\"_blank\">StringValues</a></span> struct (which itself is a special collection of strings that \"<em>represents zero/null, one, or many strings in an efficient way</em>\").</p>\n<p>The purpose of the interface is to present a collection of HTTP headers in a consistent manner. HTTP headers can have multiple declarations within the incoming request (see <a href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2\" target=\"_blank\" class=\"externallink\">the RFC specification</a>) which need to be parsed and grouped together by key before being presented in the collection.</p>\n<p>Whilst the interface provides a consistent manner to access the headers, if you want interrogate the incoming request headers, you have a few hoops to jump through, namely</p>\n<ul>\n<li>the HttpContext needs to be directly accessible <span class=\"code-item\"><a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-context?#use-httpcontext-from-a-controller\" target=\"_blank\">in controllers, it is a property on the controller</a></span>, or indirectly via the (singleton) <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.ihttpcontextaccessor\" target=\"_blank\" class=\"externallink\">IHttpContextAccessor</a> (<a href=\"https://adamstorr.azurewebsites.net/blog/are-you-registering-ihttpcontextaccessor-correctly\">which is not registered by default</a>) <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-context?view=aspnetcore-5.0#use-httpcontext-from-custom-components\">when consuming elsewhere</a></li>\n<li>once you have the context, having to navigate to the Request property</li>\n<li>Within the request object, navigate to the Headers property</li>\n</ul>\n<p>In other words you need code to get to <strong>HttpContext.Request.Headers</strong>.</p>\n<p>If you have several headers that you want to access, you have to repeat this process for each of the headers you require.</p>\n<h2 id=\"parsing-and-validating-header-values\"><a name=\"parsing-validating-header-values\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#parsing-validating-header-values\">Parsing and Validating Header Values</a></h2>\n<p>Once you have your value(s) for a header, these are still strings that you may want to convert into some other type such as integer or GUID, which then means having to parse the value(s) which then raises a number of other questions:</p>\n<ul>\n<li>What should be the default if the header is not present?</li>\n<li>What to do if the header is present, but the value is not in the correct format to parse to the required type?</li>\n<li>What to do if only expecting a single value, but multiple values are presented - first or last in wins?</li>\n<li>What to do if expecting multiple values and a single value is presented</li>\n<li>What to do if the value(s) can be parsed, but fail any domain validation that needs to be applied (E.g. integer value must be within a domain range)</li>\n<li>If any of the above cannot be worked around, how can an error be safely marshalled back to the caller without raising an exception?</li>\n</ul>\n<p>In each of these scenarios, there is the potential for an exception to be thrown if assumptions are made about the incoming data and guards are not put in place to handle non-expected scenarios (E.g. using TryParse instead of Parse when converting strings to other primitive types to avoid exceptions when parsing fails).</p>\n<p>Assuming all of the above is working correctly and you have managed to get the value(s) you require out of the headers, there is the question of passing the data from the controller action (or middleware) to some domain model or function.</p>\n<p>Ideally, your domain logic and entities should avoid any bindings to web related types such as IHeaderDictionary &amp; HttpContext. Avoiding this coupling to web concepts means they can be used in other applications (console, desktop or mobile applications) where the values that are received via the headers in a web application/API service may (or may not) be obtained in other ways (user input, configuration or hard coded).</p>\n<h2 id=\"primitive-obsession\"><a name=\"primitive-obsession\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#primitive-obsession\">Primitive Obsession</a></h2>\n<p>I recently read <a href=\"https://andrewlock.net/using-strongly-typed-entity-ids-to-avoid-primitive-obsession-part-1/\" target=\"_blank\" class=\"externallink\">Andrew Lock's blog about primitive obsession</a> where he discusses the danger of passing values around as standard data type (string, integer, GUID et al) as these are interchangeable values that do not have context.</p>\n<p>The post goes on to put forward an implementation of wrapping these values into 'strong types' that give context to a value. E.g. a customer id value expressed as a GUID is interchangeable with a product id that is also a GUID, but an instance of a CustomerIdentity type is not interchangeable with a ProductIdentity.</p>\n<p>After having read Andrew's series that shows how to create strong types that are struct based, I then went to to read <a href=\"https://thomaslevesque.com/2020/10/30/using-csharp-9-records-as-strongly-typed-ids/\" target=\"_blank\" class=\"externallink\">Thomas Levesque's series</a> that is based on Andrew's series, but this time implementing using C# 9 record types in .NET 5.</p>\n<p>I highly recommend reading both of these.</p>\n<p>The principle I want to carry through is that each HTTP header of interest to the application should be mapped into a strongly typed instance to give the value(s) some meaning and context beyond just being an incoming value. In addition, these types should abstract away the need for callers to have any knowledge of HTTP related classes and interfaces.</p>\n<h2 id=\"replacing-commonly-written-code-with-a-library\"><a name=\"replacing-commonly-written-code-with-a-library\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#replacing-commonly-written-code-with-a-library\">Replacing Commonly Written Code with a Library</a></h2>\n<p>With all of the above in mind, I have started a library to remove the need to keep writing a lot of the commonly required code. The requirements I have are</p>\n<ul>\n<li>To remove the need for any of my controller or domain code to need to have knowledge of HTTP headers to retrieve the values that have been supplied in a request. Instead I want strongly typed values to be available from the Dependency Injection container that encapsulate one or more pieces of data that have some meaning to the application</li>\n<li>To have a standard generic factory process to create the strong types using a standard signature that provides a collection of zero, one or many strings for a particular header name to be provided by the factory</li>\n<li>For the factory process to have a simple fluent syntax that can be used within the container registration process</li>\n<li>For the factory to be able to consume an expression that can take that collection of string values and decide how to map the values (or lack of values) into an instance of a type that can be consumed elsewhere</li>\n<li>For the constructed types to encapsulate not just the incoming value(s), but also be able to express a lack of value(s) or an invalid state due to invalid value formats (as raising exceptions during the factory process will be difficult to catch)</li>\n<li>For the constructed types to be automatically registered as scoped lifetime services in the container for use by controllers</li>\n<li>For the constructed types to be available via a singleton that is aware of the async context so that can be injected into standard middleware constructors (by encapsulating the HttpContextAccessor).</li>\n</ul>\n<h2 id=\"the-code\"><a name=\"the-code\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-code\">The Code</a></h2>\n<p>The library is still an initial work in progress at time of writing, but if you are interested in how I have approached the above, the code is available in a repository at <a href=\"https://github.com/stevetalkscode/TypedHttpHeaders\" target=\"_blank\" class=\"externallink\">https://github.com/stevetalkscode/TypedHttpHeaders</a></p>\n<h2 id=\"coming-up\"><a name=\"comingup\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#comingup\">Coming Up</a></h2>\n<p>As I progress with the library, I plan to have more posts in this series where I will be walking through how the code works in order to achieve the goals above and eventually hope to release a NuGet package.</p>\n",
    "sanitized": "In this first part of a series of occasional posts, I discuss the thinking behind taking  string based HTTP Headers and presenting them to your .NET code via dependency injection as strongly typed objects.\nBackground\nIf you have read my previous blog posts or seen my talks, you will be aware that I am a big fan of the configuration binding functionality in .NET Core (now .NET 5) that takes string key/value pairs stored in various ways (such as JSON or environmental variables) and binding them to a typed object that is exposed via the .NET Dependency Injection container.\nRecently, I have been giving some thought to other areas in ASP.NET Core that would benefit from this ability to bind string data into object instances, in particular looking at HTTP request headers.\nThe IHeaderDictionary Interface\nIn ASP.NET Core, HTTP request headers are presented as an instance of\nthe IHeaderDictionary\n interface which is primarily a (dictionary) collection of key/value pairs made up of a string value for the key (the header key)\nand the header values within a\nStringValues struct (which itself is a special collection of strings that \"represents zero/null, one, or many strings in an efficient way\").\nThe purpose of the interface is to present a collection of HTTP headers in a consistent manner. HTTP headers can have multiple declarations within the incoming request (see the RFC specification) which need to be parsed and grouped together by key before being presented in the collection.\nWhilst the interface provides a consistent manner to access the headers, if you want interrogate the incoming request headers, you have a few hoops to jump through, namely\n\nthe HttpContext needs to be directly accessible in controllers, it is a property on the controller, or indirectly via the (singleton) IHttpContextAccessor (which is not registered by default) when consuming elsewhere\nonce you have the context, having to navigate to the Request property\nWithin the request object, navigate to the Headers property\n\nIn other words you need code to get to HttpContext.Request.Headers.\nIf you have several headers that you want to access, you have to repeat this process for each of the headers you require.\nParsing and Validating Header Values\nOnce you have your value(s) for a header, these are still strings that you may want to convert into some other type such as integer or GUID, which then means having to parse the value(s) which then raises a number of other questions:\n\nWhat should be the default if the header is not present?\nWhat to do if the header is present, but the value is not in the correct format to parse to the required type?\nWhat to do if only expecting a single value, but multiple values are presented - first or last in wins?\nWhat to do if expecting multiple values and a single value is presented\nWhat to do if the value(s) can be parsed, but fail any domain validation that needs to be applied (E.g. integer value must be within a domain range)\nIf any of the above cannot be worked around, how can an error be safely marshalled back to the caller without raising an exception?\n\nIn each of these scenarios, there is the potential for an exception to be thrown if assumptions are made about the incoming data and guards are not put in place to handle non-expected scenarios (E.g. using TryParse instead of Parse when converting strings to other primitive types to avoid exceptions when parsing fails).\nAssuming all of the above is working correctly and you have managed to get the value(s) you require out of the headers, there is the question of passing the data from the controller action (or middleware) to some domain model or function.\nIdeally, your domain logic and entities should avoid any bindings to web related types such as IHeaderDictionary & HttpContext. Avoiding this coupling to web concepts means they can be used in other applications (console, desktop or mobile applications) where the values that are received via the headers in a web application/API service may (or may not) be obtained in other ways (user input, configuration or hard coded).\nPrimitive Obsession\nI recently read Andrew Lock's blog about primitive obsession where he discusses the danger of passing values around as standard data type (string, integer, GUID et al) as these are interchangeable values that do not have context.\nThe post goes on to put forward an implementation of wrapping these values into 'strong types' that give context to a value. E.g. a customer id value expressed as a GUID is interchangeable with a product id that is also a GUID, but an instance of a CustomerIdentity type is not interchangeable with a ProductIdentity.\nAfter having read Andrew's series that shows how to create strong types that are struct based, I then went to to read Thomas Levesque's series that is based on Andrew's series, but this time implementing using C# 9 record types in .NET 5.\nI highly recommend reading both of these.\nThe principle I want to carry through is that each HTTP header of interest to the application should be mapped into a strongly typed instance to give the value(s) some meaning and context beyond just being an incoming value. In addition, these types should abstract away the need for callers to have any knowledge of HTTP related classes and interfaces.\nReplacing Commonly Written Code with a Library\nWith all of the above in mind, I have started a library to remove the need to keep writing a lot of the commonly required code. The requirements I have are\n\nTo remove the need for any of my controller or domain code to need to have knowledge of HTTP headers to retrieve the values that have been supplied in a request. Instead I want strongly typed values to be available from the Dependency Injection container that encapsulate one or more pieces of data that have some meaning to the application\nTo have a standard generic factory process to create the strong types using a standard signature that provides a collection of zero, one or many strings for a particular header name to be provided by the factory\nFor the factory process to have a simple fluent syntax that can be used within the container registration process\nFor the factory to be able to consume an expression that can take that collection of string values and decide how to map the values (or lack of values) into an instance of a type that can be consumed elsewhere\nFor the constructed types to encapsulate not just the incoming value(s), but also be able to express a lack of value(s) or an invalid state due to invalid value formats (as raising exceptions during the factory process will be difficult to catch)\nFor the constructed types to be automatically registered as scoped lifetime services in the container for use by controllers\nFor the constructed types to be available via a singleton that is aware of the async context so that can be injected into standard middleware constructors (by encapsulating the HttpContextAccessor).\n\nThe Code\nThe library is still an initial work in progress at time of writing, but if you are interested in how I have approached the above, the code is available in a repository at https://github.com/stevetalkscode/TypedHttpHeaders\nComing Up\nAs I progress with the library, I plan to have more posts in this series where I will be walking through how the code works in order to achieve the goals above and eventually hope to release a NuGet package."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/null-injection",
    "raw": "<p>In this post, I present my Top 5 scenarios of null injections from the .NET Dependency Injection container and use these to justify adding guard clauses to your C# constructors in consuming classes.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>I was recently listening to an episode of the <a href=\"https://dotnetcore.show/episode-66-tdd-and-the-terminator-with-layla-porter/\" target=\"_blank\" class=\"externallink\">Dot Net Core Show</a> with guest <a href=\"https://www.layla.dev/\" target=\"_blank\" class=\"externallink\">Layla Porter</a> (<a href=\"https://twitter.com/LaylaCodesIt\" target=\"_blank\" class=\"externallink\">@LaylaCodesIt</a>) on the topic of her brilliant talk <a href=\"https://www.dotnetoxford.com/posts/2020-08-tdd-and-the-terminator\" target=\"_blank\" class=\"externallink\">TDD and the Terminator</a>. (Go watch the talk and listen to the podcast!)</p>\n<p>Around 43 minutes into the podcast, my ears pricked up when Layla mentioned a conversation that we had when she presented the talk at the <a href=\"https://www.dotnetoxford.com/\">Dot Net Oxford user group in the UK</a>.</p>\n<p>The conversation was instigated by a question (<a href=\"https://youtu.be/eo5HalT_X5A?t=4325\">approximately 01:12:00 in the video</a>) regarding whether your code should be checking constructor parameters for null given that the Microsoft Dependency Injection container will throw an InvalidOperationException if it cannot resolve the dependency.</p>\n<p>In the podcast, Layla says that she includes null coalesce operations on incoming constructor parameters, primarily out of habit. She then goes on to discuss the arguments as to whether this is testing your code or the injection framework.</p>\n<p><a href=\"https://twitter.com/podcasterJay\" target=\"_blank\" class=\"externallink\">Jamie Taylor</a> (host of the <a href=\"https://twitter.com/dotnetcoreshow\" target=\"_blank\" class=\"externallink\">podcast</a>) then rightly points out, what if someone instantiates your class directly and not via dependency injection - surely you should do the checks anyway to handle this situation?</p>\n<p>This all got me thinking about whether there was anywhere that documents those scenarios when it is possible to receive a null from the container; either directly into a constructor parameter or indirectly, by not populating a class&nbsp; member within an injected parameter instance.</p>\n<p>A quick search on Google did not yield a definitive reference, so I decided I would write this blog post as a one stop shop to define my Top 5 list of when you should consider writing null checking guard clauses in your consuming class' constructor.</p>\n<h1 id=\"unresolved-instances-throw-an-exception-so-i-dont-need-a-null-check\">\"Unresolved Instances Throw an Exception so I Don't Need a Null Check\"</h1>\n<p>So let's start with this statement. On initial consideration, the statement is correct, but there is some nuance to it that needs explaining.</p>\n<p>If your consuming class requires an instance of a service type to be resolved by the container and the service type has not been registered, the container will throw an InvalidOperationException. The description within the exception provides details of the service type it was unable to resolve (either directly or indirectly due to a missing registration in the dependency chain).</p>\n<p>A couple of other safeguards that are in place include:</p>\n<ul>\n<li>Registering a singleton using an uninitialised variable/field or a variable/field/class member that results in a null at point of registration will trigger an ArgumentNullException at the registration, prior to the container being built</li>\n<li>Registering an interface without an implementation will trigger an ArgumentException at the point the container is being built</li>\n</ul>\n<p>However, there is a difference between not being able to resolve the service type due to a missing registration and the service type actually being resolved, but returning a null due to something happening within the registration or the instance construction process.</p>\n<p>When a service resolves to a null, it will not throw an exception as null may conceptually be a valid result.</p>\n<h2 id=\"the-way-i-registered-the-service-will-always-return-an-instance\"><a name=\"my-service-will-always-return-an-instance\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#my-service-will-always-return-an-instance\">\"The Way I Registered The Service Will Always Return an Instance\"</a></h2>\n<p>That may be true at the time you write the registration code. If you are the sole maintainer and sole user of a project, then checking for nulls in the consuming classes may feel like overkill as it is potentially a lot of boilerplate code to write for what may appear to be little gain (though having many constructor parameters is a code smell that may need to be addressed anyway!)</p>\n<p>However, <a href=\"https://en.wikipedia.org/wiki/Software_rot\" target=\"_blank\" class=\"externallink\">source code changes over time or may \"rot\"</a> so you should consider protecting your consuming class from your future self in case you change the service registration or some other aspect that may affect the dependency resolution.</p>\n<h3 id=\"why-your-code-may-change\"><a name=\"why-your-code-may-change\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#why-your-code-may-change\">Why Your Code May Change</a></h3>\n<p>If the software you are writing is not just for your own purposes, the potential for change happening in the future increases significantly if:</p>\n<ul>\n<li>you are working in a team - another member of the team may deliberately change the registration for some reason or inadvertently do so through a source code merge that adds an additional registration that overrides your registration further down in the code</li>\n<li>you are making use of one or more third party extension methods that do a 'black box' service registration to the IServiceCollection -&nbsp; these may add one or more overriding registrations after your registration of a service type (note,&nbsp; by default, the last registration of a service type will be the one that is returned if the consumer asks for a single instance … though if playing nicely, the third party will avoid this by using one of <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.extensions.servicecollectiondescriptorextensions.tryadd\" target=\"_blank\" class=\"externallink\">the variations of TryAdd extension methods</a>)</li>\n<li>you have registered the service type to use a lambda expression that returns the instance by resolving another service registration from the container within the expression</li>\n<li>you have registered the service type by using a lambda expression that creates the instance for you either within the lambda or by using a factory class.</li>\n</ul>\n<p>The other main consideration, (as Jamie said in the podcast),&nbsp; is that if your class is publicly accessible and has a public constructor, there is nothing to stop someone (either a team member or your future self) from instantiating the instance (via the new keyword).</p>\n<p>If this is the case, then it is advisable to put guard checks in place if receiving a constructor parameter set to null will cause your class to either behave differently or, more likely, throw an unexpected NullReferenceException somewhere within its code execution.</p>\n<h2 id=\"my-top-5-null-injections\"><a name=\"top-5-null-injections\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injections\">My Top 5 Null Injections</a></h2>\n<p>So with the above in mind, let's look at some common scenarios I have picked out where the container could resolve a null either for the requested service type or one/some of the properties within a resolved instance.</p>\n<h3 id=\"redirection-to-another-service-via-the-service-provider\"><a name=\"top-5-null-injection-no-1\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injection-no-1\">1. Redirection to Another Service Via the Service Provider</a></h3>\n<p>You may have a type that has many interfaces through which you wish the container to resolve an instance (E.g. the Interface Segregation Principle has been applied).\nIn order to ensure each of the interface registrations return the same instance (and not a new instance for each of the interfaces), you need to register the class first and then subsequently register each interface with a lambda expression that uses the service provider to return the main class instance cast as the interface.</p>\n<p>So how can this approach end up with the container returning&nbsp; a null?&nbsp;In short, the lambda expressions are evaluated at runtime when the instance resolution is requested and do not explicitly check for nulls being returned.</p>\n<p>However, more specifically with regards to getting another instance from the container, there is just one method, GetService on the IServiceProvider interface to retrieve an instance for you.</p>\n<p>If GetService is used within the lambda expression, and the requested type has not been registered, it does not throw an exception. Instead, it will return a null which is then returned from the container to the consumer that requested the service type. Therefore, if the lambda just returns the result from GetService, the consumer will receive a null.</p>\n<p>Whilst the IServiceProvider interface only has the one method, the extension methods provided by the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.serviceproviderserviceextensions\" target=\"_blank\" class=\"externallink\">ServiceProviderServiceExtensions</a> class include GetRequiredService&nbsp; and it's generic counterpart GetRequiredService<t>.</t></p>\n<p>These both perform a null check and throw an InvalidOperationException when a service redirection cannot be resolved.</p>\n<p>Therefore, my advice is to always use the GetRequiredService<t> extension method if resolving an instance from the container within a lambda.</t></p>\n<hr>\n<p>Another way to get or create instances from the container is the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.activatorutilities\" target=\"_blank\" class=\"externallink\">ActivatorUtilities</a> class, but I won't cover that here.</p>\n<hr>\n<h3 id=\"other-lambda-expression-factory-resolutions\"><a name=\"top-5-null-injection-no-2\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injection-no-2\">2. Other Lambda Expression Factory Resolutions</a></h3>\n<p>In addition to using a lambda expression to resolve another type from the service provider as shown above, we can use the same tactic to resolve an instance from all manner of instance creation methods or lookup systems.</p>\n<p>You may be using a dedicated factory class to create an instance or taking some resolved instance and mapping or composing some other type instance from these constituent parts which may inadvertently result in a null.</p>\n<p>Similarly, you may be using some form of cache or in memory lookup table that takes a key and returns the result. In some cases, if the value cannot be found, the lookup may return null instead of throwing an exception (E.g. a collection that inherits from <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.collections.specialized.nameobjectcollectionbase\">NameObjectCollectionBase</a> &nbsp;or an implementation of <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.caching.distributed.idistributedcache\">IDistributedCache</a> that in turn feeds some data into an object factory).</p>\n<p>If this is the case, you may want to consider including a null check within the lambda that will either coalesce the result into a default <a href=\"https://en.wikipedia.org/wiki/Null_object_pattern#C#\" target=\"_blank\" class=\"externallink\">NullObject</a> instance or throw an exception at point of instance resolution.</p>\n<p>If you don't do this, the consumer will need to protect itself from a null parameter injection using guard clauses.</p>\n<h3 id=\"black-box-extensions-methods\"><a name=\"top-5-null-injection-no-3\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injection-no-3\">3. Black Box Extensions Methods</a></h3>\n<p>Many third party NuGet package libraries offer extension methods to perform service registration of services within the library. In many cases, these become 'black box' service resolutions.</p>\n<p>As a consumer of these services, you have no guarantee that the service will not result in a null (due to one or more of the above scenarios).</p>\n<p>Whilst you could look at the library source code if it is available on GitHub or though Source Link, this is going beyond what you should need to do as a consumer of a library.</p>\n<p>Therefore, as a consumer, you need to protect yourself from null injection if this is going to cause your code to break.</p>\n<hr>\n<p><strong>The next two items are not direct null injections, but can cause member values on instances injected from the container to be set to null which may have a knock on consequence on your consuming class.</strong></p>\n<hr>\n<h3 id=\"ienumerable-service-injection\"><a name=\"top-5-null-injection-no-4\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injection-no-4\">4. IEnumerable<t> Service Injection</t></a></h3>\n<p>If you (or a third party) have added multiple registrations for the same service type, your consumer will need to specify a constructor parameter of IEnumerable<t> where T is the service type in order to retrieve all the service resolutions.</t></p>\n<p>At time of writing, this is <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?#service-registration-methods\" target=\"_blank\" class=\"externallink\">documented on the Microsoft Docs page for DI fundamentals</a> (though this may have moved if you are reading this in the future!)</p>\n<hr>\n<p>You can also request multiple services directly from the container using the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.serviceproviderserviceextensions.getservices?#Microsoft_Extensions_DependencyInjection_ServiceProviderServiceExtensions_GetServices__1_System_IServiceProvider_\" target=\"_blank\" class=\"externallink\">GetServices</a> extension methods, but again handle with care!</p>\n<hr>\n<p>Two things are worth checking when receiving an enumerable of service instances.</p>\n<p>Firstly, if the required service type has not been registered, an exception will not be thrown when resolving the enumerable. You will get an IEnumerable<t>&nbsp; instance (not a null)&nbsp; but it will not yield any results when iterating over it as it will be empty.</t></p>\n<p>If you cast the (empty) enumerable into an array or a collection based type and have logic that applies some form of matching (on the assumption that certain types have been registered), your code will probably blow up, so it is worth having some guards around this.</p>\n<p>Secondly, for the reasons described above in the three direct null scenarios, one (or more) of the services in the returned enumeration may be null and therefore it is worth including a null check guard clause when iterating over the enumerable to confirm that each yielded instance in the enumerable is not null before using it.</p>\n<h3 id=\"configuration-binding\"><a name=\"top-5-null-injection-no-5\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#top-5-null-injection-no-5\">5. Configuration Binding</a></h3>\n<p>This last source of null injection is an edge case that is not about a null being&nbsp; injected, but properties on a resolved instance not being set due to a problem with the container.</p>\n<p>If you use <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/options\" target=\"_blank\" class=\"externallink\">configuration binding</a> to retrieve a configuration section and bind its children to properties of a class, you could end up with some of the properties (or child properties if hierarchical) being set to default values if the binding is unable to find a match.</p>\n<p>This can come about in two ways:</p>\n<ul>\n<li>The path to the configuration is not found</li>\n<li>Properties do not have matching entries in the configuration</li>\n</ul>\n<p>The first of these scenarios can happen due to errors or source code changes that mean that the whole object cannot mapped as it cannot find the root entry.&nbsp;(E.g. the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.configuration.iconfiguration.getsection\" target=\"_blank\" class=\"externallink\">IConfiguration.GetSection method</a> relies on a string value to specify the section name and therefore any errors in the path will not be detected until runtime)</p>\n<p>The second can also happen for the same reasons, but may also happen due to omission of a property within the configuration source (E.g. entry missing from appsettings.json).</p>\n<p>Whichever way the mismatch happens, the consuming class need to protect itself from defaults due to the properties not being set (these defaults may be actual values on structs, null for reference types, or an unset <a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/nullable-value-types\" target=\"_blank\" class=\"externallink\">nullable value types</a>).</p>\n<p>This protection may be done either by coalescing type default values into domain default values or making use of the various validation techniques you can apply to configuration objects that will throw an exception during or post binding.</p>\n<p>Alternatively, you could pass the buck down to the end-consumer class to validate the configuration value(s) received.</p>\n<h2 id=\"guarding-your-consumer-against-null-injection\"><a name=\"guarding-against-null-injection\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#guarding-against-null-injection\">Guarding Your Consumer Against Null Injection</a></h2>\n<p>I have put forward my personal Top 5 list of situations which justify writing guard clauses to check for constructor parameters.</p>\n<p>How you perform these checks is up to you, but my approach is to use a mixture of</p>\n<ul>\n<li>using null coalesce (<a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/null-coalescing-operator\" target=\"_blank\" class=\"externallink\">?? or its C#8 sibling ??=</a> ) into a fixed default value that your code can use</li>\n<li>check for null injection (or null properties) and throw an ArgumentNullException or (if the context justifies) one of its sibling exceptions (ArgumentException or ArgumentOutOfRangeException). This checking can make use of either an explicit if statement (<a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-9#pattern-matching-enhancements\">I like the new C# 9&nbsp; 'is not null' pattern matching</a>); or use the null coalescing operator that throws an exception on null (C# 7.0 onwards)</li>\n</ul>\n<p>Whilst you still have to do some work, some of the boilerplate code can be reduced by using a library such as <a href=\"https://www.nuget.org/packages/Ardalis.GuardClauses/\" target=\"_blank\" class=\"externallink\">Steve (@ardalis) Smith's GuardClauses NuGet package</a>.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>I hope the above list of null injection scenarios has been useful. If I think of any more, I will either post updates to this page or create another post in the future.</p>\n",
    "sanitized": "In this post, I present my Top 5 scenarios of null injections from the .NET Dependency Injection container and use these to justify adding guard clauses to your C# constructors in consuming classes.\nBackground\nI was recently listening to an episode of the Dot Net Core Show with guest Layla Porter (@LaylaCodesIt) on the topic of her brilliant talk TDD and the Terminator. (Go watch the talk and listen to the podcast!)\nAround 43 minutes into the podcast, my ears pricked up when Layla mentioned a conversation that we had when she presented the talk at the Dot Net Oxford user group in the UK.\nThe conversation was instigated by a question (approximately 01:12:00 in the video) regarding whether your code should be checking constructor parameters for null given that the Microsoft Dependency Injection container will throw an InvalidOperationException if it cannot resolve the dependency.\nIn the podcast, Layla says that she includes null coalesce operations on incoming constructor parameters, primarily out of habit. She then goes on to discuss the arguments as to whether this is testing your code or the injection framework.\nJamie Taylor (host of the podcast) then rightly points out, what if someone instantiates your class directly and not via dependency injection - surely you should do the checks anyway to handle this situation?\nThis all got me thinking about whether there was anywhere that documents those scenarios when it is possible to receive a null from the container; either directly into a constructor parameter or indirectly, by not populating a class  member within an injected parameter instance.\nA quick search on Google did not yield a definitive reference, so I decided I would write this blog post as a one stop shop to define my Top 5 list of when you should consider writing null checking guard clauses in your consuming class' constructor.\n\"Unresolved Instances Throw an Exception so I Don't Need a Null Check\"\nSo let's start with this statement. On initial consideration, the statement is correct, but there is some nuance to it that needs explaining.\nIf your consuming class requires an instance of a service type to be resolved by the container and the service type has not been registered, the container will throw an InvalidOperationException. The description within the exception provides details of the service type it was unable to resolve (either directly or indirectly due to a missing registration in the dependency chain).\nA couple of other safeguards that are in place include:\n\nRegistering a singleton using an uninitialised variable/field or a variable/field/class member that results in a null at point of registration will trigger an ArgumentNullException at the registration, prior to the container being built\nRegistering an interface without an implementation will trigger an ArgumentException at the point the container is being built\n\nHowever, there is a difference between not being able to resolve the service type due to a missing registration and the service type actually being resolved, but returning a null due to something happening within the registration or the instance construction process.\nWhen a service resolves to a null, it will not throw an exception as null may conceptually be a valid result.\n\"The Way I Registered The Service Will Always Return an Instance\"\nThat may be true at the time you write the registration code. If you are the sole maintainer and sole user of a project, then checking for nulls in the consuming classes may feel like overkill as it is potentially a lot of boilerplate code to write for what may appear to be little gain (though having many constructor parameters is a code smell that may need to be addressed anyway!)\nHowever, source code changes over time or may \"rot\" so you should consider protecting your consuming class from your future self in case you change the service registration or some other aspect that may affect the dependency resolution.\nWhy Your Code May Change\nIf the software you are writing is not just for your own purposes, the potential for change happening in the future increases significantly if:\n\nyou are working in a team - another member of the team may deliberately change the registration for some reason or inadvertently do so through a source code merge that adds an additional registration that overrides your registration further down in the code\nyou are making use of one or more third party extension methods that do a 'black box' service registration to the IServiceCollection -  these may add one or more overriding registrations after your registration of a service type (note,  by default, the last registration of a service type will be the one that is returned if the consumer asks for a single instance … though if playing nicely, the third party will avoid this by using one of the variations of TryAdd extension methods)\nyou have registered the service type to use a lambda expression that returns the instance by resolving another service registration from the container within the expression\nyou have registered the service type by using a lambda expression that creates the instance for you either within the lambda or by using a factory class.\n\nThe other main consideration, (as Jamie said in the podcast),  is that if your class is publicly accessible and has a public constructor, there is nothing to stop someone (either a team member or your future self) from instantiating the instance (via the new keyword).\nIf this is the case, then it is advisable to put guard checks in place if receiving a constructor parameter set to null will cause your class to either behave differently or, more likely, throw an unexpected NullReferenceException somewhere within its code execution.\nMy Top 5 Null Injections\nSo with the above in mind, let's look at some common scenarios I have picked out where the container could resolve a null either for the requested service type or one/some of the properties within a resolved instance.\n1. Redirection to Another Service Via the Service Provider\nYou may have a type that has many interfaces through which you wish the container to resolve an instance (E.g. the Interface Segregation Principle has been applied).\nIn order to ensure each of the interface registrations return the same instance (and not a new instance for each of the interfaces), you need to register the class first and then subsequently register each interface with a lambda expression that uses the service provider to return the main class instance cast as the interface.\nSo how can this approach end up with the container returning  a null? In short, the lambda expressions are evaluated at runtime when the instance resolution is requested and do not explicitly check for nulls being returned.\nHowever, more specifically with regards to getting another instance from the container, there is just one method, GetService on the IServiceProvider interface to retrieve an instance for you.\nIf GetService is used within the lambda expression, and the requested type has not been registered, it does not throw an exception. Instead, it will return a null which is then returned from the container to the consumer that requested the service type. Therefore, if the lambda just returns the result from GetService, the consumer will receive a null.\nWhilst the IServiceProvider interface only has the one method, the extension methods provided by the ServiceProviderServiceExtensions class include GetRequiredService  and it's generic counterpart GetRequiredService.\nThese both perform a null check and throw an InvalidOperationException when a service redirection cannot be resolved.\nTherefore, my advice is to always use the GetRequiredService extension method if resolving an instance from the container within a lambda.\n\nAnother way to get or create instances from the container is the ActivatorUtilities class, but I won't cover that here.\n\n2. Other Lambda Expression Factory Resolutions\nIn addition to using a lambda expression to resolve another type from the service provider as shown above, we can use the same tactic to resolve an instance from all manner of instance creation methods or lookup systems.\nYou may be using a dedicated factory class to create an instance or taking some resolved instance and mapping or composing some other type instance from these constituent parts which may inadvertently result in a null.\nSimilarly, you may be using some form of cache or in memory lookup table that takes a key and returns the result. In some cases, if the value cannot be found, the lookup may return null instead of throwing an exception (E.g. a collection that inherits from NameObjectCollectionBase  or an implementation of IDistributedCache that in turn feeds some data into an object factory).\nIf this is the case, you may want to consider including a null check within the lambda that will either coalesce the result into a default NullObject instance or throw an exception at point of instance resolution.\nIf you don't do this, the consumer will need to protect itself from a null parameter injection using guard clauses.\n3. Black Box Extensions Methods\nMany third party NuGet package libraries offer extension methods to perform service registration of services within the library. In many cases, these become 'black box' service resolutions.\nAs a consumer of these services, you have no guarantee that the service will not result in a null (due to one or more of the above scenarios).\nWhilst you could look at the library source code if it is available on GitHub or though Source Link, this is going beyond what you should need to do as a consumer of a library.\nTherefore, as a consumer, you need to protect yourself from null injection if this is going to cause your code to break.\n\nThe next two items are not direct null injections, but can cause member values on instances injected from the container to be set to null which may have a knock on consequence on your consuming class.\n\n4. IEnumerable Service Injection\nIf you (or a third party) have added multiple registrations for the same service type, your consumer will need to specify a constructor parameter of IEnumerable where T is the service type in order to retrieve all the service resolutions.\nAt time of writing, this is documented on the Microsoft Docs page for DI fundamentals (though this may have moved if you are reading this in the future!)\n\nYou can also request multiple services directly from the container using the GetServices extension methods, but again handle with care!\n\nTwo things are worth checking when receiving an enumerable of service instances.\nFirstly, if the required service type has not been registered, an exception will not be thrown when resolving the enumerable. You will get an IEnumerable  instance (not a null)  but it will not yield any results when iterating over it as it will be empty.\nIf you cast the (empty) enumerable into an array or a collection based type and have logic that applies some form of matching (on the assumption that certain types have been registered), your code will probably blow up, so it is worth having some guards around this.\nSecondly, for the reasons described above in the three direct null scenarios, one (or more) of the services in the returned enumeration may be null and therefore it is worth including a null check guard clause when iterating over the enumerable to confirm that each yielded instance in the enumerable is not null before using it.\n5. Configuration Binding\nThis last source of null injection is an edge case that is not about a null being  injected, but properties on a resolved instance not being set due to a problem with the container.\nIf you use configuration binding to retrieve a configuration section and bind its children to properties of a class, you could end up with some of the properties (or child properties if hierarchical) being set to default values if the binding is unable to find a match.\nThis can come about in two ways:\n\nThe path to the configuration is not found\nProperties do not have matching entries in the configuration\n\nThe first of these scenarios can happen due to errors or source code changes that mean that the whole object cannot mapped as it cannot find the root entry. (E.g. the IConfiguration.GetSection method relies on a string value to specify the section name and therefore any errors in the path will not be detected until runtime)\nThe second can also happen for the same reasons, but may also happen due to omission of a property within the configuration source (E.g. entry missing from appsettings.json).\nWhichever way the mismatch happens, the consuming class need to protect itself from defaults due to the properties not being set (these defaults may be actual values on structs, null for reference types, or an unset nullable value types).\nThis protection may be done either by coalescing type default values into domain default values or making use of the various validation techniques you can apply to configuration objects that will throw an exception during or post binding.\nAlternatively, you could pass the buck down to the end-consumer class to validate the configuration value(s) received.\nGuarding Your Consumer Against Null Injection\nI have put forward my personal Top 5 list of situations which justify writing guard clauses to check for constructor parameters.\nHow you perform these checks is up to you, but my approach is to use a mixture of\n\nusing null coalesce (?? or its C#8 sibling ??= ) into a fixed default value that your code can use\ncheck for null injection (or null properties) and throw an ArgumentNullException or (if the context justifies) one of its sibling exceptions (ArgumentException or ArgumentOutOfRangeException). This checking can make use of either an explicit if statement (I like the new C# 9  'is not null' pattern matching); or use the null coalescing operator that throws an exception on null (C# 7.0 onwards)\n\nWhilst you still have to do some work, some of the boilerplate code can be reduced by using a library such as Steve (@ardalis) Smith's GuardClauses NuGet package.\nConclusion\nI hope the above list of null injection scenarios has been useful. If I think of any more, I will either post updates to this page or create another post in the future."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/disposables-in-di-part-3",
    "raw": "<p>Following on from <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-1\">Parts 1</a> and <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-2\">2</a>, in this final part of the series, I move on to dealing with types that you do not have source control for and therefore cannot change directly to hide the Dispose method using the techniques I have described in the previous posts.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>In the previous two parts of this series, I have made the assumption that you are able to amend the source code for types that implement IDisposable.</p>\n<p>But what happens, if you don't have the source code? This is where some well known design patterns come in useful.</p>\n<h2 id=\"design-patterns\"><a name=\"design-patterns\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#design-patterns\">Design Patterns</a></h2>\n<p>When trying to hide disposability from container consumers, the general principle (as shown in the previous parts of this series), is to use an interface that excludes the dispose method from its definition so that the consumer only receives the instance via the interface and not as the concrete class, thus hiding the Dispose method.</p>\n<p>If we do not have the source code, we need to create an intermediary between the interface that we want to expose and the type that we want to use.</p>\n<p>There are four classic design patterns , each a variation of the intermediary theme, that we can use together to achieve our goal.</p>\n<p>I will not go into an in depth description of the patterns here as there are plenty of resources that can be a much better job, however, here is a brief overview.</p>\n<hr>\n<h3 id=\"adapter-and-bridge-patterns\"><a name=\"adapter-bridge-patterns\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#adapter-bridge-patterns\">Adapter and Bridge Patterns</a></h3>\n<p>You may already have another interface, either your own or some other third party that achieves the goal, but for which you do not have the source code or cannot change as it would break other dependencies. In this case, the adapter pattern is used to fill the gaps to make the two interfaces work with each other.</p>\n<p>If the desired interface does not already exist, this is becomes the bridge pattern which does the same thing, but the interface design is within your control (whereas adapter uses an interface outside your control)</p>\n<h3 id=\"facade-pattern\"><a name=\"façade-pattern\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#fa%C3%A7ade-pattern\">Façade Pattern</a></h3>\n<p>The core aim of our intermediary is to remove the Dispose method and in effect simplify the interface. If the third party has a number of members that you are not interested in or that need to be brought together into a single method, the Façade pattern can be used.</p>\n<h3 id=\"proxy-pattern\"><a name=\"proxy-pattern\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#proxy-pattern\">Proxy Pattern</a></h3>\n<p>The proxy pattern is used to prevent direct access to an object. It usually has an identical interface to the class that it represents.</p>\n<h3 id=\"decorator-pattern\"><a name=\"decorator-pattern\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#decorator-pattern\">Decorator Pattern</a></h3>\n<p>The decorator pattern is similar to the proxy pattern, but it can may additional functionality to enhance behaviour.</p>\n<hr>\n<p>For our purposes, you are likely to use an adapter pattern if you do not have control over either interface, but more likely, you will be designing the interface to be used by consumers and therefore will write a custom class that brings together elements of the other three patterns, namely</p>\n<ul>\n<li>Bridge - we will be creating one between the desired interface to receive calls and the target interface</li>\n<li>Façade - we will be simplifying the interface to remove any members not needed</li>\n<li>Proxy - passing through calls to members of our class on to the instance that we are hiding</li>\n<li>Decorator - we may be taking the opportunity to do some additional work such as logging calls</li>\n</ul>\n<h2 id=\"putting-it-together\"><a name=\"putting-it-together\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#putting-it-together\">Putting It Together</a></h2>\n<p>In the example below we have a class SomeDisposable that we do not have the source code for. The class implements IDisposable and has multiple methods, but only one of interest, namely - DoSomething().</p>\n<p>Rather than register the class directly, we want to wrap it with an intermediary that will</p>\n<ul>\n<li>Create an instance of DoSomething inside the constructor (as we do not want to register the DoSomething class with the container)</li>\n<li>Implement a simplified interface (façade) of just the one DoSomething() method</li>\n<li>Have a Dispose method (that is not exposed via the interface) that the container will call to proxy to the inner object's Dispose() method</li>\n<li>Decorate the inner DoSomething method with a call to the logger to log that the method has been called and when it has completed</li>\n</ul>\n<!--?# Gist e09c7c679b6117ee6ec2f9c636f439d9 /?-->\n<p>Once we have the intermediary class, we can register this with the container in the StartUp class with the façade interface as the service type.</p>\n<p>With this in place, we have managed to avoid consumers being able to dispose of the DoSomething singleton instance directly as it is hidden inside the intermediary class but the intermediary class (and in turn the inner instance) is still disposable by the container, but having put a façade in place, the consumer cannot call Dispose() directly.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>That is the end of this series on preventing consumers causing problems by disposing of objects that may have a longer lifetime than the consumer when under the control of the .NET DI container.</p>\n<p>I hope it has been of use.</p>\n",
    "sanitized": "Following on from Parts 1 and 2, in this final part of the series, I move on to dealing with types that you do not have source control for and therefore cannot change directly to hide the Dispose method using the techniques I have described in the previous posts.\nBackground\nIn the previous two parts of this series, I have made the assumption that you are able to amend the source code for types that implement IDisposable.\nBut what happens, if you don't have the source code? This is where some well known design patterns come in useful.\nDesign Patterns\nWhen trying to hide disposability from container consumers, the general principle (as shown in the previous parts of this series), is to use an interface that excludes the dispose method from its definition so that the consumer only receives the instance via the interface and not as the concrete class, thus hiding the Dispose method.\nIf we do not have the source code, we need to create an intermediary between the interface that we want to expose and the type that we want to use.\nThere are four classic design patterns , each a variation of the intermediary theme, that we can use together to achieve our goal.\nI will not go into an in depth description of the patterns here as there are plenty of resources that can be a much better job, however, here is a brief overview.\n\nAdapter and Bridge Patterns\nYou may already have another interface, either your own or some other third party that achieves the goal, but for which you do not have the source code or cannot change as it would break other dependencies. In this case, the adapter pattern is used to fill the gaps to make the two interfaces work with each other.\nIf the desired interface does not already exist, this is becomes the bridge pattern which does the same thing, but the interface design is within your control (whereas adapter uses an interface outside your control)\nFaçade Pattern\nThe core aim of our intermediary is to remove the Dispose method and in effect simplify the interface. If the third party has a number of members that you are not interested in or that need to be brought together into a single method, the Façade pattern can be used.\nProxy Pattern\nThe proxy pattern is used to prevent direct access to an object. It usually has an identical interface to the class that it represents.\nDecorator Pattern\nThe decorator pattern is similar to the proxy pattern, but it can may additional functionality to enhance behaviour.\n\nFor our purposes, you are likely to use an adapter pattern if you do not have control over either interface, but more likely, you will be designing the interface to be used by consumers and therefore will write a custom class that brings together elements of the other three patterns, namely\n\nBridge - we will be creating one between the desired interface to receive calls and the target interface\nFaçade - we will be simplifying the interface to remove any members not needed\nProxy - passing through calls to members of our class on to the instance that we are hiding\nDecorator - we may be taking the opportunity to do some additional work such as logging calls\n\nPutting It Together\nIn the example below we have a class SomeDisposable that we do not have the source code for. The class implements IDisposable and has multiple methods, but only one of interest, namely - DoSomething().\nRather than register the class directly, we want to wrap it with an intermediary that will\n\nCreate an instance of DoSomething inside the constructor (as we do not want to register the DoSomething class with the container)\nImplement a simplified interface (façade) of just the one DoSomething() method\nHave a Dispose method (that is not exposed via the interface) that the container will call to proxy to the inner object's Dispose() method\nDecorate the inner DoSomething method with a call to the logger to log that the method has been called and when it has completed\n\n\nOnce we have the intermediary class, we can register this with the container in the StartUp class with the façade interface as the service type.\nWith this in place, we have managed to avoid consumers being able to dispose of the DoSomething singleton instance directly as it is hidden inside the intermediary class but the intermediary class (and in turn the inner instance) is still disposable by the container, but having put a façade in place, the consumer cannot call Dispose() directly.\nConclusion\nThat is the end of this series on preventing consumers causing problems by disposing of objects that may have a longer lifetime than the consumer when under the control of the .NET DI container.\nI hope it has been of use."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/disposables-in-di-part-2",
    "raw": "<p>Following on from <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-1\">Part 1</a> where I provide an overview of hiding the Dispose method from consumers of the Dependency Injection container, in this part, I move on to dealing with objects that are created outside, but registered with the DI container.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>In <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-1\">part 1</a>, I included the table below of extension methods that can be used to register services and implementations with the Microsoft DI container which indicates which of these will automatically dispose of objects that implement the IDisposable interface.</p>\n<table lifetime=\"\" implementation=\"\" service=\"\">\n<thead>\n<tr>\n<th>Method</th>\n<th>Automatic Disposal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Add&lt;&gt;()</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Add&lt;, &gt;()</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Add&lt;&gt;(sp =&gt; new )</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td><strong>AddSingleton&lt;&gt;(new )</strong></td>\n<td><strong>No</strong></td>\n</tr>\n<tr>\n<td><strong>AddSingleton(new )</strong></td>\n<td><strong>No</strong></td>\n</tr>\n</tbody>\n</table>\n<p>In the last two of these methods, the instance is explicitly instantiated using the new keyword. Note, this style of registration is only available for Singletons and is not supported for Scoped or Transient lifetimes.</p>\n<p>Wherever possible, if the class being registered implements IDisposable, I would encourage you not to use the last two methods, and instead use the third method where the instantiation takes place inside a lambda expression. This simple change of signature ensures that the object will be disposed by the container when its lifetime comes to an end.</p>\n<p>As described in Part 1, I would also avoid registering the class as the service type as this will allow the consumer to call the Dispose method which can have unintended consequences, especially for singleton and scoped lifetime objects where the object may still be required by other consumers.</p>\n<p>If for some reason it is not possible to use one of the other methods and a new instance must be instantiated outside of the container, there are ways of ensuring that the object is disposed of by the container.</p>\n<h2 id=\"disposing-instantiated-singletons\"><a name=\"disposing-instantiated-singletons\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#disposing-instantiated-singletons\">Disposing Instantiated Singletons</a></h2>\n<p>If instances are instantiated as part of the container registration process (for example within the StartUp class's ConfigureServices method), there is no natural place to dispose of these objects and therefore, they will live for the during of the application.</p>\n<p>In most cases, this is not a major problem as, if written correctly, they will be disposed of when the application ends. However, we should try to explicitly clean up after ourselves when using unmanaged resources, but in this case, how?</p>\n<p>When using ASP.NET Core, the Host takes care of registering a number of services for you that you may not be aware of.</p>\n<p>One of the services that gets registered is <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.hosting.ihostapplicationlifetime\" target=\"_blank\" class=\"externallink\">IHostApplicationLifetime</a>. This has <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.hosting.ihostapplicationlifetime?#properties\">three properties which return cancellation tokens</a> which can be used to <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.threading.cancellationtoken.register\">register</a> callback functions that will be triggered when the host application starts, is about to stop and finally stops.</p>\n<p>With this in place, we can register a callback to our StartUp class to dispose of our objects when the application is about to stop.</p>\n<p>In order to do this, we can approach this in one of two ways, depending on where and how the object has been created during registration.</p>\n<h3 id=\"startup-class-scoped-variable\"><a name=\"startup-class-scoped-variable\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#startup-class-scoped-variable\">Startup Class Scoped Variable</a></h3>\n<p>If the instance has been created in the constructor of the StartUp class and assigned to a class level variable, this variable can be used to dispose of the object within the callback registered with the ApplicationStopping token returned from the IHostApplicationLifetime.</p>\n<p>If you have several disposable singleton instances created in this manner, they can all be disposed of within the one method.</p>\n<!--?# Gist ea202bb970d4a325fda099c0cde3e8b2 /?-->\n<h3 id=\"instance-created-inside-the-configureservices-method\"><a name=\"instance-created-inside-configureservices\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#instance-created-inside-configureservices\">Instance Created Inside the ConfigureServices Method</a></h3>\n<p>If the instance has been created 'on-the-fly' within the registration and not captured in a class variable, we will need to obtain that instance from the container in order to dispose of it.</p>\n<p>In order to obtain the instance, we need to request that instance within the Configure method in the StartUp class which gets called after the container has been created.</p>\n<p>If you have several disposable singleton instances created in this manner, they can all be disposed of within the one method. However, obtaining these instances becomes a bit messy as you need to request them all either in the Configure methods parameter signature (which can become quite lengthy if more than a couple of types are required) or use the IApplicationBuilder's ApplicationServices property to get access to the container's IServiceProvider instance and use the GetService method to obtains the container registered instances.</p>\n<!--?# Gist b91e675e8e61bb92629f0b2202bd1832 /?-->\n<h2 id=\"next-time\"><a name=\"next-time\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#next-time\">Next Time</a></h2>\n<p>In <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-3\">Part 3</a> of this series, I will discuss hiding the Dispose method using intermediate classes based on common design patterns.</p>\n",
    "sanitized": "Following on from Part 1 where I provide an overview of hiding the Dispose method from consumers of the Dependency Injection container, in this part, I move on to dealing with objects that are created outside, but registered with the DI container.\nBackground\nIn part 1, I included the table below of extension methods that can be used to register services and implementations with the Microsoft DI container which indicates which of these will automatically dispose of objects that implement the IDisposable interface.\n\n\n\nMethod\nAutomatic Disposal\n\n\n\n\nAdd<>()\nYes\n\n\nAdd<, >()\nYes\n\n\nAdd<>(sp => new )\nYes\n\n\nAddSingleton<>(new )\nNo\n\n\nAddSingleton(new )\nNo\n\n\n\nIn the last two of these methods, the instance is explicitly instantiated using the new keyword. Note, this style of registration is only available for Singletons and is not supported for Scoped or Transient lifetimes.\nWherever possible, if the class being registered implements IDisposable, I would encourage you not to use the last two methods, and instead use the third method where the instantiation takes place inside a lambda expression. This simple change of signature ensures that the object will be disposed by the container when its lifetime comes to an end.\nAs described in Part 1, I would also avoid registering the class as the service type as this will allow the consumer to call the Dispose method which can have unintended consequences, especially for singleton and scoped lifetime objects where the object may still be required by other consumers.\nIf for some reason it is not possible to use one of the other methods and a new instance must be instantiated outside of the container, there are ways of ensuring that the object is disposed of by the container.\nDisposing Instantiated Singletons\nIf instances are instantiated as part of the container registration process (for example within the StartUp class's ConfigureServices method), there is no natural place to dispose of these objects and therefore, they will live for the during of the application.\nIn most cases, this is not a major problem as, if written correctly, they will be disposed of when the application ends. However, we should try to explicitly clean up after ourselves when using unmanaged resources, but in this case, how?\nWhen using ASP.NET Core, the Host takes care of registering a number of services for you that you may not be aware of.\nOne of the services that gets registered is IHostApplicationLifetime. This has three properties which return cancellation tokens which can be used to register callback functions that will be triggered when the host application starts, is about to stop and finally stops.\nWith this in place, we can register a callback to our StartUp class to dispose of our objects when the application is about to stop.\nIn order to do this, we can approach this in one of two ways, depending on where and how the object has been created during registration.\nStartup Class Scoped Variable\nIf the instance has been created in the constructor of the StartUp class and assigned to a class level variable, this variable can be used to dispose of the object within the callback registered with the ApplicationStopping token returned from the IHostApplicationLifetime.\nIf you have several disposable singleton instances created in this manner, they can all be disposed of within the one method.\n\nInstance Created Inside the ConfigureServices Method\nIf the instance has been created 'on-the-fly' within the registration and not captured in a class variable, we will need to obtain that instance from the container in order to dispose of it.\nIn order to obtain the instance, we need to request that instance within the Configure method in the StartUp class which gets called after the container has been created.\nIf you have several disposable singleton instances created in this manner, they can all be disposed of within the one method. However, obtaining these instances becomes a bit messy as you need to request them all either in the Configure methods parameter signature (which can become quite lengthy if more than a couple of types are required) or use the IApplicationBuilder's ApplicationServices property to get access to the container's IServiceProvider instance and use the GetService method to obtains the container registered instances.\n\nNext Time\nIn Part 3 of this series, I will discuss hiding the Dispose method using intermediate classes based on common design patterns."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/disposables-in-di-part-1",
    "raw": "<p>In this post I will be discussing the traps that can catch you out by potentially creating memory leaks when registering types that implement the IDisposable interface as services with the out-of-the-box .NET Dependency Injection container.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>Typically, a type will implement the IDisposable interface when it holds unmanaged resources that need to be released or to free up memory.</p>\n<p>More information about cleaning up resource can be found on <a href=\"https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/unmanaged\" target=\"_blank\" class=\"externallink\">Microsoft Docs</a></p>\n<p>To keep things simple for the rest of this post, I will be referring to instances of types that implement IDisposable as \"Disposable Objects\".</p>\n<h2 id=\"managing-disposable-objects-without-dependency-injection\"><a name=\"managing-disposable-objects-without-di\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#managing-disposable-objects-without-di\">Managing Disposable Objects without Dependency Injection</a></h2>\n<p>Outside of dependency injection, if you create an instance of such a type, it is your responsibly to call the Dispose method on the class to initiate the release of unmanaged resources.</p>\n<p>This can be done either</p>\n<ul>\n<li>explicitly by calling the Dispose method (or some alias such as Close), typically in a <a href=\"https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/using-objects#tryfinally-block\" target=\"_blank\" class=\"externallink\">finally block of a try-catch construct</a> or</li>\n<li>implicitly via the <a href=\"https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/using-objects#the-using-statement\" target=\"_blank\" class=\"externallink\">using construct</a> to automatically dispose the object.</li>\n</ul>\n<p>The two approaches are documented <a href=\"https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/using-objects\" target=\"_blank\" class=\"externallink\">on the Microsoft Docs site</a>.</p>\n<h2 id=\"disposable-objects-created-by-the-dependency-injection-container\"><a name=\"disposable-objects-created-by-di-container\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#disposable-objects-created-by-di-container\">Disposable Objects Created by the Dependency Injection Container</a></h2>\n<p>As a general rule, if the Dependency Injection container creates an instance of the disposable object, it will clean up when the instance lifetime (transient, scoped or singleton) expires (E.g. for scoped instances in ASP.NET Core, this will be at the end of the request/response lifetime but for singletons, it is when the container itself is disposed).</p>\n<p>The following table (<a href=\"https://docs.microsoft.com/en-us/dotnet/core/extensions/dependency-injection#service-registration-methods\" target=\"_blank\" class=\"externallink\">based on the table in the Microsoft Docs page</a>) shows which registration methods will trigger the container to automatically dispose of the object.</p>\n<table lifetime=\"\" implementation=\"\" service=\"\">\n<thead>\n<tr>\n<th>Method</th>\n<th>Automatic Disposal</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Add&lt;&gt;()</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Add&lt;, &gt;()</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Add&lt;&gt;(sp =&gt; new )</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>AddSingleton&lt;&gt;(new )</td>\n<td>No</td>\n</tr>\n<tr>\n<td>AddSingleton(new )</td>\n<td>No</td>\n</tr>\n</tbody>\n</table>\n<p>As you can see from the table above, the three most common methods for adding services, where the container itself is responsible for creating the instance, will automatically dispose of the object at the appropriate time.</p>\n<p>However, the last two methods do not dispose of the object. Why? It's because in these methods, the objects have been directly instantiated with a new keyword and therefore, the container has not been responsible for creating the object.</p>\n<p>Whilst they look similar to the third method, the difference is that the instance in that method has been created within the context of a lambda expression which is within the control of the container and therefore in the container's control.</p>\n<p>In the last two methods, the object could be created at the time of registration (by using the new statement) but then again, it may have been created outside these methods (either within the scope of the ConfigureServices method in the StartUp class, or at a class level) and therefore, the container cannot possibly know of where the object has been created, the scope of its reference,&nbsp; and where else it may be used. Without this understanding, it cannot safely dispose of the object as this may throw an <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.objectdisposedexception\" target=\"_blank\" class=\"externallink\">ObjectDisposedException</a> if referenced elsewhere in code after the container has disposed of it.</p>\n<p>I will come on to dealing with ensuring these objects referenced in these last two methods can be disposed of correctly in Part 2.</p>\n<h2 id=\"hiding-disposability-from-container-consumers\"><a name=\"hiding-disposability-from-container-consumers\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#hiding-disposability-from-container-consumers\">Hiding Disposability from Container Consumers</a></h2>\n<p>The first method in the table above is the simplest way to register a type. Consumers will request an instance of the object and make use of it.</p>\n<p>However, if the type implements IDisposable, this means that the Dispose method is available to the consumer to call. This has repercussions depending on the lifetime that the dependency has been registered as.</p>\n<p>For transients that have been created specifically to be injected into the consuming class, it is not the end of the world. If dispose is called on a transient, the only place that will suffer is the consuming class (and anything it passes the reference to) as any subsequent references to the object (or to be more specific, members in the type that check the disposed status) are likely to result in an ObjectDisposedException (this will depend on the implementation of the injected class).</p>\n<p>For scoped and singleton lifetimes, things become more complicated as the object has a lifetime beyond the consumer class. If the consuming class calls Dispose and another consumer then also makes use of a member on the disposed class, that other consumer is likely to receive an ObjectDisposedException.</p>\n<p>Therefore, we want to ensure that the Dispose method on the registered class is somehow hidden from the consumer.</p>\n<p>There are several ways of hiding the Dispose method which are considered below</p>\n<h3 id=\"explicit-implementation-of-idisposable\"><a name=\"explicit-implementation-of-idisposable\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#explicit-implementation-of-idisposable\">Explicit Implementation of IDisposable</a></h3>\n<p>The quick (and dirty) way of hiding the Dispose method that exists on a class is to change the Dispose method's declaration from a public method to an explicit interface declaration (as shown below) so that it can only be called by casting the object to IDisposable.</p>\n<p>It should, however, be recognised that this is just obfuscating the availability of the Dispose method. It does not truly hide it as the consumer may be aware that the type implements IDisposable and explicitly cast the object and call Dispose.</p>\n<!--?# Gist 113ced02ddcce54540d89582881810fb /?-->\n<p>This is where extracting out other interfaces comes to our rescue when it comes to dependency injection.</p>\n<h3 id=\"register-the-implementation-type-with-a-more-restrictive-interface\"><a name=\"register-implementation-with-restrictive-interface\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#register-implementation-with-restrictive-interface\">Register the Implementation Type With a More Restrictive Interface</a></h3>\n<p>If we define an interface that has all the public members of our class except for the Dispose method and only make the object available by registering it in the DI container with the limited interface as the service, this will make it harder (but not completely impossible) for the consumer of the object to dispose of the object as the concrete type is only known to the container registration (unless the consumer uses GetType() of course, but that is splitting hairs and in many ways negates the whole point of using the container).</p>\n<p>Of course, following the Interface Segregation Principle from SOLID, this interface may be broken down into smaller interfaces which the class registered against.</p>\n<!--?# Gist 929a5278b6613e3c11d291c7ec0c5e89 /?-->\n<h2 id=\"next-time\"><a name=\"next-time\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#next-time\">Next Time ...</a></h2>\n<p>In <a href=\"https://stevetalkscode.co.uk/disposables-in-di-part-2\">Part 2</a> of this series on IDisposable in Dependency Injection, I will move on to dealing with those objects that the container will not dispose of for you.</p>\n",
    "sanitized": "In this post I will be discussing the traps that can catch you out by potentially creating memory leaks when registering types that implement the IDisposable interface as services with the out-of-the-box .NET Dependency Injection container.\nBackground\nTypically, a type will implement the IDisposable interface when it holds unmanaged resources that need to be released or to free up memory.\nMore information about cleaning up resource can be found on Microsoft Docs\nTo keep things simple for the rest of this post, I will be referring to instances of types that implement IDisposable as \"Disposable Objects\".\nManaging Disposable Objects without Dependency Injection\nOutside of dependency injection, if you create an instance of such a type, it is your responsibly to call the Dispose method on the class to initiate the release of unmanaged resources.\nThis can be done either\n\nexplicitly by calling the Dispose method (or some alias such as Close), typically in a finally block of a try-catch construct or\nimplicitly via the using construct to automatically dispose the object.\n\nThe two approaches are documented on the Microsoft Docs site.\nDisposable Objects Created by the Dependency Injection Container\nAs a general rule, if the Dependency Injection container creates an instance of the disposable object, it will clean up when the instance lifetime (transient, scoped or singleton) expires (E.g. for scoped instances in ASP.NET Core, this will be at the end of the request/response lifetime but for singletons, it is when the container itself is disposed).\nThe following table (based on the table in the Microsoft Docs page) shows which registration methods will trigger the container to automatically dispose of the object.\n\n\n\nMethod\nAutomatic Disposal\n\n\n\n\nAdd<>()\nYes\n\n\nAdd<, >()\nYes\n\n\nAdd<>(sp => new )\nYes\n\n\nAddSingleton<>(new )\nNo\n\n\nAddSingleton(new )\nNo\n\n\n\nAs you can see from the table above, the three most common methods for adding services, where the container itself is responsible for creating the instance, will automatically dispose of the object at the appropriate time.\nHowever, the last two methods do not dispose of the object. Why? It's because in these methods, the objects have been directly instantiated with a new keyword and therefore, the container has not been responsible for creating the object.\nWhilst they look similar to the third method, the difference is that the instance in that method has been created within the context of a lambda expression which is within the control of the container and therefore in the container's control.\nIn the last two methods, the object could be created at the time of registration (by using the new statement) but then again, it may have been created outside these methods (either within the scope of the ConfigureServices method in the StartUp class, or at a class level) and therefore, the container cannot possibly know of where the object has been created, the scope of its reference,  and where else it may be used. Without this understanding, it cannot safely dispose of the object as this may throw an ObjectDisposedException if referenced elsewhere in code after the container has disposed of it.\nI will come on to dealing with ensuring these objects referenced in these last two methods can be disposed of correctly in Part 2.\nHiding Disposability from Container Consumers\nThe first method in the table above is the simplest way to register a type. Consumers will request an instance of the object and make use of it.\nHowever, if the type implements IDisposable, this means that the Dispose method is available to the consumer to call. This has repercussions depending on the lifetime that the dependency has been registered as.\nFor transients that have been created specifically to be injected into the consuming class, it is not the end of the world. If dispose is called on a transient, the only place that will suffer is the consuming class (and anything it passes the reference to) as any subsequent references to the object (or to be more specific, members in the type that check the disposed status) are likely to result in an ObjectDisposedException (this will depend on the implementation of the injected class).\nFor scoped and singleton lifetimes, things become more complicated as the object has a lifetime beyond the consumer class. If the consuming class calls Dispose and another consumer then also makes use of a member on the disposed class, that other consumer is likely to receive an ObjectDisposedException.\nTherefore, we want to ensure that the Dispose method on the registered class is somehow hidden from the consumer.\nThere are several ways of hiding the Dispose method which are considered below\nExplicit Implementation of IDisposable\nThe quick (and dirty) way of hiding the Dispose method that exists on a class is to change the Dispose method's declaration from a public method to an explicit interface declaration (as shown below) so that it can only be called by casting the object to IDisposable.\nIt should, however, be recognised that this is just obfuscating the availability of the Dispose method. It does not truly hide it as the consumer may be aware that the type implements IDisposable and explicitly cast the object and call Dispose.\n\nThis is where extracting out other interfaces comes to our rescue when it comes to dependency injection.\nRegister the Implementation Type With a More Restrictive Interface\nIf we define an interface that has all the public members of our class except for the Dispose method and only make the object available by registering it in the DI container with the limited interface as the service, this will make it harder (but not completely impossible) for the consumer of the object to dispose of the object as the concrete type is only known to the container registration (unless the consumer uses GetType() of course, but that is splitting hairs and in many ways negates the whole point of using the container).\nOf course, following the Interface Segregation Principle from SOLID, this interface may be broken down into smaller interfaces which the class registered against.\n\nNext Time ...\nIn Part 2 of this series on IDisposable in Dependency Injection, I will move on to dealing with those objects that the container will not dispose of for you."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/c-sharp-9-record-factories",
    "raw": "<p>With the release of .NET 5.0 and C# 9 coming up in the next month, I have been updating my <a href=\"https://sessionize.com/s/stevetalkscode/net_core_dependency_injection__the_/27256\" target=\"_blank\" class=\"externallink\">Dependency Injection talk</a> to incorporate the latest features of the framework and language.</p>\n<p>One of the topics of the talk is using the \"Gang of Four\" Factory pattern to help with the creation of class instances where some of the data is coming from the DI container and some from the caller.</p>\n<p>In this post, I walk through using the Factory Pattern to apply the same principles to creating instances of C# 9 records.</p>\n<h2 id=\"so-how-do-records-differ-from-classes\"><a name=\"so-how-do-records-differ-from-classes\" href=\"https://stevetalkscode.co.uk/#so-how-do-records-differ-from-classes\" class=\"pageLink\">So How Do Records Differ from Classes?</a></h2>\n<p>Before getting down into the weeds of using the Factory Pattern, a quick overview of how C#9 record types differ from classes.</p>\n<p>There are plenty of articles, blog posts and videos available on the Internet that will tell you this, so I am not going to go into much depth here. However, here are a couple of resources I found useful in understanding records vs classes.</p>\n<p>Firstly, I really like the video from Microsoft's Channel 9 'On.NET' show where <a href=\"https://twitter.com/jaredpar\" target=\"_blank\" class=\"externallink\">Jared Parsons</a> explains the benefits of records over classes.</p>\n<iframe title=\"C# 9 Language Features - Microsoft Channel 9 Video\" src=\"https://channel9.msdn.com/Shows/On-NET/C-9-Language-Features/player\" width=\"800\" height=\"360\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<p>Secondly, a shout out to [Anthony Giretti](<a href=\"https://twitter.com/anthonygiretti%7Btarget=%22_blank%22\">https://twitter.com/anthonygiretti{target=\"_blank\"</a> class=\"externallink\"} for his blog post <a href=\"https://anthonygiretti.com/2020/06/17/introducing-c-9-records/\" target=\"_blank\" class=\"externallink\">https://anthonygiretti.com/2020/06/17/introducing-c-9-records/</a>&nbsp;that goes into detail about the syntax of using record types.</p>\n<p>From watching/reading these (and other bits and bobs around the Internet), my take on record types vs. classes are as follows:</p>\n<ul>\n<li>Record types are ideal for read-only 'value types' such as Data Transfer Objects (DTOs) as immutable by default (especially if using positional declaration syntax)</li>\n<li>Record types automatically provide (struct-like) value equality implementation so no need to write your own <a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/statements-expressions-operators/how-to-define-value-equality-for-a-type\" target=\"_blank\" class=\"externallink\">boiler plate for overriding default (object reference) equality behaviour</a></li>\n<li>Record types automatically provide a deconstruction implementation so you don't need to <a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/deconstruct#deconstructing-user-defined-types\" target=\"_blank\" class=\"externallink\">write your own boiler plate</a> for that.</li>\n<li>There is no need to explicitly write constructors if positional parameters syntax is used as object initialisers can be used instead</li>\n</ul>\n<p>The thing that really impressed me when watching the video is the way that the record syntax is able to replace a 40 line class definition with all the aforementioned boiler plate code with a single&nbsp; declaration</p>\n<h2 id=\"why-would-i-need-a-factory\"><a name=\"why-would-i-need-a-factory\" href=\"https://stevetalkscode.co.uk/#why-would-i-need-a-factory\" class=\"pageLink\">Why Would I Need A Factory?</a></h2>\n<p>In most cases where you may choose to use a record type over using a class type, you won't need a factory.</p>\n<p>Typically you will have some framework doing the work for you, be it a JSON deserialiser, Entity Framework or some code generation tool to create the boilerplate code for you (<a href=\"https://github.com/RicoSuter/NSwag\" target=\"_blank\" class=\"externallink\">such as NSwag for creating client code from Swagger/OpenAPI definitions</a> or possibly the <a href=\"https://devblogs.microsoft.com/dotnet/introducing-c-source-generators/\" target=\"_blank\" class=\"externallink\">new C#9 Source Generators</a>).</p>\n<p>Similarly, if all the properties of your record type can be derived from dependency injection, you can register your record type with the container with an appropriate lifetime (transient or singleton).</p>\n<p>However, in some cases you may have a need to create a new record instance as part of a user/service interaction by requiring data from a combination of</p>\n<ul>\n<li>user/service input;</li>\n<li>other objects you currently have in context;</li>\n<li>objects provided by dependency injection from the container.</li>\n</ul>\n<p>You could instantiate a new instance in your code, but to use <a href=\"https://twitter.com/ardalis\" target=\"_blank\" class=\"externallink\">Steve 'Ardalis' Smith</a>'s phrase, <a href=\"https://ardalis.com/new-is-glue/\" target=\"_blank\" class=\"externallink\">\"new is glue\"</a> and things become complicated if you need to make changes to the record type (such as adding additional mandatory properties).</p>\n<p>In these cases, try to keep in line with the <a href=\"https://en.wikipedia.org/wiki/Don%27t_repeat_yourself\" target=\"_blank\" class=\"externallink\">Don't Repeat Yourself (DRY)</a> principle and <a href=\"https://en.wikipedia.org/wiki/Single-responsibility_principle\" target=\"_blank\" class=\"externallink\">Single Responsibility Principle (SRP) from SOLID.</a> This is where a factory class comes into its own as it becomes a single place to take all these inputs from multiple sources and use them together to create the new instance, providing a simpler interaction for your code to work with.</p>\n<h2 id=\"creating-the-record\"><a name=\"creating-the-record\" href=\"https://stevetalkscode.co.uk/#creating-the-record\" class=\"pageLink\">Creating the Record</a></h2>\n<p>When looking at the properties required for your record type, consider</p>\n<ul>\n<li>Which parameters can be derived by dependency injection from the container and therefore do not need to be passed in from the caller</li>\n<li>Which parameters are only known by the caller and cannot be derived from the container</li>\n<li>Which parameters need to be created by performing some function or computation using these inputs that are not directly consumed by the record.</li>\n</ul>\n<p>For example, a Product type may have the following properties</p>\n<ul>\n<li>A product name - from some user input</li>\n<li>A SKU value&nbsp; - generated by some SKU generation function</li>\n<li>A created date/time value - generated at the time the record is created</li>\n<li>The name of the person who has created the value - taken from some identity source.</li>\n</ul>\n<p>The record is declared as follows</p>\n<!--?# Gist 1c1aeb459daadb166d2c101b4e3d9a78 /?-->\n<p>You may have several places in your application where a product can be created, so adhering to the DRY principle, we want to encapsulate the creation process into a single process.</p>\n<p>In addition, the only property coming from actual user input is the product name, so we don't want to drag all these other dependencies around the application.</p>\n<p>This is where the factory can be a major help.</p>\n<h2 id=\"building-a-record-factory\"><a name=\"building-a-record-factory\" href=\"https://stevetalkscode.co.uk/#why-would-i-need-a-factory\" class=\"pageLink\">Building a Record Factory</a></h2>\n<p>I have created an example at <a href=\"https://github.com/stevetalkscode/RecordFactory\" target=\"_blank\" class=\"externallink\">https://github.com/stevetalkscode/RecordFactory</a> that you may want to download and follow along with for the rest of the post.</p>\n<p>In my example, I am making the assumption that</p>\n<ul>\n<li>the SKU generation is provided by a custom delegate function registered with the container (as it is a single function and therefore does not necessarily require a class with a method to represent it - if you are unfamiliar with this approach within dependency injection, have a look at <a href=\"https://stevetalkscode.co.uk/simplifying-di-with-functions\">my previous blog about using delegates with DI</a></li>\n<li>the current date and time are generated by a class instance (registered as a singleton in the container) that may have several methods to choose from as to which is most appropriate (though again, for a single method, this could be represented by a delegate)</li>\n<li>the user's identity is provided by an 'accessor' type that is a singleton registered with the container that can retrieve user information from the current <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.threading.asynclocal-1?view=net-5.0\" target=\"_blank\" class=\"externallink\">AsyncLocal</a> context (E.g. in an ASP.NET application, via <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-context?view=aspnetcore-5.0#use-httpcontext-from-custom-components\" target=\"_blank\" class=\"externallink\">IHttpContextAccessor's HttpContext.User</a>).</li>\n<li>All of the above can be injected from the container, leaving the Create method only needing the single parameter of the product name</li>\n</ul>\n<!--?# Gist 8f2e6c7b7284e380621aa03b3b253a57 /?-->\n<p>(You may notice that there is not an implementation of GetCurrentTimeUtc. This is provided directly in the service registration process in the StartUp class below).</p>\n<p>In order to make this all work in a unit test, the date/time generation and user identity generation are provided by mock implementations that are registered with the service collection. The service registrations in the StartUp class will only be applied if these registrations have not already taken place by making user of the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.extensions.servicecollectiondescriptorextensions\" target=\"_blank\" class=\"externallink\">TryAdd_xxx_ extension methods</a>.</p>\n<h2 id=\"keeping-the-record-simple\"><a name=\"keeping-the-record-simple\" href=\"https://stevetalkscode.co.uk/#keeping-the-record-simple\" class=\"pageLink\">Keeping the Record Simple</a></h2>\n<p>As the record itself is a value type,&nbsp;it does not need to know how to obtain the information from these participants as it is effectively just a read-only Data Transfer Object (DTO).</p>\n<p>Therefore, the factory will need to provide the following in order to create a record instance:</p>\n<ul>\n<li>consume the SKU generator function referenced in its constructor and make a call to it when creating a new Product instance to get a new SKU</li>\n<li>consume the DateTime abstraction in the constructor and call a method to get the current UTC date and time when creating a new Product instance</li>\n<li>consume the user identity abstraction to get the user's details via a delegate.</li>\n</ul>\n<p>The factory will have one method Create() that will take a single parameter of productName (as all the other inputs are provided from within the factory class)</p>\n<h2 id=\"hiding-the-service-locator-pattern\"><a name=\"hiding-the-service-locator-pattern\" href=\"https://stevetalkscode.co.uk/#hiding-the-service-locator-pattern\" class=\"pageLink\">Hiding the Service Locator Pattern</a></h2>\n<p>Over the years, the Service Locator pattern has come to be recognised as an anti-pattern, especially when it requires that the caller has some knowledge of the container.</p>\n<p>For example, it would be easy for the ProductFactory class to take a single parameter of <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.iserviceprovider?view=net-5.0\" target=\"_blank\" class=\"externallink\">IServiceProvider</a> and make use of the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.serviceproviderserviceextensions.getrequiredservice\" target=\"_blank\" class=\"externallink\">GetRequiredService<t></t></a> extension method to obtain an instance from the container.&nbsp;If the factory is a public class, this would tie the implementation to the container technology (in this case, the Microsoft .NET Dependency Injection 'conforming' container).</p>\n<p>In some cases, there may be no way of getting around this due to some problem in resolving an dependency. In particular, with factory classes, you may encounter difficulties where the factory is registered as a singleton but one or more dependencies are scoped or transient.</p>\n<p>In these scenarios, there is a danger of the shorter-lived dependencies (transient and scoped) being resolved when the singleton is created and becoming 'captured dependencies' that are trapped for the lifetime of the singleton and not using the correctly scoped value when the Create method is called.</p>\n<p>You may need to make these dependencies parameters of the Create method (see warning about scoped dependencies below), but in some cases, this then (mis)places a responsibility onto the caller of the method to obtain those dependencies (and thus creating an unnecessary dependency chain through the application).</p>\n<p>There are two approaches that can be used in the scenario where a singleton has dependencies on lesser scoped instances.</p>\n<h3 id=\"approach-1-making-the-service-locator-private\"><a name=\"making-the-service-locator-private\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#making-the-service-locator-private\">Approach 1- Making the Service Locator Private</a></h3>\n<p>The first approach is to adopt the service locator pattern (by requiring the IServiceProvider as described above), but making the factory a private class within the StartUp class.</p>\n<p>This hiding of the factory within the StartUp class also hides the service locator pattern from the outside world as the only way of instantiating the factory is through the container. The outside world will only be aware of the abstracted interface through which it has been registered and can be consumed in the normal manner from the container.</p>\n<h3 id=\"approach-2-redirect-to-a-service-locator-embedded-within-the-service-registration\"><a name=\"redirect-to-a-service-locator\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#redirect-to-a-service-locator\">Approach 2 - Redirect to a Service Locator Embedded Within the Service Registration</a></h3>\n<p>The second way of getting around this is to add a level of indirection by using a custom delegate.</p>\n<p><a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection#service-registration-methods\" target=\"_blank\" class=\"externallink\">The extension methods for IServiceCollection</a> allow for a service locator to be embedded within a service registration by using an expression that has access to the IServiceProvider.</p>\n<p>To avoid the problem of a 'captured dependency' when injecting transient dependencies, we can register a delegate signature that wraps the service locator thus moving the service locator up into the registration process itself (as seen here) and leaving our factory unaware of the container technology.</p>\n<!--?# Gist dab2c92a091c88e1e85edf340e4cf153 /?-->\n<p>At this point the factory may be made public (or left as private) as the custom delegate takes care of obtaining the values from the container when the Create method is called and not when the (singleton) factory is created, thus avoiding capturing the dependency.</p>\n<hr>\n<h4 id=\"special-warning-about-scoped-dependencies\"><em>Special warning about scoped dependencies</em></h4>\n<p>Wrapping access to transient registered dependencies with a singleton delegate works as both singleton and transient instances are resolved from the root provider. However, this does not work for dependencies registered as scoped lifetimes which are resolved from a scoped provider which the singleton does not have access to.</p>\n<p>If you use the root provider, you will end up with a captured instance of the scoped type that is created when the singleton is created (as the container will not throw an exception unless scope checking is turned on).</p>\n<p>Unfortunately, for these dependencies, you will still need to pass these to the Create method in most cases.</p>\n<p>If you are using ASP.NET Core, there is a workaround (already partially illustrated above with accessing the User's identity).</p>\n<p>IHttpContextAcessor.HttpContext.RequestServices exposes the scoped container but is accessible from singleton services as IHttpContextAcessor is registered as a singleton service. Therefore, you could write a delegate that uses this to access the scoped dependencies via a delegate. My advice is to approach this with caution as you may find it hard to debug unexpected behaviour.</p>\n<hr>\n<h2 id=\"go-create-a-product\"><a name=\"go-create-a-product\" href=\"https://stevetalkscode.co.uk/#go-create-a-product\" class=\"pageLink\">Go Create a Product</a></h2>\n<p>In the example, the factory class is registered as a singleton to avoid the cost of recreating it every time a new Product is needed.</p>\n<p>The three dependencies are injected into the constructor, but as already mentioned the transient values are not captured at this point - we are only capturing the function pointers.</p>\n<p>It is within the Create method that we call the delegate functions to obtain the real-time transient values that are then used with the caller provided <em>productName</em> parameter to then instantiate a Product instance.</p>\n<p>&nbsp;</p>\n<!--?# Gist 50bc0a6013383cb293d69397daa1df06 /?-->&nbsp;\n<h2 id=\"conclusion\"><a name=\"conclusion\" href=\"https://stevetalkscode.co.uk/#conclusion\" class=\"pageLink\">Conclusion</a></h2>\n<p>I'll leave things there, as the best way to understand the above is to step through the code at <a href=\"https://github.com/stevetalkscode/RecordFactory\" target=\"_blank\" class=\"externallink\">https://github.com/stevetalkscode/RecordFactory</a> that accompanies this post.</p>\n<p>Whilst a factory is not required for the majority of scenarios where you use a record type, it can be of help when you need to encapsulate the logic for gathering all the dependencies that are used to create an instance without polluting the record declaration itself.</p>\n",
    "sanitized": "With the release of .NET 5.0 and C# 9 coming up in the next month, I have been updating my Dependency Injection talk to incorporate the latest features of the framework and language.\nOne of the topics of the talk is using the \"Gang of Four\" Factory pattern to help with the creation of class instances where some of the data is coming from the DI container and some from the caller.\nIn this post, I walk through using the Factory Pattern to apply the same principles to creating instances of C# 9 records.\nSo How Do Records Differ from Classes?\nBefore getting down into the weeds of using the Factory Pattern, a quick overview of how C#9 record types differ from classes.\nThere are plenty of articles, blog posts and videos available on the Internet that will tell you this, so I am not going to go into much depth here. However, here are a couple of resources I found useful in understanding records vs classes.\nFirstly, I really like the video from Microsoft's Channel 9 'On.NET' show where Jared Parsons explains the benefits of records over classes.\n\nSecondly, a shout out to [Anthony Giretti](https://twitter.com/anthonygiretti{target=\"_blank\" class=\"externallink\"} for his blog post https://anthonygiretti.com/2020/06/17/introducing-c-9-records/ that goes into detail about the syntax of using record types.\nFrom watching/reading these (and other bits and bobs around the Internet), my take on record types vs. classes are as follows:\n\nRecord types are ideal for read-only 'value types' such as Data Transfer Objects (DTOs) as immutable by default (especially if using positional declaration syntax)\nRecord types automatically provide (struct-like) value equality implementation so no need to write your own boiler plate for overriding default (object reference) equality behaviour\nRecord types automatically provide a deconstruction implementation so you don't need to write your own boiler plate for that.\nThere is no need to explicitly write constructors if positional parameters syntax is used as object initialisers can be used instead\n\nThe thing that really impressed me when watching the video is the way that the record syntax is able to replace a 40 line class definition with all the aforementioned boiler plate code with a single  declaration\nWhy Would I Need A Factory?\nIn most cases where you may choose to use a record type over using a class type, you won't need a factory.\nTypically you will have some framework doing the work for you, be it a JSON deserialiser, Entity Framework or some code generation tool to create the boilerplate code for you (such as NSwag for creating client code from Swagger/OpenAPI definitions or possibly the new C#9 Source Generators).\nSimilarly, if all the properties of your record type can be derived from dependency injection, you can register your record type with the container with an appropriate lifetime (transient or singleton).\nHowever, in some cases you may have a need to create a new record instance as part of a user/service interaction by requiring data from a combination of\n\nuser/service input;\nother objects you currently have in context;\nobjects provided by dependency injection from the container.\n\nYou could instantiate a new instance in your code, but to use Steve 'Ardalis' Smith's phrase, \"new is glue\" and things become complicated if you need to make changes to the record type (such as adding additional mandatory properties).\nIn these cases, try to keep in line with the Don't Repeat Yourself (DRY) principle and Single Responsibility Principle (SRP) from SOLID. This is where a factory class comes into its own as it becomes a single place to take all these inputs from multiple sources and use them together to create the new instance, providing a simpler interaction for your code to work with.\nCreating the Record\nWhen looking at the properties required for your record type, consider\n\nWhich parameters can be derived by dependency injection from the container and therefore do not need to be passed in from the caller\nWhich parameters are only known by the caller and cannot be derived from the container\nWhich parameters need to be created by performing some function or computation using these inputs that are not directly consumed by the record.\n\nFor example, a Product type may have the following properties\n\nA product name - from some user input\nA SKU value  - generated by some SKU generation function\nA created date/time value - generated at the time the record is created\nThe name of the person who has created the value - taken from some identity source.\n\nThe record is declared as follows\n\nYou may have several places in your application where a product can be created, so adhering to the DRY principle, we want to encapsulate the creation process into a single process.\nIn addition, the only property coming from actual user input is the product name, so we don't want to drag all these other dependencies around the application.\nThis is where the factory can be a major help.\nBuilding a Record Factory\nI have created an example at https://github.com/stevetalkscode/RecordFactory that you may want to download and follow along with for the rest of the post.\nIn my example, I am making the assumption that\n\nthe SKU generation is provided by a custom delegate function registered with the container (as it is a single function and therefore does not necessarily require a class with a method to represent it - if you are unfamiliar with this approach within dependency injection, have a look at my previous blog about using delegates with DI\nthe current date and time are generated by a class instance (registered as a singleton in the container) that may have several methods to choose from as to which is most appropriate (though again, for a single method, this could be represented by a delegate)\nthe user's identity is provided by an 'accessor' type that is a singleton registered with the container that can retrieve user information from the current AsyncLocal context (E.g. in an ASP.NET application, via IHttpContextAccessor's HttpContext.User).\nAll of the above can be injected from the container, leaving the Create method only needing the single parameter of the product name\n\n\n(You may notice that there is not an implementation of GetCurrentTimeUtc. This is provided directly in the service registration process in the StartUp class below).\nIn order to make this all work in a unit test, the date/time generation and user identity generation are provided by mock implementations that are registered with the service collection. The service registrations in the StartUp class will only be applied if these registrations have not already taken place by making user of the TryAdd_xxx_ extension methods.\nKeeping the Record Simple\nAs the record itself is a value type, it does not need to know how to obtain the information from these participants as it is effectively just a read-only Data Transfer Object (DTO).\nTherefore, the factory will need to provide the following in order to create a record instance:\n\nconsume the SKU generator function referenced in its constructor and make a call to it when creating a new Product instance to get a new SKU\nconsume the DateTime abstraction in the constructor and call a method to get the current UTC date and time when creating a new Product instance\nconsume the user identity abstraction to get the user's details via a delegate.\n\nThe factory will have one method Create() that will take a single parameter of productName (as all the other inputs are provided from within the factory class)\nHiding the Service Locator Pattern\nOver the years, the Service Locator pattern has come to be recognised as an anti-pattern, especially when it requires that the caller has some knowledge of the container.\nFor example, it would be easy for the ProductFactory class to take a single parameter of IServiceProvider and make use of the GetRequiredService extension method to obtain an instance from the container. If the factory is a public class, this would tie the implementation to the container technology (in this case, the Microsoft .NET Dependency Injection 'conforming' container).\nIn some cases, there may be no way of getting around this due to some problem in resolving an dependency. In particular, with factory classes, you may encounter difficulties where the factory is registered as a singleton but one or more dependencies are scoped or transient.\nIn these scenarios, there is a danger of the shorter-lived dependencies (transient and scoped) being resolved when the singleton is created and becoming 'captured dependencies' that are trapped for the lifetime of the singleton and not using the correctly scoped value when the Create method is called.\nYou may need to make these dependencies parameters of the Create method (see warning about scoped dependencies below), but in some cases, this then (mis)places a responsibility onto the caller of the method to obtain those dependencies (and thus creating an unnecessary dependency chain through the application).\nThere are two approaches that can be used in the scenario where a singleton has dependencies on lesser scoped instances.\nApproach 1- Making the Service Locator Private\nThe first approach is to adopt the service locator pattern (by requiring the IServiceProvider as described above), but making the factory a private class within the StartUp class.\nThis hiding of the factory within the StartUp class also hides the service locator pattern from the outside world as the only way of instantiating the factory is through the container. The outside world will only be aware of the abstracted interface through which it has been registered and can be consumed in the normal manner from the container.\nApproach 2 - Redirect to a Service Locator Embedded Within the Service Registration\nThe second way of getting around this is to add a level of indirection by using a custom delegate.\nThe extension methods for IServiceCollection allow for a service locator to be embedded within a service registration by using an expression that has access to the IServiceProvider.\nTo avoid the problem of a 'captured dependency' when injecting transient dependencies, we can register a delegate signature that wraps the service locator thus moving the service locator up into the registration process itself (as seen here) and leaving our factory unaware of the container technology.\n\nAt this point the factory may be made public (or left as private) as the custom delegate takes care of obtaining the values from the container when the Create method is called and not when the (singleton) factory is created, thus avoiding capturing the dependency.\n\nSpecial warning about scoped dependencies\nWrapping access to transient registered dependencies with a singleton delegate works as both singleton and transient instances are resolved from the root provider. However, this does not work for dependencies registered as scoped lifetimes which are resolved from a scoped provider which the singleton does not have access to.\nIf you use the root provider, you will end up with a captured instance of the scoped type that is created when the singleton is created (as the container will not throw an exception unless scope checking is turned on).\nUnfortunately, for these dependencies, you will still need to pass these to the Create method in most cases.\nIf you are using ASP.NET Core, there is a workaround (already partially illustrated above with accessing the User's identity).\nIHttpContextAcessor.HttpContext.RequestServices exposes the scoped container but is accessible from singleton services as IHttpContextAcessor is registered as a singleton service. Therefore, you could write a delegate that uses this to access the scoped dependencies via a delegate. My advice is to approach this with caution as you may find it hard to debug unexpected behaviour.\n\nGo Create a Product\nIn the example, the factory class is registered as a singleton to avoid the cost of recreating it every time a new Product is needed.\nThe three dependencies are injected into the constructor, but as already mentioned the transient values are not captured at this point - we are only capturing the function pointers.\nIt is within the Create method that we call the delegate functions to obtain the real-time transient values that are then used with the caller provided productName parameter to then instantiate a Product instance.\n \n \nConclusion\nI'll leave things there, as the best way to understand the above is to step through the code at https://github.com/stevetalkscode/RecordFactory that accompanies this post.\nWhilst a factory is not required for the majority of scenarios where you use a record type, it can be of help when you need to encapsulate the logic for gathering all the dependencies that are used to create an instance without polluting the record declaration itself."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/merge-to-monorepo-2",
    "raw": "<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>Following on from <a href=\"https://stevetalkscode.co.uk/merge-to-monorepo-1\">Part 1</a> where I give the background as to the reasons that I wanted to move to a single Git repository (also known as a mono-repo), this post provides a walk-through of the PowerShell script that I created to do the job.</p>\n<p>The full script can be found at&nbsp;<a href=\"https://github.com/stevetalkscode/MigrateToGitMonoRepo\" target=\"_blank\" class=\"externallink\">on GitHub in the MigrateToGitMonoRepo repository.</a> The script make use of three 'dummy' repos that I have also created there. In addition, it also shows how to include repositories from other URLs by pulling in an archived Microsoft repository for MS-DOS.</p>\n<h2 id=\"before-running-the-script\"><a name=\"before-running-the-script\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#before-running-the-script\">Before Running the Script</a></h2>\n<p>There are a few things to be aware of when considering using the script.</p>\n<p>The first is that I am neither a PowerShell nor Git expert. The script has been put together to achieve a goal I had and has been shared in the hope that it may be of use to other people (even if it is just my future self). I am sure there are more elegant ways of using both these tools, but the aim here was to get the job done as it is a 'one-off' process. Please feel free to fork the script and change it as much as you want for your own needs with my blessing.</p>\n<p>The second thing to know is that the Git command line writes information to StdErr and therefore, when running, a lot of Git information will appear in red. All this 'noise' does make it hard to identify genuine errors. To this end, when developing and running the script, I used the PowerShell ISE to add breakpoints and step through the execution of code so I could spot when things were going wrong.</p>\n<p>The last thing to be aware of is that there is no error handling within the script. For example, if a repo can't be found or a branch specified for merging is not present, you may have unexpected results of empty temporary directories being rolled forward and then appearing as errors when Git tries to move and rename those directories.</p>\n<p>With this said, the rest of the post will focus on how to use the script and some things I learnt along the way while writing it.</p>\n<h2 id=\"initialising-variables\"><a name=\"initialising-variables\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#initialising-variables\">Initialising Variables</a></h2>\n<p>At the start of the script there are a number of variables that you will need to set.</p>\n<!--?# Gist 9d1177afb467b21d8ad0780023f24b22 /?-->\n<p>The <strong>$GitTargetRoot</strong> and <strong>$GitTargetFolder</strong> refer to the file system directory structure. You may not want to have a double nested directory structure you can override this further down in the script. The reason I did this is that I like to have a single root for all my Git repos on the file system (C:\\Git) and then a directory per repo under this.</p>\n<p>The <strong>$GitArchiveFolder</strong> and <strong>$GitArchiveTags</strong> will be used as part of the paths in the target repo to respectively group all the existing branches and existing tags together so that there is less 'noise' when navigating to branches and tags created post-merge.</p>\n<p>If all the existing repositories have the same root URL it can be set in the <strong>$originRoot</strong> variable. This can be overridden later on in the script to bring in repositories from disparate sources (in the script example, we pull in the MS-DOS archive repository from Microsoft's GitHub account).</p>\n<p>While the merge is in progress, it is important to avoid clashes with directory names on the file system and branch names in Git.</p>\n<p>The <strong>$newMergeTarget</strong> and <strong>$TempFolderPrefix</strong> are used for the purpose of creating non-clashing versions of these. There is a clean up at the end of the script to rename temporary folders on the file system. The script does not automatically rename the target branch as this should be a manual process after the merge when ready to push to a new origin.</p>\n<h1 id=\"define-the-source-repositories-and-merge-behaviour\">Define the Source Repositories and Merge Behaviour</h1>\n<!--?# Gist be5b6dbd0d8e832c6023fe17b0ba93a5 /?-->\n<p>The next stage in the script is to define all the existing repositories that you want to merge into a single repository. To keep the script agnostic in terms of PowerShell versions, I have used the pscustomobject type instead of using classes (supported from PowerShell 5 onwards).</p>\n<p>In each entry, the following values should be set:</p>\n<p><strong>originRoot</strong> is usually left as an empty string to indicate that the root specified globally at the start of the script should be used. In the example, the last entry demonstrates pulling in a repo from a different origin.</p>\n<p><strong>repo</strong> is the repository within the origin. In the example I have three dummy repositories that I have created in my GitHub account that can be used as a trial run to get used to the script works before embarking on using your own repositories.</p>\n<p><strong>folder</strong> is the file system directory that the contents of the repository will be moved to once the migration is complete. This is used to ensure that there are no clashes between directories of the same name within different repositories. You are free to change how the overall hierarchy is structured once the migration is complete.</p>\n<p><strong>subDirectory</strong> is usually an empty string, but if you have several repositories that are you want to logically group together in the file system hierarchy, you can set folder to the same value, E.g. Archived and then use subDirectory to then define the target for each repo under that common area.</p>\n<p><strong>mergeBranch</strong> is the branch in the source repository that you want to merge into the common branch specified in $newMergeTarget. In most cases, this will be your 'live' branch with a name like main, master, develop or build. If left as an empty string, the repository will be included in the new mono-repo, but will effectively be orphaned into the archive branches and tags.</p>\n<p>In my real-world case , the team had a few repositories that were created and had code committed, but the code never went anywhere, so not needed in the new main branch. However, we still want access to the contents for reference.</p>\n<p><strong>tempFolder</strong> is a belt-and-braces effort to ensure that there are no folder clashes if the new folder name in&nbsp;<strong>folder</strong> happens to exist in another repository while merging. The value here will be appended to the global <strong>$TempFolderPrefix</strong> with the intention of creating a unique directory name.</p>\n<h2 id=\"file-system-clean-up\"><a name=\"file-system-clean-up\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#file-system-clean-up\">File System Clean Up</a></h2>\n<p>Before getting into the main process loop, the script does some cleaning up to ensure that previous runs of the script are deleted from the file system to ensure a clean run. You may want to change this if you want to compare results so that previous runs are archived by renaming the folder .</p>\n<p>Once cleaned up, a new Git repository is created and an initial commit is created in the new branch. This is required so that Git merges can take place herein.</p>\n<h2 id=\"the-main-loop\"><a name=\"the-main-loop\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-main-loop\">The Main Loop</a></h2>\n<p>With the array of source metadata created, we move into the main loop. I won't go into a line by line breakdown here, but instead give an overview of the process.</p>\n<p>The first thing to do for each repository is to set it as an origin and pull down all the branches to the file system.&nbsp;An important thing to note about the Git Pull is the <em>--allow-unrelated-histories</em> switch. Without this, Git will complain about no common histories to be able to merge.</p>\n<hr>\n<p>As as aside, if your source repository is large, this may take some time. When developing the script, I thought something had gone wrong - it hadn't - it was just slow.</p>\n<hr>\n<p>With that done, we can then enter a loop of iterating through each branch and checking it out to its new branch name in the new repository (in effect, performing a logical move of the branch into an archive branch, but really this is just using branch naming conventions to create a logical hierarchy).</p>\n<p>You may notice some pattern matching going on in this area of the script. The reason for this is that the Git branch -r command to list all the remote branches includes a line indicating where the orgin/HEAD is pointing. We do not need this as we are only interested in the actual branch names.</p>\n<!--?# LinkSizedImage \"CreateCommonRepo2_GitRemoteBranchHead.png\" Alt=\"Screen shot of Git output when listing remote branches\" /?-->\n<p>Once all the branches have been checked out and renamed, we return back to our common branch and remote the remote.</p>\n<p>At this point, if we have specified a branch to merge into our common branch, the script will then</p>\n<ul>\n<li>merge the specified branch, again using the --allow-unrelated-histories switch to let Git know that the merge has no common history to work with</li>\n<li>create a temporary folder (as defined in the array of metadata) in the common branch</li>\n<li>move the complete contents of the branch to that temporary folder</li>\n</ul>\n<p>Care is needed in this last step once we have performed the first merge as the common folder will include previously merged repositories in their temporary folders. Therefore, to avoid these temporary folders being moved, we build up a list of the temporary folders we have created on each iteration and them to the exclude list that is fed into the Git mv command.</p>\n<p>At this point, an error can creep in if the branch name specified in the item metadata does not exist in the source repository. When writing the script I received Git errors indicating there were no files to move and ended up with empty temporary folders littered around the new repository.</p>\n<p>Again, you may choose to put some error handling in or, on the other hand, just correct the branch name and repeat the process from the start again.</p>\n<p>Before moving to the next item in the metadata array, the script copies all the tags to the the logical folder of tags specified in $GitArchiveTags.</p>\n<h2 id=\"the-post-migration-clean-up\"><a name=\"post-migration-clean-up\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#post-migration-clean-up\">The Post Migration Clean Up</a></h2>\n<p>Once the migration has completed, there is a bit of tidying up to do.</p>\n<p>If you remember, to avoid clashes between directories while the migration takes place, we used temporary directory names. We now need to do a sweep through to rename those temporary directory names to the intended destination names.</p>\n<p>At this point, we are ready with the final mono-repo.</p>\n<p>If you have run the script 'as-is' using my demo values, when you look on your file system, it should like like this</p>\n<!--?# LinkSizedImage \"CreateCommonRepo2_FolderList.png\" Alt=\"Screen shot of file system using the examples in the script\" /?-->\n<p>If you use a tool such as Atlassian SourceTree, you get a visual idea of what we have achieved with the merge process.</p>\n<!--?# LinkSizedImage \"CreateCommonRepo2_GitVisual.png\" Alt=\"Screen shot of SourceTree view of the migrated repository using the examples in the script\" /?-->\n<h2 id=\"before-pushing-to-a-remote\"><a name=\"#before-pushing-to-a-remote\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#before-pushing-to-a-remote\">Before Pushing to a Remote</a></h2>\n<p>With our migrated repository, we are now almost ready to push it up to a remote (be it GitHub, Azure DevOps, BitBucket et al).</p>\n<p>However, at this point you may want to do some tidying up of renaming the __RepoMigration branch to main.</p>\n<p>The repository is now in a state where you are ready to push it to a remote 'as-is'. On the other hand, you may want to create an empty repository in the remote up front and then merge the migrated repository into it. If you do this, remember to use the # git pull --all --allow-unrelated-histories -v after adding the new remote.</p>\n<p>At the end of the script, there is a commented out section that provides the commands I used to push up all the branches and tags created.</p>\n<p>Alternatively, you may want to take manual control via the Git command line (or a GUI tool such as SourceTree).</p>\n<h2 id=\"lessons-learnt\"><a name=\"lessons-learnt\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#lessons-learnt\">Lessons Learnt</a></h2>\n<p>I have already mentioned earlier about problems with non-existent branches being specified, but there are other things to know.</p>\n<p>My first piece of advice is to use the PowerShell Integrated Script Editor (ISE) to single step your way through the script using my dummy repositories to familiarise yourself with how the script works.</p>\n<p>Once familiar, start with using one or two if your own repositories that are small and simple to migrate, to get a feel for how you want to merge branches into the new 'main' branch.</p>\n<p>By single stepping, you will get instant feedback of errors occurring. As mentioned above, because Git writes to StdErr, it is hard to tease out the errors if running the script from start to finish.</p>\n<p>Next, don't automate pushing the results to your remote until you are happy that there is nothing missing and that the merges specified meet how you want to take the repository forward.</p>\n<p>If you use a tool like SourceTree, don't leave it running while the migration is taking place. Whilst it feels useful to graphically see what is happening while the script is running. it slows the process down and can in some cases cause the script to fail as files may become locked. Wait until the migration is complete and then open SourceTree to get a visual understanding of the changes made.</p>\n<p>My last lesson is to have patience.</p>\n<p>When I worked on this using real repositories, some of which had many years of histories, there are some heart-stopping moments when the repositories are being pulled down and it feels like something has gone wrong, but it hasn't - it's just Git doing its thing, albeit slowly!</p>\n<h2 id=\"moving-forward\"><a name=\"moving-forward\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#moving-forward\">Moving Forward</a></h2>\n<p>One of the downsides of mono-repos is size. In my real-world scenario that inspired this script and blog, the final migrated repo is 1.4GB in size. This is not massive compared to the likes of some well known mono-repos that are in the hundreds of gigabytes in size.</p>\n<p>Once you have pushed the repository up to a remote, my advise is to clone the repo into a different local directory and only checkout the main branch (especially if you have a lot of orphaned archive branches that you don't need to pull).</p>\n<p>If disk size is still an issue, it is worth looking at the <a href=\"https://github.com/microsoft/VFSForGit\" target=\"_blank\" class=\"externallink\">Git Virtual File System</a> to limit the files that are pulled down to your local system\n&nbsp;</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>I hope that the two posts and the script are of help to people.</p>\n<p>There is a lot of debate about the relative merits of poly-repo vs. mono-repo that I haven't gone into. My view is to do what fits best and enables your team' to work with minimal friction.</p>\n<p>The reason for the migration that inspired this post was having difficulties in coordinating a release for a distributed monolith that was spread across several repositories. If you have many repos that have very little to do with one another (being true microservices or completely unrelated projects), there is probably no benefit to moving to a mono-repo.</p>\n<p>In summary, to use a well worn cliché, \"it depends\".</p>\n",
    "sanitized": "Background\nFollowing on from Part 1 where I give the background as to the reasons that I wanted to move to a single Git repository (also known as a mono-repo), this post provides a walk-through of the PowerShell script that I created to do the job.\nThe full script can be found at on GitHub in the MigrateToGitMonoRepo repository. The script make use of three 'dummy' repos that I have also created there. In addition, it also shows how to include repositories from other URLs by pulling in an archived Microsoft repository for MS-DOS.\nBefore Running the Script\nThere are a few things to be aware of when considering using the script.\nThe first is that I am neither a PowerShell nor Git expert. The script has been put together to achieve a goal I had and has been shared in the hope that it may be of use to other people (even if it is just my future self). I am sure there are more elegant ways of using both these tools, but the aim here was to get the job done as it is a 'one-off' process. Please feel free to fork the script and change it as much as you want for your own needs with my blessing.\nThe second thing to know is that the Git command line writes information to StdErr and therefore, when running, a lot of Git information will appear in red. All this 'noise' does make it hard to identify genuine errors. To this end, when developing and running the script, I used the PowerShell ISE to add breakpoints and step through the execution of code so I could spot when things were going wrong.\nThe last thing to be aware of is that there is no error handling within the script. For example, if a repo can't be found or a branch specified for merging is not present, you may have unexpected results of empty temporary directories being rolled forward and then appearing as errors when Git tries to move and rename those directories.\nWith this said, the rest of the post will focus on how to use the script and some things I learnt along the way while writing it.\nInitialising Variables\nAt the start of the script there are a number of variables that you will need to set.\n\nThe $GitTargetRoot and $GitTargetFolder refer to the file system directory structure. You may not want to have a double nested directory structure you can override this further down in the script. The reason I did this is that I like to have a single root for all my Git repos on the file system (C:\\Git) and then a directory per repo under this.\nThe $GitArchiveFolder and $GitArchiveTags will be used as part of the paths in the target repo to respectively group all the existing branches and existing tags together so that there is less 'noise' when navigating to branches and tags created post-merge.\nIf all the existing repositories have the same root URL it can be set in the $originRoot variable. This can be overridden later on in the script to bring in repositories from disparate sources (in the script example, we pull in the MS-DOS archive repository from Microsoft's GitHub account).\nWhile the merge is in progress, it is important to avoid clashes with directory names on the file system and branch names in Git.\nThe $newMergeTarget and $TempFolderPrefix are used for the purpose of creating non-clashing versions of these. There is a clean up at the end of the script to rename temporary folders on the file system. The script does not automatically rename the target branch as this should be a manual process after the merge when ready to push to a new origin.\nDefine the Source Repositories and Merge Behaviour\n\nThe next stage in the script is to define all the existing repositories that you want to merge into a single repository. To keep the script agnostic in terms of PowerShell versions, I have used the pscustomobject type instead of using classes (supported from PowerShell 5 onwards).\nIn each entry, the following values should be set:\noriginRoot is usually left as an empty string to indicate that the root specified globally at the start of the script should be used. In the example, the last entry demonstrates pulling in a repo from a different origin.\nrepo is the repository within the origin. In the example I have three dummy repositories that I have created in my GitHub account that can be used as a trial run to get used to the script works before embarking on using your own repositories.\nfolder is the file system directory that the contents of the repository will be moved to once the migration is complete. This is used to ensure that there are no clashes between directories of the same name within different repositories. You are free to change how the overall hierarchy is structured once the migration is complete.\nsubDirectory is usually an empty string, but if you have several repositories that are you want to logically group together in the file system hierarchy, you can set folder to the same value, E.g. Archived and then use subDirectory to then define the target for each repo under that common area.\nmergeBranch is the branch in the source repository that you want to merge into the common branch specified in $newMergeTarget. In most cases, this will be your 'live' branch with a name like main, master, develop or build. If left as an empty string, the repository will be included in the new mono-repo, but will effectively be orphaned into the archive branches and tags.\nIn my real-world case , the team had a few repositories that were created and had code committed, but the code never went anywhere, so not needed in the new main branch. However, we still want access to the contents for reference.\ntempFolder is a belt-and-braces effort to ensure that there are no folder clashes if the new folder name in folder happens to exist in another repository while merging. The value here will be appended to the global $TempFolderPrefix with the intention of creating a unique directory name.\nFile System Clean Up\nBefore getting into the main process loop, the script does some cleaning up to ensure that previous runs of the script are deleted from the file system to ensure a clean run. You may want to change this if you want to compare results so that previous runs are archived by renaming the folder .\nOnce cleaned up, a new Git repository is created and an initial commit is created in the new branch. This is required so that Git merges can take place herein.\nThe Main Loop\nWith the array of source metadata created, we move into the main loop. I won't go into a line by line breakdown here, but instead give an overview of the process.\nThe first thing to do for each repository is to set it as an origin and pull down all the branches to the file system. An important thing to note about the Git Pull is the --allow-unrelated-histories switch. Without this, Git will complain about no common histories to be able to merge.\n\nAs as aside, if your source repository is large, this may take some time. When developing the script, I thought something had gone wrong - it hadn't - it was just slow.\n\nWith that done, we can then enter a loop of iterating through each branch and checking it out to its new branch name in the new repository (in effect, performing a logical move of the branch into an archive branch, but really this is just using branch naming conventions to create a logical hierarchy).\nYou may notice some pattern matching going on in this area of the script. The reason for this is that the Git branch -r command to list all the remote branches includes a line indicating where the orgin/HEAD is pointing. We do not need this as we are only interested in the actual branch names.\n\nOnce all the branches have been checked out and renamed, we return back to our common branch and remote the remote.\nAt this point, if we have specified a branch to merge into our common branch, the script will then\n\nmerge the specified branch, again using the --allow-unrelated-histories switch to let Git know that the merge has no common history to work with\ncreate a temporary folder (as defined in the array of metadata) in the common branch\nmove the complete contents of the branch to that temporary folder\n\nCare is needed in this last step once we have performed the first merge as the common folder will include previously merged repositories in their temporary folders. Therefore, to avoid these temporary folders being moved, we build up a list of the temporary folders we have created on each iteration and them to the exclude list that is fed into the Git mv command.\nAt this point, an error can creep in if the branch name specified in the item metadata does not exist in the source repository. When writing the script I received Git errors indicating there were no files to move and ended up with empty temporary folders littered around the new repository.\nAgain, you may choose to put some error handling in or, on the other hand, just correct the branch name and repeat the process from the start again.\nBefore moving to the next item in the metadata array, the script copies all the tags to the the logical folder of tags specified in $GitArchiveTags.\nThe Post Migration Clean Up\nOnce the migration has completed, there is a bit of tidying up to do.\nIf you remember, to avoid clashes between directories while the migration takes place, we used temporary directory names. We now need to do a sweep through to rename those temporary directory names to the intended destination names.\nAt this point, we are ready with the final mono-repo.\nIf you have run the script 'as-is' using my demo values, when you look on your file system, it should like like this\n\nIf you use a tool such as Atlassian SourceTree, you get a visual idea of what we have achieved with the merge process.\n\nBefore Pushing to a Remote\nWith our migrated repository, we are now almost ready to push it up to a remote (be it GitHub, Azure DevOps, BitBucket et al).\nHowever, at this point you may want to do some tidying up of renaming the __RepoMigration branch to main.\nThe repository is now in a state where you are ready to push it to a remote 'as-is'. On the other hand, you may want to create an empty repository in the remote up front and then merge the migrated repository into it. If you do this, remember to use the # git pull --all --allow-unrelated-histories -v after adding the new remote.\nAt the end of the script, there is a commented out section that provides the commands I used to push up all the branches and tags created.\nAlternatively, you may want to take manual control via the Git command line (or a GUI tool such as SourceTree).\nLessons Learnt\nI have already mentioned earlier about problems with non-existent branches being specified, but there are other things to know.\nMy first piece of advice is to use the PowerShell Integrated Script Editor (ISE) to single step your way through the script using my dummy repositories to familiarise yourself with how the script works.\nOnce familiar, start with using one or two if your own repositories that are small and simple to migrate, to get a feel for how you want to merge branches into the new 'main' branch.\nBy single stepping, you will get instant feedback of errors occurring. As mentioned above, because Git writes to StdErr, it is hard to tease out the errors if running the script from start to finish.\nNext, don't automate pushing the results to your remote until you are happy that there is nothing missing and that the merges specified meet how you want to take the repository forward.\nIf you use a tool like SourceTree, don't leave it running while the migration is taking place. Whilst it feels useful to graphically see what is happening while the script is running. it slows the process down and can in some cases cause the script to fail as files may become locked. Wait until the migration is complete and then open SourceTree to get a visual understanding of the changes made.\nMy last lesson is to have patience.\nWhen I worked on this using real repositories, some of which had many years of histories, there are some heart-stopping moments when the repositories are being pulled down and it feels like something has gone wrong, but it hasn't - it's just Git doing its thing, albeit slowly!\nMoving Forward\nOne of the downsides of mono-repos is size. In my real-world scenario that inspired this script and blog, the final migrated repo is 1.4GB in size. This is not massive compared to the likes of some well known mono-repos that are in the hundreds of gigabytes in size.\nOnce you have pushed the repository up to a remote, my advise is to clone the repo into a different local directory and only checkout the main branch (especially if you have a lot of orphaned archive branches that you don't need to pull).\nIf disk size is still an issue, it is worth looking at the Git Virtual File System to limit the files that are pulled down to your local system\n \nConclusion\nI hope that the two posts and the script are of help to people.\nThere is a lot of debate about the relative merits of poly-repo vs. mono-repo that I haven't gone into. My view is to do what fits best and enables your team' to work with minimal friction.\nThe reason for the migration that inspired this post was having difficulties in coordinating a release for a distributed monolith that was spread across several repositories. If you have many repos that have very little to do with one another (being true microservices or completely unrelated projects), there is probably no benefit to moving to a mono-repo.\nIn summary, to use a well worn cliché, \"it depends\"."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/merge-to-monorepo-1",
    "raw": "<p>Following on from my <a href=\"https://stevetalkscode.co.uk/install-octopus-with-domain-account\">last blog</a> about the problems I had setting up Octopus Deploy with a service account, this is another DevOps related post that describes the approach I have taken to merging multiple Git repositories into a single Git repository (commonly known as a mono-repo).</p>\n<h2 id=\"disclaimer\"><a name=\"disclaimer\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#disclaimer\">Disclaimer</a></h2>\n<p>To be clear, I am not going to provide a wide ranging discussion about the relative merits and disadvantages of using a mono-repo for source control vs. having one repository per project (poly-repo).</p>\n<p>In the end it comes down to what works best for the team to manage the overall code base by reducing friction.</p>\n<p>If you have lots of disparate projects that have no impact on each other's existence or a true micro-service architecture where each service is managed within its own repository, there is very little point in bringing these into a mono-repo.</p>\n<p>If on the other hand you have a distributed monolith where a feature requests or bug fixes may be spread across several repositories and require synchronisation when negotiating their way through the CI/CD pipeline (or worse having to jump through hoops to develop or test in concert while developing on your local machine), then a move to mono-repo may be of benefit.</p>\n<p>There is no 'one size fits all' and you may end up with a hybrid of some projects occupying their own repositories, whilst others live in one big repository.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>What prompted the need for a move to a mono-repo in my case was having to coordinate features within a distributed monolith where a feature request may span one, some or all of four key repositories and the only coordinating factor is a ticket number in the branch names used in each of the repositories.</p>\n<p>This causes problems when having to context switch between multiple issues and making sure that</p>\n<ul>\n<li>the correct branches are checked out in the repositories</li>\n<li>configuration files are amended to point to appropriate local or remote instances of services</li>\n<li>ensuring pull requests to branches monitored by TeamCity are coordinated as these also trigger Octopus Deploy to deploy to our common development environment</li>\n<li>version numbers for different projects are understood and the inter-relationships are documented.</li>\n</ul>\n<p>Now moving to a mono-repo is not going to solve all these problems, but it is the first step on the road.</p>\n<h2 id=\"moving-to-a-whole-new-world\"><a name=\"whole-new-world\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#whole-new-world\">(Moving To) A Whole New World</a></h2>\n<p>As described in my previous blog post, the team I am currently working with is in the process of completely rebuilding the CI/CD pipeline with the latest versions of Team City and Octopus Deploy.</p>\n<p>This has provided the ideal opportunity to migrate from the current poly-repo structure to a new mono-repo. But how should we approach it?</p>\n<h3 id=\"approaches-to-consider\"><a name=\"approaches-to-consider\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#approaches-to-consider\">Approaches to Consider</a></h3>\n<p>At it's simplest, we could just take a copy of the current 'live' code and paste it into a new repository. The problem with this would be the loss of the ability to look at the (decade long) history in the context of the current repository. Instead, this would require hopping over to the existing repositories to view the history of files. We could live with, but is not ideal as it introduces friction of a different kind.</p>\n<p>So, somehow, we need to try to migrate everything to one place, but this comes with complications.</p>\n<p>In each of the current repositories, the source code is held at the root of each repo, so when trying to merge the repositories 'as-is', it introduces problems when trying to merge the contents of each of the existing repositories as it will cause no end of merge conflicts and muddy the code base. Therefore, the first thing we will need to do is to move the source code down from the root into dedicated (uniquely named) folders.</p>\n<p>This could be done within the existing repositories before we think about merging repositories. However, this will mean having to revisit all the existing Team City projects to repoint the watched projects to the new folders. This also causes disruption to any current work that is in progress. So this approach should be ruled out.</p>\n<p>There is also the problem of what to do with all the branches and tags in the old repositories. Ideally we want to also bring them along into the new repository, but we have a similar problem regarding trying to avoid naming conflicts (as I mentioned above, the branch names are the coordinating factor that we currently use, so these will be the same in each of the repositories where code has changed for a particular feature), so these will need renaming as well.</p>\n<h2 id=\"ill-tell-you-what-i-want-what-i-really-really-want\"><a name=\"what-i-really-really-want\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#what-i-really-really-want\">I'll Tell You What I Want (What I Really, Really Want)</a></h2>\n<p>With all the above in mind, we need a migration plan that can accommodate the following requirements:</p>\n<ul>\n<li>No changes required to the existing repositories</li>\n<li>The full history needs to be migrated</li>\n<li>All live branches need to be migrated</li>\n<li>All tags need to be migrated</li>\n<li>Avoid clashes between migrated repositories when brought into a single structure</li>\n<li>Allow for a pre-migration to be run so that the new Team City can be set up without impacting the existing repositories and existing CI/CD pipeline</li>\n<li>The process must be repeatable with minimum effort so that any problems can be identified and corrected, but also so that the new CI/CD pipeline can be built in preparation for a low-impact cut-over.</li>\n</ul>\n<p>At first this seemed like a tall order, but ultimately what this boils down to is creating a new repository and then repeating the following steps for each legacy repository to be merged:</p>\n<ul>\n<li>Pull each legacy repository the new repository</li>\n<li>Rename the legacy branches and tags to they do not clash</li>\n<li>Select a 'live' branch to merge into the main branch of the new repository and check it out</li>\n<li>Move the content of the 'live' branch to a sub-folder that will not clash as other repositories are subsequently migrated</li>\n<li>Merge the 'live' branch into the main branch</li>\n</ul>\n<p>These steps can all be achieved by a combination of Git commands and file system commands which can be put together into a script.</p>\n<p>In <a href=\"https://stevetalkscode.co.uk/merge-to-monorepo-2\">Part 2</a>, I will show you how I created a PowerShell script to achieve the goal.</p>\n",
    "sanitized": "Following on from my last blog about the problems I had setting up Octopus Deploy with a service account, this is another DevOps related post that describes the approach I have taken to merging multiple Git repositories into a single Git repository (commonly known as a mono-repo).\nDisclaimer\nTo be clear, I am not going to provide a wide ranging discussion about the relative merits and disadvantages of using a mono-repo for source control vs. having one repository per project (poly-repo).\nIn the end it comes down to what works best for the team to manage the overall code base by reducing friction.\nIf you have lots of disparate projects that have no impact on each other's existence or a true micro-service architecture where each service is managed within its own repository, there is very little point in bringing these into a mono-repo.\nIf on the other hand you have a distributed monolith where a feature requests or bug fixes may be spread across several repositories and require synchronisation when negotiating their way through the CI/CD pipeline (or worse having to jump through hoops to develop or test in concert while developing on your local machine), then a move to mono-repo may be of benefit.\nThere is no 'one size fits all' and you may end up with a hybrid of some projects occupying their own repositories, whilst others live in one big repository.\nBackground\nWhat prompted the need for a move to a mono-repo in my case was having to coordinate features within a distributed monolith where a feature request may span one, some or all of four key repositories and the only coordinating factor is a ticket number in the branch names used in each of the repositories.\nThis causes problems when having to context switch between multiple issues and making sure that\n\nthe correct branches are checked out in the repositories\nconfiguration files are amended to point to appropriate local or remote instances of services\nensuring pull requests to branches monitored by TeamCity are coordinated as these also trigger Octopus Deploy to deploy to our common development environment\nversion numbers for different projects are understood and the inter-relationships are documented.\n\nNow moving to a mono-repo is not going to solve all these problems, but it is the first step on the road.\n(Moving To) A Whole New World\nAs described in my previous blog post, the team I am currently working with is in the process of completely rebuilding the CI/CD pipeline with the latest versions of Team City and Octopus Deploy.\nThis has provided the ideal opportunity to migrate from the current poly-repo structure to a new mono-repo. But how should we approach it?\nApproaches to Consider\nAt it's simplest, we could just take a copy of the current 'live' code and paste it into a new repository. The problem with this would be the loss of the ability to look at the (decade long) history in the context of the current repository. Instead, this would require hopping over to the existing repositories to view the history of files. We could live with, but is not ideal as it introduces friction of a different kind.\nSo, somehow, we need to try to migrate everything to one place, but this comes with complications.\nIn each of the current repositories, the source code is held at the root of each repo, so when trying to merge the repositories 'as-is', it introduces problems when trying to merge the contents of each of the existing repositories as it will cause no end of merge conflicts and muddy the code base. Therefore, the first thing we will need to do is to move the source code down from the root into dedicated (uniquely named) folders.\nThis could be done within the existing repositories before we think about merging repositories. However, this will mean having to revisit all the existing Team City projects to repoint the watched projects to the new folders. This also causes disruption to any current work that is in progress. So this approach should be ruled out.\nThere is also the problem of what to do with all the branches and tags in the old repositories. Ideally we want to also bring them along into the new repository, but we have a similar problem regarding trying to avoid naming conflicts (as I mentioned above, the branch names are the coordinating factor that we currently use, so these will be the same in each of the repositories where code has changed for a particular feature), so these will need renaming as well.\nI'll Tell You What I Want (What I Really, Really Want)\nWith all the above in mind, we need a migration plan that can accommodate the following requirements:\n\nNo changes required to the existing repositories\nThe full history needs to be migrated\nAll live branches need to be migrated\nAll tags need to be migrated\nAvoid clashes between migrated repositories when brought into a single structure\nAllow for a pre-migration to be run so that the new Team City can be set up without impacting the existing repositories and existing CI/CD pipeline\nThe process must be repeatable with minimum effort so that any problems can be identified and corrected, but also so that the new CI/CD pipeline can be built in preparation for a low-impact cut-over.\n\nAt first this seemed like a tall order, but ultimately what this boils down to is creating a new repository and then repeating the following steps for each legacy repository to be merged:\n\nPull each legacy repository the new repository\nRename the legacy branches and tags to they do not clash\nSelect a 'live' branch to merge into the main branch of the new repository and check it out\nMove the content of the 'live' branch to a sub-folder that will not clash as other repositories are subsequently migrated\nMerge the 'live' branch into the main branch\n\nThese steps can all be achieved by a combination of Git commands and file system commands which can be put together into a script.\nIn Part 2, I will show you how I created a PowerShell script to achieve the goal."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/install-octopus-with-domain-account",
    "raw": "<p>This post is primarily for my future self to document how to deal with the problems I had installing Octopus Deploy using a domain account as the service account.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>I am currently working with a team that has old versions of TeamCity and Octopus Deploy and want to move to the latest versions. The upgrade path from these (very old) versions to the latest versions is complicated and therefore we have set up a new server to host the latest versions.</p>\n<p>The (virtual) server we will be installing on is hosted in a managed environment (with no self-service facilities). This makes taking snapshots/checkpoints to rollback to if things go wrong&nbsp; during installation a bit more complicated.&nbsp;It also means that we effectively only get one shot at getting things right.&nbsp;</p>\n<p>For these reasons,&nbsp;I created a small test lab at home to practice all the steps required so that I could create an installation step-by-step 'run-book' to do the real installation with as few hiccups as possible.</p>\n<p>In my test lab I have the following set up as Hyper-V virtual machines:</p>\n<ul>\n<li>A Windows 2016 server that is the Active Directory Domain Controller and hosts a SQL Server 2017 database server</li>\n<li>A Windows 2016 server that doubles up as the TeamCity and Octopus Deploy server</li>\n<li>A Windows 2012 server that will be a target server for deploying web sites to via an Octopus Deploy Tentacle.</li>\n</ul>\n<p>This three server set-up does not truly represent a real-world environment (for example, the SQL Server would not normally reside on the domain controller), but is enough to emulate the core elements of the environment that I will be using in the real world.</p>\n<h2 id=\"the-challenge\"><a name=\"the-challenge\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-challenge\">The Challenge</a></h2>\n<p>The previous installation of Octopus Deploy used a local Windows system account, but for the new installation, we want to use a domain account</p>\n<ol>\n<li>to connect to SQL Server using Windows Authentication and&nbsp;</li>\n<li>to have tight control over what the service can do vs the local system account <a href=\"https://docs.microsoft.com/en-us/windows/win32/ad/the-localsystem-account\" target=\"_blank\" class=\"externallink\">which is tantamount to having full local administrator rights.</a></li>\n</ol>\n<h2 id=\"preparing-the-windows-environment\"><a name=\"preparing-the-windows-environment\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#preparing-the-windows-environment\">Preparing the Windows Environment</a></h2>\n<p><a href=\"https://octopus.com/docs/installation/permissions-for-the-octopus-windows-service\" target=\"_blank\" class=\"externallink\">The details of the requirements for the domain service account are provided on the Octopus Deploy web site</a>.</p>\n<p>At time of writing, the details are a bit vague in places and require further information.</p>\n<p>Based on this, I went through the following actions.</p>\n<ul>\n<li>Create the Windows domain account with password locked down so it can be used as a service account</li>\n</ul>\n<p><img src=\"https://stevetalkscode.co.uk/images/InstallOctopusBlog_CreateServiceAccount.png\" alt=\"Screen shot of setting password rules on windows account to be used as a service account\"></p>\n<p>Then on the server that Octopus Deploy will be installed on:</p>\n<ul>\n<li>Create a folder called 'Octopus' on the D: drive for the Octopus Deploy application to reside in and grant Full Control to the Octopus service account created in AD</li>\n<li>Create a folder called 'OctopusData' on the E: drive that will be used as the Octopus 'home' location and grant Full Control to the Octopus service account created in AD</li>\n<li>Grant the Octopus service account created in AD the Log on as a Service permission.&nbsp;</li>\n</ul>\n<h2 id=\"preparing-the-database\"><a name=\"preparing-the-database\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#preparing-the-database\">Preparing the Database</a></h2>\n<p>Details of the requirements for the SQL Server version and creating a database can be found on the <a href=\"https://octopus.com/docs/installation/sql-server-database\" target=\"_blank\" class=\"externallink\">Octopus Deploy website</a>&nbsp;but in summary, if the SQL Server is not on the same server as Octopus, the following needs to be done before installation starts.</p>\n<ul>\n<li>Ensure network connectively to the server over TCP/IP</li>\n<li>Ensure no firewall rules preventing access over TCP/IP on appropriate ports between the server hosting SQL Server and the server hosting Octopus Deploy</li>\n<li>Create an empty database in SQL Server 2017 called Octopus&nbsp;</li>\n<li>Set the collation of the database to case insensitive using<br>\nALTER DATABASE [Octopus] COLLATE Latin1_General_CI_AS</li>\n<li>Create a SQL Server login for the Octopus service account created in AD</li>\n<li>Create a user in the Octopus database for the Octopus service account login</li>\n<li>Set the default schema for the user to [dbo] so the account has full control over the database.</li>\n</ul>\n<h2 id=\"next-steps-before-installation\"><a name=\"next-steps-before-installation\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#next-steps-before-installation\">Next Steps Before Installation</a></h2>\n<p>At this point there are some other things that I should have done, but did not find out until I had problems further down the line.</p>\n<p>To view these next steps, <a href=\"https://stevetalkscode.co.uk/#FixAccessDenied\">scroll down to here</a>.</p>\n<h3 id=\"starting-the-installation\"><a name=\"starting-the-installation\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#starting-the-installation\">Starting the Installation</a></h3>\n<p>Starting the installation is simple enough, with just one change from the default option, and that is to change the installation folder to D:\\Octopus (where permissions have already been granted).</p>\n<p>On clicking 'Finish' there is a brief pause while PowerShell is checked and then the 'Getting Started' Wizard is opened.</p>\n<h3 id=\"using-the-setup-wizard\"><a name=\"using-the-setup-wizard\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#using-the-setup-wizard\">Using the Setup Wizard</a></h3>\n<p>After entering the licence key on the first page, the next step is to the change the 'Home' directory to the E: drive where there is plenty of disk space.</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_ChangeHomeDirectory.png\" Alt=\"Screen shot of selecting home directory for Octopus Deploy\" /?-->\n<p>Clicking next to move to the next page, it is time to select the service account.</p>\n<p>By default, this is set to the Local System account, but as described above, we want to use a custom domain account, so we change the radio button option and enter the domain credentials (using the Select User to find the service account created in Active Directory).</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_SelectSerivceAccount-1.png\" Alt=\"Screen shot of service account selection from Octopus Deploy\" /?-->\n<p>The next step is to point to the Octopus database that has been created in SQL Server 2017 and for which the service account has been granted DBO rights so that the database tables and other artefacts can be created.&nbsp;</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_SetDatabase.png\" Alt=\"Screen shot of database screen from Octopus Deploy\" /?-->\n<p>The next screen prompts for a port for the web portal. As the server that I am installing on has IIS, TeamCity and a couple of other self hosted web applications, it is not appropriate to use the default port 80. In the example below I am using port 93.</p>\n<p>This is one of the areas where things started to go wrong later down the track!</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_SetWebPortal.png\" Alt=\"Screen shot of the Octopus Deploy wizard page for selecting the Web Portal port to listen on\" /?-->\n<p>Moving on to the next page, this is where the administrator for Octopus Deploy is selected.</p>\n<p>This is the next place that there are a few gotchas!</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_SelectAdministrator.png\" Alt=\"Screen shot of Select Administrator screen\" /?-->\n<p>So the screen looks innocent enough. When you first arrive on the screen , it defaults the Authentication Mode drop down to using usernames and passwords in Octopus. However, to make life simpler, we want to use Active Directory. When this is selected, the username will default to Administrator.</p>\n<p>DO NOT JUST ACCEPT THIS! This will be the local Administrator account for the machine you are installing on.</p>\n<p>Instead, use the Select User link to bring up the standard Windows dialog for selecting a domain user.&nbsp;</p>\n<p>As I was installing in a test lab, I was a bit lazy and just selected the Domain Administrator account. DO NOT DO THIS EITHER!</p>\n<p>The problem that you will find (after installation is complete), is that, by default, the built in Domain Administrator account does not have a User Principal Name (UPN) and without a UPN, Octopus cannot find the account in Active Directory.</p>\n<p>When this happens, you get a \"No fallback name was provided\" error when trying to login.</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_NoFallback.png\" Alt=\"Screen shot of Error screen on login\" /?-->\n<p>At that point, you are pretty much locked out of Octopus and either have to add a UPN to the Domain Administrator account or re-install Octopus from scratch to use another user as the Octopus administrator (though you may be able to change the authentication provider through the command line (I haven't tried this! See <a href=\"https://octopus.com/docs/octopus-rest-api/octopus.server.exe-command-line/configure\" target=\"_blank\" class=\"externallink\">https://octopus.com/docs/octopus-rest-api/octopus.server.exe-command-line/configure</a>. )</p>\n<hr>\n<p>As this can cause a lot of head-scratching, it would be nice if the installer could do the checks to verify whether the account selected will work at login stage before proceeding to the next stage.</p>\n<hr>\n<p>Instead, depending on your team setup, you should select your own domain account from the dialog, or (if you have the right AD permissions) create a dedicated account for this. This second option adds complexity as you will need to log in to Windows using that account to make full use of Active Directory integration, or (if using Chrome for example), enter the credentials in using Forms Authentication or via a browser dialog box.</p>\n<p>To get around all this I changed the account to my domain account with a view of adding other administrators later.&nbsp;</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_SelectAdministratorUsingAccount.png\" Alt=\"Screen shot of Octopus Deploy administrator account set up\" /?-->\n<hr>\n<p>Another feature that would be nice here is to be able to make an AD group the administrators rather than one specific user as it is all too easy to forget how this has been set up when that person leaves and their AD account is deactivated.</p>\n<hr>\n<p>Clicking Next takes you to the final page to click Install and get ready to get up and running with Octopus Deploy.</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_LastPage.png\" Alt=\"Screen shot of final page of Octopus Deploy set up\" /?-->\n<p>Except ... things did not go according to plan!</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_GoneWrong.png\" Alt=\"Screen shot of Octopus Deploy error\" /?-->\n<h2 id=\"so-where-did-it-go-wrong\"><a name=\"where-did-it-go-wrong\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#where-did-it-go-wrong\">So Where Did It Go Wrong?</a></h2>\n<p>Looking at the both the Windows Event Log and the Octopus (text) logs , I found a couple of occurrences of this</p>\n<p>Microsoft.AspNetCore.Server.HttpSys.HttpSysException (5): Access is denied.</p>\n<!--?# LinkSizedImage \"InstallOctopusBlog_ErrorLogs.png\" Alt=\"Screen shot of Octopus Deploy error logs\" /?-->\n<p>In short, whilst I had prepared the basics of granting file system permissions to the service account, I had failed to grant permissions to access the HTTP traffic over the ports that Octopus needs to monitor.</p>\n<p>I had made the mistake of assuming that the installer would take care of this, not having really understood the prerequisite notes on the web site.</p>\n<p>This is where I found help on the Octopus website at&nbsp; <a href=\"https://help.octopus.com/t/octopus-server-not-starting-httpsysexception-from-microsoft-aspnetcore-server-httpsys-urlgroup-registerprefix/25025/3\" target=\"_blank\" class=\"externallink\">https://help.octopus.com/t/octopus-server-not-starting-httpsysexception-from-microsoft-aspnetcore-server-httpsys-urlgroup-registerprefix/25025/3</a>.</p>\n<p>In that post, it explains that the NETSH command is required to provide permissions to HTTP access on certain ports.</p>\n<p>NETSH http add urlacl url=https://+:port/ user=domain\\username listen=yes</p>\n<p>An explanation of the syntax can be found at <a href=\"https://docs.microsoft.com/en-us/windows-server/networking/technologies/netsh/netsh-http#add-urlacl\" target=\"_blank\" class=\"externallink\">https://docs.microsoft.com/en-us/windows-server/networking/technologies/netsh/netsh-http#add-urlacl</a>, however that is still a bit dry and didn't really explain what the problem was.</p>\n<p>A bit of searching lead me to this Stack Exchange answer <a href=\"https://superuser.com/questions/1272374/in-what-scenarios-will-i-use-netsh-http-add-urlacl\" target=\"_blank\" class=\"externallink\">https://superuser.com/questions/1272374/in-what-scenarios-will-i-use-netsh-http-add-urlacl</a> that made things a bit clearer and in turn pointed to this Microsoft explanation <a href=\"https://docs.microsoft.com/en-us/windows/win32/http/namespace-reservations-registrations-and-routing?redirectedfrom=MSDN\">https://docs.microsoft.com/en-us/windows/win32/http/namespace-reservations-registrations-and-routing?redirectedfrom=MSDN</a>.</p>\n<h2 id=\"fixing-octopus-deploy-not-starting-due-to-access-denied-error\"><a name=\"fixing-octopus-deploy-not-starting-due-to-access-denied-error\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#fixing-octopus-deploy-not-starting-due-to-access-denied-error\">Fixing Octopus Deploy Not Starting Due to Access Denied Error</a></h2>\n<p>So, here's the fix that is required to address the Access Denied error.</p>\n<p>In short, you have to grant the permissions to the service account to listen for HTTP requests on certain ports.</p>\n<p>To do this, open a Command Prompt on the Windows Server you will be installing / have just installed on using Run As Administrator and enter the following command for each of the ports required, substituting <em>port_number</em> with the actual port number required</p>\n<p>netsh http add urlacl url=http://+:<em>port_number</em>/ user=domain\\username listen=yes</p>\n<p>In my case, as I am using port 93 instead of 80</p>\n<p>netsh http add urlacl url=http://+:<strong>93</strong>/ user=MyDomain\\Octopus listen=yes<br>\nnetsh http add urlacl url=http://+:<strong>10943</strong>/ user=MyDomain\\Octopus listen=yes</p>\n<p>The entry for port 10943 is so that Octopus tentacles can poll the server. See&nbsp;<a href=\"https://octopus.com/network-topologies\" target=\"_blank\" class=\"externallink\">https://octopus.com/network-topologies</a> for details.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>This blog is the result of an afternoon of re-applying Hyper-V snapshots and installing Octopus (it took seven attempts in all) until all the quirks I describe above had been ironed out.</p>\n<p>I must admit, I am surprised that given that the installer knows the ports and the service account, that it can't do this work for you. Same goes for checking the administrator account meets requirements such as having a UPN.</p>\n<p>Perhaps it may be a UAC elevated rights problem as it seems to be a common thing to have to do (try searching for \"NETSH http add urlacl\" and you will find results for various software packages that have this manual step documented), though this is the first time I have had to address this problem manually.</p>\n<p>Don't get me wrong - I love Octopus Deploy, but as this was my first time installing it from scratch, having to go through these hoops to get the latest version installed and running as I need it did test my patience.</p>\n",
    "sanitized": "This post is primarily for my future self to document how to deal with the problems I had installing Octopus Deploy using a domain account as the service account.\nBackground\nI am currently working with a team that has old versions of TeamCity and Octopus Deploy and want to move to the latest versions. The upgrade path from these (very old) versions to the latest versions is complicated and therefore we have set up a new server to host the latest versions.\nThe (virtual) server we will be installing on is hosted in a managed environment (with no self-service facilities). This makes taking snapshots/checkpoints to rollback to if things go wrong  during installation a bit more complicated. It also means that we effectively only get one shot at getting things right. \nFor these reasons, I created a small test lab at home to practice all the steps required so that I could create an installation step-by-step 'run-book' to do the real installation with as few hiccups as possible.\nIn my test lab I have the following set up as Hyper-V virtual machines:\n\nA Windows 2016 server that is the Active Directory Domain Controller and hosts a SQL Server 2017 database server\nA Windows 2016 server that doubles up as the TeamCity and Octopus Deploy server\nA Windows 2012 server that will be a target server for deploying web sites to via an Octopus Deploy Tentacle.\n\nThis three server set-up does not truly represent a real-world environment (for example, the SQL Server would not normally reside on the domain controller), but is enough to emulate the core elements of the environment that I will be using in the real world.\nThe Challenge\nThe previous installation of Octopus Deploy used a local Windows system account, but for the new installation, we want to use a domain account\n\nto connect to SQL Server using Windows Authentication and \nto have tight control over what the service can do vs the local system account which is tantamount to having full local administrator rights.\n\nPreparing the Windows Environment\nThe details of the requirements for the domain service account are provided on the Octopus Deploy web site.\nAt time of writing, the details are a bit vague in places and require further information.\nBased on this, I went through the following actions.\n\nCreate the Windows domain account with password locked down so it can be used as a service account\n\n\nThen on the server that Octopus Deploy will be installed on:\n\nCreate a folder called 'Octopus' on the D: drive for the Octopus Deploy application to reside in and grant Full Control to the Octopus service account created in AD\nCreate a folder called 'OctopusData' on the E: drive that will be used as the Octopus 'home' location and grant Full Control to the Octopus service account created in AD\nGrant the Octopus service account created in AD the Log on as a Service permission. \n\nPreparing the Database\nDetails of the requirements for the SQL Server version and creating a database can be found on the Octopus Deploy website but in summary, if the SQL Server is not on the same server as Octopus, the following needs to be done before installation starts.\n\nEnsure network connectively to the server over TCP/IP\nEnsure no firewall rules preventing access over TCP/IP on appropriate ports between the server hosting SQL Server and the server hosting Octopus Deploy\nCreate an empty database in SQL Server 2017 called Octopus \nSet the collation of the database to case insensitive using\nALTER DATABASE [Octopus] COLLATE Latin1_General_CI_AS\nCreate a SQL Server login for the Octopus service account created in AD\nCreate a user in the Octopus database for the Octopus service account login\nSet the default schema for the user to [dbo] so the account has full control over the database.\n\nNext Steps Before Installation\nAt this point there are some other things that I should have done, but did not find out until I had problems further down the line.\nTo view these next steps, scroll down to here.\nStarting the Installation\nStarting the installation is simple enough, with just one change from the default option, and that is to change the installation folder to D:\\Octopus (where permissions have already been granted).\nOn clicking 'Finish' there is a brief pause while PowerShell is checked and then the 'Getting Started' Wizard is opened.\nUsing the Setup Wizard\nAfter entering the licence key on the first page, the next step is to the change the 'Home' directory to the E: drive where there is plenty of disk space.\n\nClicking next to move to the next page, it is time to select the service account.\nBy default, this is set to the Local System account, but as described above, we want to use a custom domain account, so we change the radio button option and enter the domain credentials (using the Select User to find the service account created in Active Directory).\n\nThe next step is to point to the Octopus database that has been created in SQL Server 2017 and for which the service account has been granted DBO rights so that the database tables and other artefacts can be created. \n\nThe next screen prompts for a port for the web portal. As the server that I am installing on has IIS, TeamCity and a couple of other self hosted web applications, it is not appropriate to use the default port 80. In the example below I am using port 93.\nThis is one of the areas where things started to go wrong later down the track!\n\nMoving on to the next page, this is where the administrator for Octopus Deploy is selected.\nThis is the next place that there are a few gotchas!\n\nSo the screen looks innocent enough. When you first arrive on the screen , it defaults the Authentication Mode drop down to using usernames and passwords in Octopus. However, to make life simpler, we want to use Active Directory. When this is selected, the username will default to Administrator.\nDO NOT JUST ACCEPT THIS! This will be the local Administrator account for the machine you are installing on.\nInstead, use the Select User link to bring up the standard Windows dialog for selecting a domain user. \nAs I was installing in a test lab, I was a bit lazy and just selected the Domain Administrator account. DO NOT DO THIS EITHER!\nThe problem that you will find (after installation is complete), is that, by default, the built in Domain Administrator account does not have a User Principal Name (UPN) and without a UPN, Octopus cannot find the account in Active Directory.\nWhen this happens, you get a \"No fallback name was provided\" error when trying to login.\n\nAt that point, you are pretty much locked out of Octopus and either have to add a UPN to the Domain Administrator account or re-install Octopus from scratch to use another user as the Octopus administrator (though you may be able to change the authentication provider through the command line (I haven't tried this! See https://octopus.com/docs/octopus-rest-api/octopus.server.exe-command-line/configure. )\n\nAs this can cause a lot of head-scratching, it would be nice if the installer could do the checks to verify whether the account selected will work at login stage before proceeding to the next stage.\n\nInstead, depending on your team setup, you should select your own domain account from the dialog, or (if you have the right AD permissions) create a dedicated account for this. This second option adds complexity as you will need to log in to Windows using that account to make full use of Active Directory integration, or (if using Chrome for example), enter the credentials in using Forms Authentication or via a browser dialog box.\nTo get around all this I changed the account to my domain account with a view of adding other administrators later. \n\n\nAnother feature that would be nice here is to be able to make an AD group the administrators rather than one specific user as it is all too easy to forget how this has been set up when that person leaves and their AD account is deactivated.\n\nClicking Next takes you to the final page to click Install and get ready to get up and running with Octopus Deploy.\n\nExcept ... things did not go according to plan!\n\nSo Where Did It Go Wrong?\nLooking at the both the Windows Event Log and the Octopus (text) logs , I found a couple of occurrences of this\nMicrosoft.AspNetCore.Server.HttpSys.HttpSysException (5): Access is denied.\n\nIn short, whilst I had prepared the basics of granting file system permissions to the service account, I had failed to grant permissions to access the HTTP traffic over the ports that Octopus needs to monitor.\nI had made the mistake of assuming that the installer would take care of this, not having really understood the prerequisite notes on the web site.\nThis is where I found help on the Octopus website at  https://help.octopus.com/t/octopus-server-not-starting-httpsysexception-from-microsoft-aspnetcore-server-httpsys-urlgroup-registerprefix/25025/3.\nIn that post, it explains that the NETSH command is required to provide permissions to HTTP access on certain ports.\nNETSH http add urlacl url=https://+:port/ user=domain\\username listen=yes\nAn explanation of the syntax can be found at https://docs.microsoft.com/en-us/windows-server/networking/technologies/netsh/netsh-http#add-urlacl, however that is still a bit dry and didn't really explain what the problem was.\nA bit of searching lead me to this Stack Exchange answer https://superuser.com/questions/1272374/in-what-scenarios-will-i-use-netsh-http-add-urlacl that made things a bit clearer and in turn pointed to this Microsoft explanation https://docs.microsoft.com/en-us/windows/win32/http/namespace-reservations-registrations-and-routing?redirectedfrom=MSDN.\nFixing Octopus Deploy Not Starting Due to Access Denied Error\nSo, here's the fix that is required to address the Access Denied error.\nIn short, you have to grant the permissions to the service account to listen for HTTP requests on certain ports.\nTo do this, open a Command Prompt on the Windows Server you will be installing / have just installed on using Run As Administrator and enter the following command for each of the ports required, substituting port_number with the actual port number required\nnetsh http add urlacl url=http://+:port_number/ user=domain\\username listen=yes\nIn my case, as I am using port 93 instead of 80\nnetsh http add urlacl url=http://+:93/ user=MyDomain\\Octopus listen=yes\nnetsh http add urlacl url=http://+:10943/ user=MyDomain\\Octopus listen=yes\nThe entry for port 10943 is so that Octopus tentacles can poll the server. See https://octopus.com/network-topologies for details.\nConclusion\nThis blog is the result of an afternoon of re-applying Hyper-V snapshots and installing Octopus (it took seven attempts in all) until all the quirks I describe above had been ironed out.\nI must admit, I am surprised that given that the installer knows the ports and the service account, that it can't do this work for you. Same goes for checking the administrator account meets requirements such as having a UPN.\nPerhaps it may be a UAC elevated rights problem as it seems to be a common thing to have to do (try searching for \"NETSH http add urlacl\" and you will find results for various software packages that have this manual step documented), though this is the first time I have had to address this problem manually.\nDon't get me wrong - I love Octopus Deploy, but as this was my first time installing it from scratch, having to go through these hoops to get the latest version installed and running as I need it did test my patience."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/simplifying-di-with-functions",
    "raw": "<p>In my <a href=\"https://stevetalkscode.co.uk/named-dependencies-part-2\">last post</a>, I showed how using a function delegate can be used to create named or keyed dependency injection resolutions.</p>\n<p>When I was writing the demo code, it struck me that the object orientated code I was writing seemed to be bloated for what I was trying to achieve.</p>\n<p>Now don't get me wrong, I am a big believer in the SOLID principles, but when the interface has only a single function, having to create multiple class implementations seems overkill.</p>\n<p>This was the 'aha' moment you sometimes get as a developer where reading and going to user groups or conferences plants seeds in your mind that make you think about problems differently. In other words, if the only tool you have in your tool-belt is a hammer, then every problem looks like a nail; but if you have access to other tools, then you can choose the right tool for the job.</p>\n<p>Over the past few months, I have been looking more at functional programming. This was initially triggered by the excellent talk 'Functional C#' given by Simon Painter (a YouTube video from the Dot Net Sheffield user group is <a href=\"https://www.youtube.com/watch?v=v7WLC5As6g4\" target=\"_blank\" class=\"externallink\">here</a>, along with my own talk - plug, plug, <a href=\"https://www.youtube.com/watch?v=kLl2Mt3eYxU\" target=\"_blank\" class=\"externallink\">here</a>).</p>\n<p>In the talk, Simon advocates using functional programming techniques in C#. At the end of the talk, he recommends the <a href=\"https://www.manning.com/books/functional-programming-in-c-sharp\" target=\"_blank\" class=\"externallink\">Functional Programming in C# book by Enrico Buonanno</a>. It is in Chapter 7 of this book that the seed of this blog was planted.</p>\n<p>In short, if your interface has only a single method that is a pure function, register a function delegate instead of an interface.</p>\n<p>This has several benefits</p>\n<ul>\n<li>There is a less code to write</li>\n<li>The intent of what is being provided is not masked by having an interface getting in the way - it is just the function exposed via a delegate</li>\n<li>There is no object creation of interface implementations, so less memory allocations and&nbsp;may be faster to initially execute (as there is no object creation involved)</li>\n<li>Mocking is easier - you are just providing an implementation of a function signature without the cruft of having to hand craft an interface implementation or use a mocking framework to mock the interface for you.</li>\n</ul>\n<p>So with this in mind, I revisited the <a href=\"https://github.com/configureappio/NamedDiDemo\" target=\"_blank\" class=\"externallink\">demo</a> from the last post and performed the following refactoring:</p>\n<ul>\n<li>Replaced the delegate signature to perform the temperature conversion instead of returning an interface implementation that has a method</li>\n<li>Moved the methods in the class implementations to static functions within the startup class (but could easily be a new static class)</li>\n<li>Change the DI registration to return the results from the appropriate static function instead of using the DI container to find the correct implementation and forcing the caller to execute the implementation's method.</li>\n</ul>\n<p>As can be seen from the two code listings, Listing 1 (Functional) is a lot cleaner than Listing 2 (Object Orientated).</p>\n<p><strong>Listing 1</strong></p>\n<!--?# Gist 32de0b4a10a963c50c931c4494cfa167 /?-->\n<p><strong>Listing 2</strong></p>\n<!--?# Gist eedf384f35acda1798fce0dcc5c5fb32 /?-->\n<p>Having done this, it also got me thinking about how I approach other problems.&nbsp;An example of this is unit-testable timestamps.</p>\n<p>Previously, when I needed to use a timestamp to indicate the current date and time, I would create an interface of ICurrentDateTime that would have a property for the current datetime. The implementation for production use would be a wrapper over the DateTime.Now property, but for unit testing purposes would be a mock with a fixed date and time to fulfill a test criteria.</p>\n<p>Whilst not a pure function, the same approach used above can be applied to this requirement, by creating a delegate to return the current date and time and then registering the delegate to return the system's DateTime.Now.</p>\n<p>This achieves the same goal of decoupling code from the system implementation via an abstraction, but negates the need to create an unnecessary object and interface to simply bridge to the underlying system property.</p>\n<p>If you are interested in looking at getting into functional programming while staying within the comfort zone of C#, I highly recommend Enrico's book.</p>\n<p>The demo of both the OO and Functional approaches can be found in the GitHub project at&nbsp;<a href=\"https://github.com/configureappio/NamedDiDemo\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/NamedDiDemo</a>.</p>\n",
    "sanitized": "In my last post, I showed how using a function delegate can be used to create named or keyed dependency injection resolutions.\nWhen I was writing the demo code, it struck me that the object orientated code I was writing seemed to be bloated for what I was trying to achieve.\nNow don't get me wrong, I am a big believer in the SOLID principles, but when the interface has only a single function, having to create multiple class implementations seems overkill.\nThis was the 'aha' moment you sometimes get as a developer where reading and going to user groups or conferences plants seeds in your mind that make you think about problems differently. In other words, if the only tool you have in your tool-belt is a hammer, then every problem looks like a nail; but if you have access to other tools, then you can choose the right tool for the job.\nOver the past few months, I have been looking more at functional programming. This was initially triggered by the excellent talk 'Functional C#' given by Simon Painter (a YouTube video from the Dot Net Sheffield user group is here, along with my own talk - plug, plug, here).\nIn the talk, Simon advocates using functional programming techniques in C#. At the end of the talk, he recommends the Functional Programming in C# book by Enrico Buonanno. It is in Chapter 7 of this book that the seed of this blog was planted.\nIn short, if your interface has only a single method that is a pure function, register a function delegate instead of an interface.\nThis has several benefits\n\nThere is a less code to write\nThe intent of what is being provided is not masked by having an interface getting in the way - it is just the function exposed via a delegate\nThere is no object creation of interface implementations, so less memory allocations and may be faster to initially execute (as there is no object creation involved)\nMocking is easier - you are just providing an implementation of a function signature without the cruft of having to hand craft an interface implementation or use a mocking framework to mock the interface for you.\n\nSo with this in mind, I revisited the demo from the last post and performed the following refactoring:\n\nReplaced the delegate signature to perform the temperature conversion instead of returning an interface implementation that has a method\nMoved the methods in the class implementations to static functions within the startup class (but could easily be a new static class)\nChange the DI registration to return the results from the appropriate static function instead of using the DI container to find the correct implementation and forcing the caller to execute the implementation's method.\n\nAs can be seen from the two code listings, Listing 1 (Functional) is a lot cleaner than Listing 2 (Object Orientated).\nListing 1\n\nListing 2\n\nHaving done this, it also got me thinking about how I approach other problems. An example of this is unit-testable timestamps.\nPreviously, when I needed to use a timestamp to indicate the current date and time, I would create an interface of ICurrentDateTime that would have a property for the current datetime. The implementation for production use would be a wrapper over the DateTime.Now property, but for unit testing purposes would be a mock with a fixed date and time to fulfill a test criteria.\nWhilst not a pure function, the same approach used above can be applied to this requirement, by creating a delegate to return the current date and time and then registering the delegate to return the system's DateTime.Now.\nThis achieves the same goal of decoupling code from the system implementation via an abstraction, but negates the need to create an unnecessary object and interface to simply bridge to the underlying system property.\nIf you are interested in looking at getting into functional programming while staying within the comfort zone of C#, I highly recommend Enrico's book.\nThe demo of both the OO and Functional approaches can be found in the GitHub project at https://github.com/configureappio/NamedDiDemo."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/named-dependencies-part-2",
    "raw": "<p>If you have come here from a search engine, I suggest reading <a href=\"https://stevetalkscode.co.uk/named-dependencies-part-1\">Part 1</a> first to get some background as to the problem that this post solves.</p>\n<p>In Part 1, I gave the background as to why you may have a requirement to have a way of resolving instances from a DI container using a string name or some other key.</p>\n<p>In this part, I propose a solution that leaves your code ignorant of any DI container and that is easily mockable for unit tests.</p>\n<h2 id=\"making-use-of-strongly-typed-delegates\"><a name=\"making-use-of-strongly-typed-delegates\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#making-use-of-strongly-typed-delegates\">Making Use of Strongly Typed Delegates</a></h2>\n<p>A key thing to understand about DI containers is that the service type that is registered is exactly that - a type, not just interfaces and classes.&nbsp;Therefore, you can register delegates as well.</p>\n<p>The idea of using delegates for resolution is not new. &nbsp;There is an example in the <a href=\"https://autofaccn.readthedocs.io/en/latest/advanced/delegate-factories.html\" target=\"_blank\" class=\"externallink\">Autofac documentation</a>&nbsp;and this&nbsp;<a href=\"https://www.c-sharpcorner.com/article/funct-dynamic-instantiation/\" target=\"_blank\" class=\"externallink\">article on c-sharpcorner</a>.</p>\n<p>However, for .NET Core, when you search for something like '.NET Core named dependency injection', you tend to only find the answer buried in StackOverflow answers.</p>\n<p>That is why I have written this post, so that I can find the answer later myself, and also hopefully help other find it too.</p>\n<h2 id=\"func-delegates-vs.strongly-typed-delegates\"><a name=\"func-delegates-vs-strongly-typed-delegates\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#func-delegates-vs-strongly-typed-delegates\">Func<t> Delegates Vs. Strongly Typed Delegates</t></a></h2>\n<p>Before getting into a detail of using delegates as service types, we need to talk about generic <a href=\"https://docs.microsoft.com/en-us/dotnet/api/system.func-1?view=netcore-2.1\" target=\"_blank\" class=\"externallink\">Func delegates</a> vs <a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/delegates/\" target=\"_blank\" class=\"externallink\">strongly typed delegates</a>.</p>\n<p>The generic Func type was introducted in C# 3.0 and is a generic delegate included in the&nbsp;<code>System</code>&nbsp;namespace that has multiple overloads that accept between zero and sixteen input parameters (denoted T1 to T16) and a single output parameter (TResult). These are convenience types so that as a developer you do not need to repeatedly create your own customer delegates.</p>\n<p>For the purposes of retrieving an instance of a type from the DI container, the various Func delegates can be used as the type for service registration and as a parameter to the constructors of classes. E.g. Func&lt;string, IServiceType&gt;.</p>\n<p>However, I have two reasons for not using this approach.</p>\n<ol>\n<li>If you change the signature of the service registration to a different overload of Func, you have no tools in the IDE to reflect that change throughout all the references (as the dependency resolution does not take place until runtime). Instead you will need to perform a text 'search and replace' task across your whole code base (and hope that there are no other uses of the generic signature for other purposes). By using a strongly typed delegate, this is a unique type and therefore, when changing the signature of the delegate, the compiler will indicate when references and instances are broken (or a tool like Reshaper will do the work for you)</li>\n<li>Though this is probably a very rare occurrence, you may have a need to have two different factories with the same signature. If the generic Func&lt;T, IServiceType&gt; is used, it will not be possible to differentiate the two factories. With strongly typed delegates, it is the delegate type that is registered and referenced in the constructor, so the same signature can be used with two different delegate types and the DI container will resolve tHem correctly.</li>\n</ol>\n<h2 id=\"how-does-using-typed-delegates-help\"><a name=\"how-does-using-typed-delegates-help\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#how-does-using-typed-delegates-help\">How Does Using Typed Delegates Help?</a></h2>\n<p>By using&nbsp;typed delegates, the code that requires the named/keyed dependency is ignorant of the DI container technology that is resolving the instance for it.</p>\n<p>This differs from other approaches where either the consuming class itself or an intercepting factory class takes the IServiceProvider as a dependency in its constructor and has logic to resolve the instance itself which defeats the purpose of dependency injection as it is creating a glue between the classes.</p>\n<p>By registering the delegate within the DI container registration code, it keeps the codebase (outside of Di registration) ignorant of how the name/key resolution is working.</p>\n<h2 id=\"the-demo\"><a name=\"the-demo\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-demo\">The Demo</a></h2>\n<p>For the rest of this post, I am going to use a test project (which can be found on GitHub at&nbsp;<a href=\"https://github.com/configureappio/NamedDiDemo\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/NamedDiDemo</a>) that converts temperatures to the Kelvin scale from four different scales - Centigrade, Fahrenheit, Rankine and Kelvins.</p>\n<p>To do this, there are four formulas that are needed</p>\n<table>\n<thead>\n<tr>\n<th>Calculation</th>\n<th>Formula</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Celcius(°C) to Kelvin(K)</td>\n<td><em>T</em>(K)&nbsp;=&nbsp;<em>T</em>(°C)&nbsp;+ 273.15</td>\n</tr>\n<tr>\n<td>Fahrenheit(°F) to Kelvin(K)</td>\n<td><em>T</em>(K)&nbsp;= &nbsp;(<em>T</em>(°F)&nbsp;+ 459.67) x 5 / 9</td>\n</tr>\n<tr>\n<td>Rankine to Kelvin(K)</td>\n<td><em>T</em>(K)&nbsp;=&nbsp;<em>T</em>(°R)&nbsp;× 5/9</td>\n</tr>\n<tr>\n<td>Kelvin(K) to Kelvin(K)</td>\n<td><em>T</em>(K)&nbsp;= <em>T</em>(K)</td>\n</tr>\n</tbody>\n</table>\n<p>Each formula is implemented as a mapper class with an implementation of the IKelvinMapper interface as defined below.</p>\n<!--?# Gist bb5ad50704820242935c49c468f752d0 /?-->\n<p>Each of the implementations is registered with the DI container</p>\n<!--?# Gist 669a52076b51a818354f09956c420f85 /?-->\n<p>For the caller to retrieve the correct conversion, we need a delegate that uses the input temperature scale in order to retrieve the correct mapper and return the mapper as the IKelvinMapper interface</p>\n<!--?# Gist ff51aefc98c90e8b2ac5c6362e16e0d5 /?-->\n<p>Lastly, a lookup function is registered with the DI container against the delegate type as the service type to perform the lookup of the correct mapper. To keep things simple, the lookup is based on the first (capitalised) letter of the input temperature scale (normally as more robust lookup would be used, but I've tried to keep it simple so as not to distract from the core purpose of what I am demonstrating).</p>\n<!--?# Gist 7cb3c72a026689c79676a7d497042039 /?-->\n<p>With the returned interface implementation, a conversation from the input temperature scale and value can be made to a consistent value in the Kelvin scale.</p>\n<h2 id=\"so-how-does-it-work\"><a name=\"so-how-does-it-work\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#so-how-does-it-work\">So How Does It Work?</a></h2>\n<p>The demo works on a very simple basis of leveraging the DI container to do all the hard work of determining which mapping converter is required. The caller just needs to know that it uses the delegate to take in the input temperature scale and value and return the value in Kelvins.</p>\n<h3 id=\"object-instantiation\"><a name=\"object-instantiation\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#object-instantiation\">Object Instantiation</a></h3>\n<p>Depending on the complexity of the type to be instantiated, there are three ways to instantiate the object:</p>\n<ol>\n<li>Perform the instantiation with a 'new' keyword. This is fine if the constructor of the type has no dependencies and a new instance is required on each occasion. However, I would discourage this approach as it is not an efficient way of managing the lifetimes of the objects and could lead to problems with garbage collection due to long lived objects</li>\n<li>Rely on the DI container to instantiate the object. This is the approach taken in the demo. This has two benefits. The first is that if the object can be a singleton, then the DI container takes care of that for you provided the type has been registered as a singleton lifetime when registered. The second is that if the type has dependencies in the constructor, the DI container will resolve these for you</li>\n<li>Lastly, if one or more parameters to the type constructor need the be generated from the delegate input (and therefore not directly resolvable from prior DI registrations), but other parameters can be resolved, then the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.activatorutilities.createinstance?view=dotnet-plat-ext-3.1\" target=\"_blank\" class=\"externallink\">ActivatorUtilities CreateInstance method</a> can be used to create an instance using a hybrid of provided parameters and using the DI container to resolve those parameters not provided.</li>\n</ol>\n<p>This is where you take advantage of the ability to register a service resolution using a lambda expression as it provides access to the IServiceProvider container instance to retrieve instances from the service provider for (2) and (3) above.</p>\n<h3 id=\"deciding-which-type-to-instantiate\"><a name=\"deciding-which-type-to-instantiate\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#deciding-which-type-to-instantiate\">Deciding Which Type to Instantiate</a></h3>\n<p>For simple keys, you can use a case statement inside a lambda expression to derive the type required and let the DI container perform the object instantiation for you. (This is the approach I have used in the demo project).</p>\n<p>However, if you have many different items or complex keys, you may want to consider other ways of deriving the service type required.</p>\n<p>For example, by registering a singleton Dictionary&lt;string, serviceType&gt; to map keys to types, if there are many keys, you may gain a performance boost from the hash lookup algorithm.</p>\n<p>Instead of using the out-of-the-box generic dictionary thought, I recommend creating a dedicated class that implements IDictionary&lt;string, Type&gt; for the same reasons that I recommend using custom delegates over the generic Func<t> - it removes any ambiguity if there is (deliberately or inadvertently) more than one registration of the generic dictionary.</t></p>\n<h3 id=\"care-with-scoped-lifetimes\"><a name=\"care-with-scoped-lifetimes\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#care-with-scoped-lifetimes\">Care with Scoped Lifetimes</a></h3>\n<p>Careful thought needs to be given to the lifetime of both the factory delegate and the object being retrieved from the DI container.</p>\n<p>Typically, as&nbsp;factory classes have no state,&nbsp; they can usually be registered with a singleton lifetime. The same applies to the delegate being used as the factory or to access the factory class.</p>\n<p>Where things get complicated is when the service type that the factory is instantiating for you has been registered in the DI container as having a scoped lifetime.</p>\n<p>In this case, the resolution within the factory fails as the DI container cannot resolve scoped dependencies for constructor injection inside a singleton or transient lifetime object (in our case, the singleton factory delegate).</p>\n<p>There are two ways to get around this problem.</p>\n<p>(i) Register the factory delegate as scoped as well</p>\n<p>(ii) Change the singleton factory delegate to take an IServiceProvider as part of the function signature. It is then the caller's responsibility to pass the correctly scoped IServiceProvider to the delegate. However, this effectively takes us back to a service locator pattern again.</p>\n<h3 id=\"accessing-the-delegate-for-calling\"><a name=\"accessing-the-delegate-for-calling\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#accessing-the-delegate-for-calling\">Accessing the Delegate for Calling</a></h3>\n<p>Now that we have abstracted the mapper instance generation into a delegate function, the client just needs to be able to resolve the instance.</p>\n<p>For classes that want to use the mapper (and are also registered with the DI container), this is simply a case of making the delegate a constructor parameter.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>Hopefully, these two posts have provided some insight into how the limitations of the Microsoft DI container to support named or keyed resolutions can be worked around without having to resort to using a third-party container or custom service locator pattern implementation.</p>\n<p>Whilst not part of this two-part blog post, I have written another <a href=\"https://stevetalkscode.co.uk/simplifying-di-with-functions\">post</a> about simplifying the demo to work in a more functional way by replacing the interface implementations with direct functions to (a) reduce the amount of code and (b) reduce the number of memory allocations required to create the objects.</p>\n",
    "sanitized": "If you have come here from a search engine, I suggest reading Part 1 first to get some background as to the problem that this post solves.\nIn Part 1, I gave the background as to why you may have a requirement to have a way of resolving instances from a DI container using a string name or some other key.\nIn this part, I propose a solution that leaves your code ignorant of any DI container and that is easily mockable for unit tests.\nMaking Use of Strongly Typed Delegates\nA key thing to understand about DI containers is that the service type that is registered is exactly that - a type, not just interfaces and classes. Therefore, you can register delegates as well.\nThe idea of using delegates for resolution is not new.  There is an example in the Autofac documentation and this article on c-sharpcorner.\nHowever, for .NET Core, when you search for something like '.NET Core named dependency injection', you tend to only find the answer buried in StackOverflow answers.\nThat is why I have written this post, so that I can find the answer later myself, and also hopefully help other find it too.\nFunc Delegates Vs. Strongly Typed Delegates\nBefore getting into a detail of using delegates as service types, we need to talk about generic Func delegates vs strongly typed delegates.\nThe generic Func type was introducted in C# 3.0 and is a generic delegate included in the System namespace that has multiple overloads that accept between zero and sixteen input parameters (denoted T1 to T16) and a single output parameter (TResult). These are convenience types so that as a developer you do not need to repeatedly create your own customer delegates.\nFor the purposes of retrieving an instance of a type from the DI container, the various Func delegates can be used as the type for service registration and as a parameter to the constructors of classes. E.g. Func<string, IServiceType>.\nHowever, I have two reasons for not using this approach.\n\nIf you change the signature of the service registration to a different overload of Func, you have no tools in the IDE to reflect that change throughout all the references (as the dependency resolution does not take place until runtime). Instead you will need to perform a text 'search and replace' task across your whole code base (and hope that there are no other uses of the generic signature for other purposes). By using a strongly typed delegate, this is a unique type and therefore, when changing the signature of the delegate, the compiler will indicate when references and instances are broken (or a tool like Reshaper will do the work for you)\nThough this is probably a very rare occurrence, you may have a need to have two different factories with the same signature. If the generic Func<T, IServiceType> is used, it will not be possible to differentiate the two factories. With strongly typed delegates, it is the delegate type that is registered and referenced in the constructor, so the same signature can be used with two different delegate types and the DI container will resolve tHem correctly.\n\nHow Does Using Typed Delegates Help?\nBy using typed delegates, the code that requires the named/keyed dependency is ignorant of the DI container technology that is resolving the instance for it.\nThis differs from other approaches where either the consuming class itself or an intercepting factory class takes the IServiceProvider as a dependency in its constructor and has logic to resolve the instance itself which defeats the purpose of dependency injection as it is creating a glue between the classes.\nBy registering the delegate within the DI container registration code, it keeps the codebase (outside of Di registration) ignorant of how the name/key resolution is working.\nThe Demo\nFor the rest of this post, I am going to use a test project (which can be found on GitHub at https://github.com/configureappio/NamedDiDemo) that converts temperatures to the Kelvin scale from four different scales - Centigrade, Fahrenheit, Rankine and Kelvins.\nTo do this, there are four formulas that are needed\n\n\n\nCalculation\nFormula\n\n\n\n\nCelcius(°C) to Kelvin(K)\nT(K) = T(°C) + 273.15\n\n\nFahrenheit(°F) to Kelvin(K)\nT(K) =  (T(°F) + 459.67) x 5 / 9\n\n\nRankine to Kelvin(K)\nT(K) = T(°R) × 5/9\n\n\nKelvin(K) to Kelvin(K)\nT(K) = T(K)\n\n\n\nEach formula is implemented as a mapper class with an implementation of the IKelvinMapper interface as defined below.\n\nEach of the implementations is registered with the DI container\n\nFor the caller to retrieve the correct conversion, we need a delegate that uses the input temperature scale in order to retrieve the correct mapper and return the mapper as the IKelvinMapper interface\n\nLastly, a lookup function is registered with the DI container against the delegate type as the service type to perform the lookup of the correct mapper. To keep things simple, the lookup is based on the first (capitalised) letter of the input temperature scale (normally as more robust lookup would be used, but I've tried to keep it simple so as not to distract from the core purpose of what I am demonstrating).\n\nWith the returned interface implementation, a conversation from the input temperature scale and value can be made to a consistent value in the Kelvin scale.\nSo How Does It Work?\nThe demo works on a very simple basis of leveraging the DI container to do all the hard work of determining which mapping converter is required. The caller just needs to know that it uses the delegate to take in the input temperature scale and value and return the value in Kelvins.\nObject Instantiation\nDepending on the complexity of the type to be instantiated, there are three ways to instantiate the object:\n\nPerform the instantiation with a 'new' keyword. This is fine if the constructor of the type has no dependencies and a new instance is required on each occasion. However, I would discourage this approach as it is not an efficient way of managing the lifetimes of the objects and could lead to problems with garbage collection due to long lived objects\nRely on the DI container to instantiate the object. This is the approach taken in the demo. This has two benefits. The first is that if the object can be a singleton, then the DI container takes care of that for you provided the type has been registered as a singleton lifetime when registered. The second is that if the type has dependencies in the constructor, the DI container will resolve these for you\nLastly, if one or more parameters to the type constructor need the be generated from the delegate input (and therefore not directly resolvable from prior DI registrations), but other parameters can be resolved, then the ActivatorUtilities CreateInstance method can be used to create an instance using a hybrid of provided parameters and using the DI container to resolve those parameters not provided.\n\nThis is where you take advantage of the ability to register a service resolution using a lambda expression as it provides access to the IServiceProvider container instance to retrieve instances from the service provider for (2) and (3) above.\nDeciding Which Type to Instantiate\nFor simple keys, you can use a case statement inside a lambda expression to derive the type required and let the DI container perform the object instantiation for you. (This is the approach I have used in the demo project).\nHowever, if you have many different items or complex keys, you may want to consider other ways of deriving the service type required.\nFor example, by registering a singleton Dictionary<string, serviceType> to map keys to types, if there are many keys, you may gain a performance boost from the hash lookup algorithm.\nInstead of using the out-of-the-box generic dictionary thought, I recommend creating a dedicated class that implements IDictionary<string, Type> for the same reasons that I recommend using custom delegates over the generic Func - it removes any ambiguity if there is (deliberately or inadvertently) more than one registration of the generic dictionary.\nCare with Scoped Lifetimes\nCareful thought needs to be given to the lifetime of both the factory delegate and the object being retrieved from the DI container.\nTypically, as factory classes have no state,  they can usually be registered with a singleton lifetime. The same applies to the delegate being used as the factory or to access the factory class.\nWhere things get complicated is when the service type that the factory is instantiating for you has been registered in the DI container as having a scoped lifetime.\nIn this case, the resolution within the factory fails as the DI container cannot resolve scoped dependencies for constructor injection inside a singleton or transient lifetime object (in our case, the singleton factory delegate).\nThere are two ways to get around this problem.\n(i) Register the factory delegate as scoped as well\n(ii) Change the singleton factory delegate to take an IServiceProvider as part of the function signature. It is then the caller's responsibility to pass the correctly scoped IServiceProvider to the delegate. However, this effectively takes us back to a service locator pattern again.\nAccessing the Delegate for Calling\nNow that we have abstracted the mapper instance generation into a delegate function, the client just needs to be able to resolve the instance.\nFor classes that want to use the mapper (and are also registered with the DI container), this is simply a case of making the delegate a constructor parameter.\nConclusion\nHopefully, these two posts have provided some insight into how the limitations of the Microsoft DI container to support named or keyed resolutions can be worked around without having to resort to using a third-party container or custom service locator pattern implementation.\nWhilst not part of this two-part blog post, I have written another post about simplifying the demo to work in a more functional way by replacing the interface implementations with direct functions to (a) reduce the amount of code and (b) reduce the number of memory allocations required to create the objects."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/named-dependencies-part-1",
    "raw": "<p><em>If you want to go straight to the solution of how to resolve instances by a name or key without reading the background, go to <a href=\"https://stevetalkscode.co.uk/named-dependencies-part-2\">Part 2 of this post.</a></em></p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>Many dependency injection container solutions support the concept of labeling a registered dependency with a name or a key that can be used by the container to resolve the dependency required when presented by a dependent class.</p>\n<p>For instance, <a href=\"https://autofaccn.readthedocs.io/en/latest/advanced/keyed-services.html\" target=\"_blank\" class=\"externallink\">Autofac supports both named and keyed labeling</a>&nbsp;using a syntax like this to register the dependency:</p>\n<p>builder.RegisterType<onlinestate>().Named<idevicestate>(\"online\");</idevicestate></onlinestate></p>\n<p>It provides both an explicit method to resolve the dependency, and an attributed way on the constructor parameters of a class.</p>\n<p>var r = container.ResolveNamed<idevicestate>(\"online\");public class DeviceInfo { public DeviceInfo([KeyFilter(\"online\")] IDeviceState deviceState)  }</idevicestate></p>\n<p>However, the container that is provided out of the box with .NET Core does not support named or keyed registrations.</p>\n<h2 id=\"mix-and-match-containers\"><a name=\"mix-match-containers\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#mix-match-containers\">Mix And Match Containers</a></h2>\n<p>The Microsoft DI Container is a '<a href=\"https://blog.ploeh.dk/2014/05/19/conforming-container/\" target=\"_blank\" class=\"externallink\">Conforming Container</a>' that provides a common abstraction that can be wired up to other containers such as Autofac, Ninject, StructureMap et al.</p>\n<p>As a result, it only supports the lowest common denominator across multiple DI solutions, and more accurately, only what Microsoft deemed as necessary for its purposes.</p>\n<p>This means that if you want to use features such as named/keyed resolution, you end up having to mix and match both the Microsoft DI Container and your other container of choice.</p>\n<p>Now, most of the big name containers support this and provide methods that register the custom container so it can be retrieved in constructor injection and the custom features accessed using that containers methods.</p>\n<p>However, the downside to this is that</p>\n<ol>\n<li>you are losing the common abstraction as you are including code that is specific to a container technology in your code base</li>\n<li>if you inject the specific container into a constructor, you are inadvertently being led towards the <a href=\"https://blog.ploeh.dk/2010/02/03/ServiceLocatorisanAnti-Pattern/\" target=\"_blank\" class=\"externallink\">Concrete Service Locator Anti-Pattern</a>&nbsp;where the specific container &nbsp;is effectively acting as the service locator for you</li>\n<li>you are binding your solution to a particular container technology which needs changing across the codebase if you decide to use a different container.</li>\n</ol>\n<h2 id=\"i-could-write-an-adapter\"><a name=\"i-could-write-an-adapter\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#i-could-write-an-adapter\">\"I could write an adapter ...\"</a></h2>\n<p>The first and last of these issues can be mitigated by creating an interface for the functionality you require to abstract your code away from a particular container technology and then writing an adapter.</p>\n<p>However, you are still have a service locator being injected into constructors in order to get to the abstracted functionality (in this case, obtaining an instance based on a name or key).</p>\n<h2 id=\"why-would-i-need-to-get-an-instance-by-name-or-key-surely-it-is-an-anti-pattern\"><a name=\"get-by-name-or-key-anti-pattern-question\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#get-by-name-or-key-anti-pattern-question\">Why Would I Need to Get An Instance By Name or Key? - Surely it is an Anti-Pattern?</a></h2>\n<p>To an extent, I agree that using a name string or key to resolve an instance is effectively a service locator as it is forcing a lookup within the container to retrieve an instance of the required type.</p>\n<p>Under normal circumstances, I would be looking at ways to avoid using this way of resolving an instance such as creating alias interfaces.</p>\n<p>For example, say I have three different classes FooA, FooB &amp; FooC that all implement an interface IFoo.</p>\n<p>I have a class that has needs all three implementations to perform some functionality.&nbsp;I have two choices in how I can inject the classes.</p>\n<p>I can (a) either explicitly define the three class types as parameters in the constructor of my class, or (b) I can register all three classes as IFoo and define a single parameter of IEnumerable<ifoo>.</ifoo></p>\n<p>The downside to (a) is that I am binding to the concrete implementations which may limit my unit testing if I am unable to access or mock the concrete instances in my unit test project. It also means that if I want to replace one of the classes with a different implementation I would need to change the constructor signature of my class.</p>\n<p>The downside to (b) is that the class receives the implementation instances in the order that the container decides to present them (usually the order of registration) and the class code needs to know how to tell the implementations apart. This effectively is moving the service locator inside my class.</p>\n<p>To get around this scenario, I could create three interfaces IFooA, IFooB and IFooC that each directly inherits from IFoo. The three class implementations would then be implementations of the appropriate interface and can be registered with the DI container as such.</p>\n<p>The receiving class can then explicitly define IFooA, IFooB and IFooC as parameters to the constructor. This is effectively the same as we had before, but makes the class more testable as the interfaces can be easily mocked in unit testing instead of being bound to the concrete classes.</p>\n<p>Alternatively, we can still leave the classes registered as implementations of IFoo and have a constructor that takes IEnumerable as its parameter. However, &nbsp;this will mean that our class has to iterate over the enumeration to identify which classes are implementations of IFooA, IFooB &amp; IFooC and then cast as appropriate. It then also has to know how to handle multiple registrations of the three derived interfaces. E.g. what happens if there are three IFooA implementations? Which should be used? The first? The last? Chain all of them?</p>\n<p>With all this said, I recently had a situation where I needed to resolve instances from the container using a string as the key, which is what got me thinking about this in the first place.</p>\n<h2 id=\"data-driven-instance-resolution\"><a name=\"data-driven-instance-resolution\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#data-driven-instance-resolution\">Data Driven Instance Resolution</a></h2>\n<p>Say you have a data source that has child records in multiple different formats. The association is defined by an identifier and a string that indicates the type of data.</p>\n<p>In order to correctly retrieve the data and then transform it into a common format suitable for export to another system, you are reliant upon that string to identify the correct mapper to use. In this situation, we need some way of being able to identify a mapper class instance from the identifying string in order to perform the correct mapping work required.</p>\n<p>You could create a class mapping factory that maintains a dictionary of the acceptable strings that will be in the data and the type required for that string, then has a method that provides an instance of the type for a particular string. E.g.</p>\n<p>public interface IFooFactory { IFoo GetFooByKey(string key); }</p>\n<p>The problem here is that your implementation of IFooFactory becomes a service locator as it will need the IServiceProvider passed to its constructor in order to be able to create the instance required.</p>\n<p>Ideally, you want your code to be ignorant of IServiceProvider so that you do not have to mock it in your unit tests.</p>\n<p>This is the point where you want your DI container to be able to be able to inject by name or key.</p>\n<p>This is where I come to the whole point of this post.</p>\n<h2 id=\"resolving-an-instance-by-name-from-the-microsoft-di-container-using-a-strongly-typed-delegate\"><a name=\"resolving-instance-using-strongly-typed-delegate\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#resolving-instance-using-strongly-typed-delegate\">Resolving an Instance by Name from the Microsoft DI Container using a Strongly Typed Delegate.</a></h2>\n<p>We want our classes to be completely ignorant of the DI container being used and just be able to dynamically obtain the instance.</p>\n<p>This is where our old friend, the delegate keyword can help us, which I describe in more detail in <a href=\"https://stevetalkscode.co.uk/named-dependencies-part-2\">Part 2</a>.</p>\n",
    "sanitized": "If you want to go straight to the solution of how to resolve instances by a name or key without reading the background, go to Part 2 of this post.\nBackground\nMany dependency injection container solutions support the concept of labeling a registered dependency with a name or a key that can be used by the container to resolve the dependency required when presented by a dependent class.\nFor instance, Autofac supports both named and keyed labeling using a syntax like this to register the dependency:\nbuilder.RegisterType().Named(\"online\");\nIt provides both an explicit method to resolve the dependency, and an attributed way on the constructor parameters of a class.\nvar r = container.ResolveNamed(\"online\");public class DeviceInfo { public DeviceInfo([KeyFilter(\"online\")] IDeviceState deviceState)  }\nHowever, the container that is provided out of the box with .NET Core does not support named or keyed registrations.\nMix And Match Containers\nThe Microsoft DI Container is a 'Conforming Container' that provides a common abstraction that can be wired up to other containers such as Autofac, Ninject, StructureMap et al.\nAs a result, it only supports the lowest common denominator across multiple DI solutions, and more accurately, only what Microsoft deemed as necessary for its purposes.\nThis means that if you want to use features such as named/keyed resolution, you end up having to mix and match both the Microsoft DI Container and your other container of choice.\nNow, most of the big name containers support this and provide methods that register the custom container so it can be retrieved in constructor injection and the custom features accessed using that containers methods.\nHowever, the downside to this is that\n\nyou are losing the common abstraction as you are including code that is specific to a container technology in your code base\nif you inject the specific container into a constructor, you are inadvertently being led towards the Concrete Service Locator Anti-Pattern where the specific container  is effectively acting as the service locator for you\nyou are binding your solution to a particular container technology which needs changing across the codebase if you decide to use a different container.\n\n\"I could write an adapter ...\"\nThe first and last of these issues can be mitigated by creating an interface for the functionality you require to abstract your code away from a particular container technology and then writing an adapter.\nHowever, you are still have a service locator being injected into constructors in order to get to the abstracted functionality (in this case, obtaining an instance based on a name or key).\nWhy Would I Need to Get An Instance By Name or Key? - Surely it is an Anti-Pattern?\nTo an extent, I agree that using a name string or key to resolve an instance is effectively a service locator as it is forcing a lookup within the container to retrieve an instance of the required type.\nUnder normal circumstances, I would be looking at ways to avoid using this way of resolving an instance such as creating alias interfaces.\nFor example, say I have three different classes FooA, FooB & FooC that all implement an interface IFoo.\nI have a class that has needs all three implementations to perform some functionality. I have two choices in how I can inject the classes.\nI can (a) either explicitly define the three class types as parameters in the constructor of my class, or (b) I can register all three classes as IFoo and define a single parameter of IEnumerable.\nThe downside to (a) is that I am binding to the concrete implementations which may limit my unit testing if I am unable to access or mock the concrete instances in my unit test project. It also means that if I want to replace one of the classes with a different implementation I would need to change the constructor signature of my class.\nThe downside to (b) is that the class receives the implementation instances in the order that the container decides to present them (usually the order of registration) and the class code needs to know how to tell the implementations apart. This effectively is moving the service locator inside my class.\nTo get around this scenario, I could create three interfaces IFooA, IFooB and IFooC that each directly inherits from IFoo. The three class implementations would then be implementations of the appropriate interface and can be registered with the DI container as such.\nThe receiving class can then explicitly define IFooA, IFooB and IFooC as parameters to the constructor. This is effectively the same as we had before, but makes the class more testable as the interfaces can be easily mocked in unit testing instead of being bound to the concrete classes.\nAlternatively, we can still leave the classes registered as implementations of IFoo and have a constructor that takes IEnumerable as its parameter. However,  this will mean that our class has to iterate over the enumeration to identify which classes are implementations of IFooA, IFooB & IFooC and then cast as appropriate. It then also has to know how to handle multiple registrations of the three derived interfaces. E.g. what happens if there are three IFooA implementations? Which should be used? The first? The last? Chain all of them?\nWith all this said, I recently had a situation where I needed to resolve instances from the container using a string as the key, which is what got me thinking about this in the first place.\nData Driven Instance Resolution\nSay you have a data source that has child records in multiple different formats. The association is defined by an identifier and a string that indicates the type of data.\nIn order to correctly retrieve the data and then transform it into a common format suitable for export to another system, you are reliant upon that string to identify the correct mapper to use. In this situation, we need some way of being able to identify a mapper class instance from the identifying string in order to perform the correct mapping work required.\nYou could create a class mapping factory that maintains a dictionary of the acceptable strings that will be in the data and the type required for that string, then has a method that provides an instance of the type for a particular string. E.g.\npublic interface IFooFactory { IFoo GetFooByKey(string key); }\nThe problem here is that your implementation of IFooFactory becomes a service locator as it will need the IServiceProvider passed to its constructor in order to be able to create the instance required.\nIdeally, you want your code to be ignorant of IServiceProvider so that you do not have to mock it in your unit tests.\nThis is the point where you want your DI container to be able to be able to inject by name or key.\nThis is where I come to the whole point of this post.\nResolving an Instance by Name from the Microsoft DI Container using a Strongly Typed Delegate.\nWe want our classes to be completely ignorant of the DI container being used and just be able to dynamically obtain the instance.\nThis is where our old friend, the delegate keyword can help us, which I describe in more detail in Part 2."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/separating-aspnetcore-startup",
    "raw": "<p>I was recently listening to <a href=\"https://dotnetrocks.com/?show=1538\" target=\"_blank\" class=\"externallink\">an episode of the brilliant .Net Rocks</a> where Carl and Richard were talking to <a href=\"https://ardalis.com/blog\">Steve Smith (a.k.a @ardalis)</a> in which he talks about clean architecture in ASP.Net Core.</p>\n<p>One of the things discussed was the separation of concerns, where Steve discusses creating an architecture in which you try to break up your application in such a way that hides implementation detail in one project from another consuming project. Instead, the consuming project is only aware of interfaces or abstract classes from shared libraries from which instances are are created by the dependency injection framework in use.</p>\n<p>The aim is to try and guide a developer of the consuming project away from ‘new-ing’ up instances of a class from outside the project. To use Steve's catch phrase, \"new is glue\".</p>\n<p>I was listening to the podcast on my commute to work and it got me thinking about the project I had just started working on. So much so, that I had to put the podcast on pause to give myself some thinking time for the second half of the commute.</p>\n<p>What was causing the sparks to go off in my head was about how dependencies are registered in the Startup class in ASP.Net Core.</p>\n<p>By default, when you create a new ASP.Net Core project, the Startup class is created as part of that project, and it is here that you register your dependencies. If your dependencies are in another project/assembly/Nuget package, it means that the references to wherever the dependency is has to be added to the consuming project.</p>\n<p>Of course, if you do this, that means that the developer of the consuming project is free to ‘new up’ an instance of a dependency rather than rely on the DI container. The gist of Steve Smith’s comment in the podcast was do what you can to help try to prevent this.</p>\n<p>When I got to work, I had a look at the code and pondered about whether the Startup class could be moved out to another project. That way the main ASP.Net project would only have a reference to the new project (we’ll call it the infrastructure project for simplicity) and not the myriad of other projects/Nugets. Simple huh? Yeah right!</p>\n<p>So the first problem I hit was all the ASP/MVC plumbing that would be needed in the new project. When I copied the Startup class to the new project, Visual Studio started moaning about all the missing references.</p>\n<p>Now when you create a new MVC/Web.API project with .Net Core, the VS template uses the Microsoft.AspNetCore.All meta NuGet package. For those not familiar with meta packages, these are NuGet packages that bundle up a number of other NuGet packages – and Microsoft.AspNetCore.All is massive. When I opened the nuspec file from the cache on my machine, there were 136 dependencies on other packages. For my infrastructure project, I was not going to need all of these. I was only interested in the ones required to support the interfaces, classes and extension methods I would need in the Startup class.</p>\n<p>Oh boy, that was a big mistake. It was a case of adding all the dependencies I would actually need one by one to ensure I was not bringing any unnecessary packages along for the ride. Painful, but I did it.</p>\n<p>So I made all the updates to the main MVC project required to use the Startup class from my new project and remove the references I previously had to other projects (domain, repository etc) as this was the point of the exercise.</p>\n<p>It all compiled! Great. Pressed F5 to run and … hang on what?</p>\n<div style=\"margin:0 25% 0 25%\">\n<!--?# LinkSizedImage \"Edge404.jpg\" Alt=\"404 when MVC controller not found\" /?-->\n</div>\n<p>After a bit of head scratching, I realised the problem was that MVC could not find the controller? WHY?</p>\n<p>At this point, I parked my so-called ‘best practice’ changes as I did not want to waste valuable project time on a wild goose chase.</p>\n<p>This was really bugging me, so outside of work, I started to do some more digging.</p>\n<p>After reading some blogs and looking at the source code in GitHub, the penny dropped. ASP.Net MVC makes the assumption that the controllers are in the same assembly as the Startup class.</p>\n<p>When the Startup class is registered with the host builder, it sets the ApplicationName property in&nbsp;HostingEnvironment instance to the name of the assembly where the Startup class is.</p>\n<!--?# Gist 3aab13c6cacceac340b5943baad8f445 /?-->\n<p>The ApplicationName property of the IHostingEnvironment instance is used by the AddMvc extension to register the assembly where controllers can be found.</p>\n<!--?# Gist 747c751b461019405ea038060e865e88 /?-->\n<p>Eventually, I found the <a href=\"https://github.com/aspnet/Hosting/issues/903#issuecomment-269103645\" target=\"_blank\" class=\"externallink\">workaround</a> from David Fowler in an answer to an issue on GitHub. In short, you need to use the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.hosting.iwebhostbuilder.usesetting?view=aspnetcore-2.0\">UseSetting</a> extension method on the IWebHostBuilder instance to change the assembly used in the ApplicationName property to point to where the controllers are.&nbsp;In my case this was as follows:</p>\n<p>UseSetting(WebHostDefaults.ApplicationKey, typeof(Program).GetTypeInfo().Assembly.FullName)</p>\n<p>Therefore, without this line redirecting the application name to the correct assembly, if the controllers are not in the same assembly as the Startup class, that's when things go wrong - as I found.</p>\n<p>With this problem fixed, everything fell into place and started working correctly.</p>\n<p>However, with this up and running, something did not feel right about it.</p>\n<p>The solution I had created was fine if all the dependencies are accessible from the new Infra project, either directly within the project or by referencing other projects from the Infra project. But what if I have some dependencies in my MVC project I want to add to the DI container?</p>\n<p>This is where my thought experiment broke down. As it stood, it would create a circular reference of the Infra project needing to know about classes in the main MVC project which in turn referenced the Infra project. I needed to go back to the drawing board and think about what I was trying to achieve.</p>\n<p>I broke the goal into the following thoughts:</p>\n<ol>\n<li>The main MVC project should not have direct references to projects that provide services other than the Infra project. This is to try to prevent developers from creating instances of classes directly</li>\n<li>Without direct access to those projects, it is not possible for the DI container to register those classes either if the Startup is in the main MVC project</li>\n<li>Moving the Startup and DI container registration to the Infra project will potentially create circular references if classes in the MVC project need to be registered</li>\n<li>Moving the Startup class out of the main MVC project creates a need to change the ApplicationName in the IHostingEnvironment for the controllers to be found</li>\n<li>Moving the Startup class into the Infra project means that the Infra project has to have knowledge of MVC features such as routing etc. which it should not really need to know as MVC is the consumer.</li>\n</ol>\n<p>By breaking down the goal, it hit me what is required!</p>\n<p>To achieve the goal set out above, a hybrid of the two approaches is needed whereby the Startup and DI container registration remain in the main MVC project, but registration of classes that I don't want to be directly accessed in the MVC project get registered in the Infra project so access in the MVC project is only through interfaces, serviced by the DI container.</p>\n<p>To achieve this, all I needed to do was make the Infra project aware of DI registration through the IServiceCollection interface and extension methods, but create a method that has the IServiceCollection injected into it from the MVC project that is calling it.</p>\n<h2 id=\"startup-separation\"><a name=\"startup-separation\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#startup-separation\">Startup Separation</a></h2>\n<p>The first part of the process was to refactor the work I has done in the Startup class in the Infra project and create a public static method to do that work, taking the dependencies from outside.</p>\n<!--?# Gist 30e690ecb7b227d67580d5c9d9b6240e /?-->\n<p>The new ConfigureServices method takes an IServiceCollection instance for registering services from within the infrastructure project, and also an IMvcBuilder as well, so that any MVC related infrastructure tasks that I want to hide from the main MVC project (and not dependent on code in the MVC project) can also be registered.</p>\n<p>In the example above, I add a custom validation filter (to ensure all post-back check if the ModelState is valid rather than this being done in each Action in the MVC controllers)&nbsp; and add the FluentValidation framework for domain validation.</p>\n<p>To make things a bit more interesting, I also added an extension method to use Autofac as the service provider instead of the out of the box Microsoft one.</p>\n<p>With this in place, a took the Startup class out of the Infra project and put it back into the MVC project and then refactored it so that it would do the following in the ConfigureServices method:</p>\n<ul>\n<li>Perform any local registrations, such as AddMvc and any classes that are in the MVC project</li>\n<li>Call the static methods created in the Infra project to register classes that are hidden away from the MVC project and use Autofac as the service provider.</li>\n</ul>\n<p>I ended up with a Startup class that looked like this:</p>\n<!--?# Gist ba73f98191531beacd2dd638e3d5b19f /?-->\n<h2 id=\"the-full-example\"><a name=\"the-full-example\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#the-full-example\">The Full Example</a></h2>\n<p>My description of the problem and my solution above only really scratches the surface, but hopefully it is of use to someone. It is probably better to let the code speak for itself, and for this I have created a Git repo with three versions of an example project which show the three different approaches to the problem.</p>\n<p>First is the out-of-the-box do everything in the main project</p>\n<p>Then there is the refactoring to do all the registration in the Infra project</p>\n<p>Lastly, there is the hybrid where the Startup is in the main project, but delegates registration of non-MVC classes to the Infra project.</p>\n<p>The example projects cover other things I plan on blogging about, so are a bit bigger that just dealing with separating the Startup class.</p>\n<p>The repo can be found at <a href=\"https://github.com/configureappio/SeparateStartup\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/SeparateStartup</a></p>\n<p>For details of the projects, look at the <a href=\"https://github.com/configureappio/SeparateStartup/blob/master/README.md\" target=\"_blank\" class=\"externallink\">Readme.md</a> file in the root of the repo.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>In answer to the question posed in the title of this post, my personal view is that the answer is - \"No\" ... but I do think that extracting out a lot of plumbing away from the Startup into another assembly does make things cleaner and achieves the goal of steering developers away from creating instances of classes and instead, relying on using the DI container to do the work it is intended for.&nbsp;This then helps promote SOLID principles.</p>\n<p>Hopefully, the discussion of the trials and tribulations I had in trying to completely move the Startup.cs class show how painful this can be and how a hybrid approach is more suitable.</p>\n<p>The underlying principle of using a clean code approach is sound when approached the correct way, by thinking through the actual goal rather than concentrating on trying to fix or workaround the framework you are using.</p>\n<p>The lessons I am taking away from my experiences above are:</p>\n<ul>\n<li>I am a big fan of clean architecture, but sometimes it is hard to implement when the frameworks you are working with are trying to make life easy for everyone and make assumptions about your code-base.</li>\n<li>It is very easy to tie yourself up in knots when you don't know what the framework is doing under the bonnet.</li>\n<li>If in doubt, go look at the source code of the framework, either through Git repos or by using the Source Stepping feature of Visual Studio.</li>\n<li>Look at 'what' you are trying to achieve rather than starting with the 'how' - in the case above, the actual goal I was trying to achieve was to abstract the dependency registration out of Startup rather than jumping straight in with 'move whole of the Startup.cs'.</li>\n</ul>\n",
    "sanitized": "I was recently listening to an episode of the brilliant .Net Rocks where Carl and Richard were talking to Steve Smith (a.k.a @ardalis) in which he talks about clean architecture in ASP.Net Core.\nOne of the things discussed was the separation of concerns, where Steve discusses creating an architecture in which you try to break up your application in such a way that hides implementation detail in one project from another consuming project. Instead, the consuming project is only aware of interfaces or abstract classes from shared libraries from which instances are are created by the dependency injection framework in use.\nThe aim is to try and guide a developer of the consuming project away from ‘new-ing’ up instances of a class from outside the project. To use Steve's catch phrase, \"new is glue\".\nI was listening to the podcast on my commute to work and it got me thinking about the project I had just started working on. So much so, that I had to put the podcast on pause to give myself some thinking time for the second half of the commute.\nWhat was causing the sparks to go off in my head was about how dependencies are registered in the Startup class in ASP.Net Core.\nBy default, when you create a new ASP.Net Core project, the Startup class is created as part of that project, and it is here that you register your dependencies. If your dependencies are in another project/assembly/Nuget package, it means that the references to wherever the dependency is has to be added to the consuming project.\nOf course, if you do this, that means that the developer of the consuming project is free to ‘new up’ an instance of a dependency rather than rely on the DI container. The gist of Steve Smith’s comment in the podcast was do what you can to help try to prevent this.\nWhen I got to work, I had a look at the code and pondered about whether the Startup class could be moved out to another project. That way the main ASP.Net project would only have a reference to the new project (we’ll call it the infrastructure project for simplicity) and not the myriad of other projects/Nugets. Simple huh? Yeah right!\nSo the first problem I hit was all the ASP/MVC plumbing that would be needed in the new project. When I copied the Startup class to the new project, Visual Studio started moaning about all the missing references.\nNow when you create a new MVC/Web.API project with .Net Core, the VS template uses the Microsoft.AspNetCore.All meta NuGet package. For those not familiar with meta packages, these are NuGet packages that bundle up a number of other NuGet packages – and Microsoft.AspNetCore.All is massive. When I opened the nuspec file from the cache on my machine, there were 136 dependencies on other packages. For my infrastructure project, I was not going to need all of these. I was only interested in the ones required to support the interfaces, classes and extension methods I would need in the Startup class.\nOh boy, that was a big mistake. It was a case of adding all the dependencies I would actually need one by one to ensure I was not bringing any unnecessary packages along for the ride. Painful, but I did it.\nSo I made all the updates to the main MVC project required to use the Startup class from my new project and remove the references I previously had to other projects (domain, repository etc) as this was the point of the exercise.\nIt all compiled! Great. Pressed F5 to run and … hang on what?\n\n\n\nAfter a bit of head scratching, I realised the problem was that MVC could not find the controller? WHY?\nAt this point, I parked my so-called ‘best practice’ changes as I did not want to waste valuable project time on a wild goose chase.\nThis was really bugging me, so outside of work, I started to do some more digging.\nAfter reading some blogs and looking at the source code in GitHub, the penny dropped. ASP.Net MVC makes the assumption that the controllers are in the same assembly as the Startup class.\nWhen the Startup class is registered with the host builder, it sets the ApplicationName property in HostingEnvironment instance to the name of the assembly where the Startup class is.\n\nThe ApplicationName property of the IHostingEnvironment instance is used by the AddMvc extension to register the assembly where controllers can be found.\n\nEventually, I found the workaround from David Fowler in an answer to an issue on GitHub. In short, you need to use the UseSetting extension method on the IWebHostBuilder instance to change the assembly used in the ApplicationName property to point to where the controllers are. In my case this was as follows:\nUseSetting(WebHostDefaults.ApplicationKey, typeof(Program).GetTypeInfo().Assembly.FullName)\nTherefore, without this line redirecting the application name to the correct assembly, if the controllers are not in the same assembly as the Startup class, that's when things go wrong - as I found.\nWith this problem fixed, everything fell into place and started working correctly.\nHowever, with this up and running, something did not feel right about it.\nThe solution I had created was fine if all the dependencies are accessible from the new Infra project, either directly within the project or by referencing other projects from the Infra project. But what if I have some dependencies in my MVC project I want to add to the DI container?\nThis is where my thought experiment broke down. As it stood, it would create a circular reference of the Infra project needing to know about classes in the main MVC project which in turn referenced the Infra project. I needed to go back to the drawing board and think about what I was trying to achieve.\nI broke the goal into the following thoughts:\n\nThe main MVC project should not have direct references to projects that provide services other than the Infra project. This is to try to prevent developers from creating instances of classes directly\nWithout direct access to those projects, it is not possible for the DI container to register those classes either if the Startup is in the main MVC project\nMoving the Startup and DI container registration to the Infra project will potentially create circular references if classes in the MVC project need to be registered\nMoving the Startup class out of the main MVC project creates a need to change the ApplicationName in the IHostingEnvironment for the controllers to be found\nMoving the Startup class into the Infra project means that the Infra project has to have knowledge of MVC features such as routing etc. which it should not really need to know as MVC is the consumer.\n\nBy breaking down the goal, it hit me what is required!\nTo achieve the goal set out above, a hybrid of the two approaches is needed whereby the Startup and DI container registration remain in the main MVC project, but registration of classes that I don't want to be directly accessed in the MVC project get registered in the Infra project so access in the MVC project is only through interfaces, serviced by the DI container.\nTo achieve this, all I needed to do was make the Infra project aware of DI registration through the IServiceCollection interface and extension methods, but create a method that has the IServiceCollection injected into it from the MVC project that is calling it.\nStartup Separation\nThe first part of the process was to refactor the work I has done in the Startup class in the Infra project and create a public static method to do that work, taking the dependencies from outside.\n\nThe new ConfigureServices method takes an IServiceCollection instance for registering services from within the infrastructure project, and also an IMvcBuilder as well, so that any MVC related infrastructure tasks that I want to hide from the main MVC project (and not dependent on code in the MVC project) can also be registered.\nIn the example above, I add a custom validation filter (to ensure all post-back check if the ModelState is valid rather than this being done in each Action in the MVC controllers)  and add the FluentValidation framework for domain validation.\nTo make things a bit more interesting, I also added an extension method to use Autofac as the service provider instead of the out of the box Microsoft one.\nWith this in place, a took the Startup class out of the Infra project and put it back into the MVC project and then refactored it so that it would do the following in the ConfigureServices method:\n\nPerform any local registrations, such as AddMvc and any classes that are in the MVC project\nCall the static methods created in the Infra project to register classes that are hidden away from the MVC project and use Autofac as the service provider.\n\nI ended up with a Startup class that looked like this:\n\nThe Full Example\nMy description of the problem and my solution above only really scratches the surface, but hopefully it is of use to someone. It is probably better to let the code speak for itself, and for this I have created a Git repo with three versions of an example project which show the three different approaches to the problem.\nFirst is the out-of-the-box do everything in the main project\nThen there is the refactoring to do all the registration in the Infra project\nLastly, there is the hybrid where the Startup is in the main project, but delegates registration of non-MVC classes to the Infra project.\nThe example projects cover other things I plan on blogging about, so are a bit bigger that just dealing with separating the Startup class.\nThe repo can be found at https://github.com/configureappio/SeparateStartup\nFor details of the projects, look at the Readme.md file in the root of the repo.\nConclusion\nIn answer to the question posed in the title of this post, my personal view is that the answer is - \"No\" ... but I do think that extracting out a lot of plumbing away from the Startup into another assembly does make things cleaner and achieves the goal of steering developers away from creating instances of classes and instead, relying on using the DI container to do the work it is intended for. This then helps promote SOLID principles.\nHopefully, the discussion of the trials and tribulations I had in trying to completely move the Startup.cs class show how painful this can be and how a hybrid approach is more suitable.\nThe underlying principle of using a clean code approach is sound when approached the correct way, by thinking through the actual goal rather than concentrating on trying to fix or workaround the framework you are using.\nThe lessons I am taking away from my experiences above are:\n\nI am a big fan of clean architecture, but sometimes it is hard to implement when the frameworks you are working with are trying to make life easy for everyone and make assumptions about your code-base.\nIt is very easy to tie yourself up in knots when you don't know what the framework is doing under the bonnet.\nIf in doubt, go look at the source code of the framework, either through Git repos or by using the Source Stepping feature of Visual Studio.\nLook at 'what' you are trying to achieve rather than starting with the 'how' - in the case above, the actual goal I was trying to achieve was to abstract the dependency registration out of Startup rather than jumping straight in with 'move whole of the Startup.cs'."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/using-iconfigureoptions",
    "raw": "<p>In one of my previous posts,&nbsp;<a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-4\">Hiding Secrets in appsettings.json – Using a Bridge in your ASP.Net Core Configuration (Part 4)</a>&nbsp;I had a comment from Anderson asking if I had considered IConfigureOptions<toptions> as a way of injecting dependencies into the TOptions class that had been bound to a configuration section.</toptions></p>\n<p>I had not come across the interface, so with an example provided by Anderson, I started to look further into it. <a href=\"https://andrewlock.net/access-services-inside-options-and-startup-using-configureoptions/\" target=\"_blank\" class=\"externallink\">Andrew Lock has post on his blog that describes how an implementation works</a>&nbsp;which was a starting point for me.</p>\n<h2 id=\"iconfigureoptions\"><a name=\"iconfigureoptions-toptions>\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#iconfigureoptions-toptions\">IConfigureOptions<toptions></toptions></a></h2>\n<p>The IConfigureOptions<toptions> interface has a single method (Configure) that takes an instance of TOptions where TOptions is a class that you will use for binding configuration values. If unfamiliar with this, please read my previous posts starting with&nbsp;<a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-1\">Creating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 1)</a>.</toptions></p>\n<!--?# Gist 37e2ab8ec4ea813919b21a67e132156a /?-->\n<p>In your implementation of IConfigureOptions<toptions>, you are free to manipulate the options instance by changing property values. Your implementation can have a constructor that will take any dependencies you may need into order to set the properties. These constructor parameters will be resolved by the DI container. However, be aware of different scopes that may apply and how these are resolved. <a href=\"https://andrewlock.net/access-services-inside-options-and-startup-using-configureoptions/\" target=\"_blank\" class=\"externallink\">Read Andrew's post for more info</a>.</toptions></p>\n<p>In the example below, I have:</p>\n<ul>\n<li>a class MySettings which I will bind the configuration section to.</li>\n<li>three implementations of IConfigureOptions<mysettings> which change the Setting1 string in different ways.</mysettings></li>\n<li>an interface IDateTimeResolver and class implementation to demonstrate the DI container injecting a resolved instance into the last of the&nbsp;IConfigureOptions<mysettings> implementations (and also serves as a&nbsp; reminder that using DateTime.Now directly in your code is evil and should be wrapped in another class so it can be substituted when unit testing)</mysettings></li>\n</ul>\n<!--?# Gist 91386848a0e7ed37656d83397fb7250f /?-->\n<p>The interface/implementation mappings are registered with the DI container in the Startup class. Note the Confgure<mysettings>() and AddOptions() extension methods have been registered first.</mysettings></p>\n<!--?# Gist d4ad9a95a1283f66eecdd982dc696085 /?-->\n<p>What is interesting, is that multiple classes can implement the interface and, when registered with the DI container, be applied in the order they have been registered.</p>\n<p>When I first saw this, I was confused as my understanding of DI registration was that the last registration is the one that is used to return the resolved instance by the DI container, so how could multiple registrations be applied? Of course, the penny dropped in that whatever is using these instances is not asking for the DI container to resolve a single instance, but is somehow iterating over the relevant registrations and resolving each registration in turn then calling the .Configure method.</p>\n<p>A quick delve in <a href=\"https://github.com/aspnet/\" target=\"_blank\" class=\"externallink\">Microsoft's ASP.Net repository</a> on GitHub, revealed the <a href=\"https://github.com/aspnet/Options/blob/dev/src/Microsoft.Extensions.Options/OptionsFactory.cs\" target=\"_blank\" class=\"externallink\">OptionsFactory</a> class.</p>\n<!--?# Gist 6784ae9d89341970874767f94034d976 /?-->\n<p>This pretty much confirmed the theory that the services collection is being iterated over, except you will notice that the constructor takes a parameter of&nbsp;IEnumerable&lt;IConfigureOptions&lt;TOptions&gt;&gt;. So what is resolving these multiple instances?</p>\n<p>Well it turns out to be the DI Container itself! I will go into more detail in a future post (as I don't want to deviate from the main topic of this post)*, but in short, if your constructor takes an IEnumerable<t> as a parameter, then the DI container will use all registered mappings of T to resolve and inject multiple instances as an enumeration.</t></p>\n<p><em>*Update - Since the post was written, Steve Gordon has written a great post that goes into more detail about the DI container and resolving multiple implementations&nbsp; - see&nbsp;<a href=\"https://www.stevejgordon.co.uk/asp-net-core-dependency-injection-registering-multiple-implementations-interface\" target=\"_blank\" class=\"externallink\">ASP.NET Core Dependency Injection – Registering Multiple Implementations of an Interface</a>.</em></p>\n<p>So in the OptionsFactory instance that will be created by ASP.Net using the DI container (where TOptions is MySettings, the class I have set up for configuration binding), the DI container will resolve the parameter of&nbsp;IEnumerable&lt;IConfigureOptions&lt;MySettings&gt;&gt; to our three registered implementations of IConfigureOptions<mysettings>. These will be injected and then the Create method in the factory will call the Configure method on each instance in order of registration in the Create method.</mysettings></p>\n<p>Therefore to demonstrate this, we will start with an appsettings.json that looks like this:</p>\n<!--?# Gist b8744fafdcdb0d33402ec4893e30942e /?-->\n<p>We can step through the three implementations as they are called by the factory Create method to see how the Settings1 value in MySettings is updated:</p>\n<!--?# LinkSizedImage \"StepThruConfigOptions-1.png\" Alt=\"Example of how the Settings1 value changes as each configuration option is applied\" /?-->\n<p>IConfigureNamedOptions<toption> and&nbsp;IPostConfigureOptions<toptions></toptions></toption></p>\n<p>The eagle eyed will have spotted that in the OptionsFactory, there is a check on each instance of IConfigureOption<toption> to see if it is an implementation of IConfigureNamedOptions<toption>.</toption></toption></p>\n<p>This is very similar to IConfigureOption<toption>, but the signature of the Configure method has an extra string parameter called name to point to a named configuration instance.</toption></p>\n<p>Once each of the&nbsp;registered implementations of IConfigureOption<toption> has been processed, the factory then looks for implementations of the IPostConfigureOptions<toptions> interface.</toptions></toption></p>\n<p>I plan on covering these two interfaces in a future post as I don't want to make this post too long.</p>\n<h2 id=\"summary\"><a name=\"summary\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#summary\">Summary</a></h2>\n<p>At first glance, this approach is similar to the bridge pattern approach I have described in my earlier posts in that the responsibility for changing the state values is delegated to an outside class.</p>\n<p>However it is not a bridge as the properties of the underlying configuration instance are being changed. A bridge will intercept requests and then change the return value, leaving the underlying configuration instance unchanged.</p>\n<p>When using the&nbsp;IConfigureOptions<toptions> approach, the TOptions instance that is passed in through the parameter in the Configure method is being mutated. The changes will live on in the instance.</toptions></p>\n<p>My personal opinion is that this not as clean as the bridge approach (but I guess I am biased).</p>\n<p>I prefer to try to avoid potentially unexpected mutation of objects and instead return either a new instance of the object with property values copied from the existing into the new one or a substitute (e.g. the bridge). In other words, a functional approach where the input object is treated as immutable.</p>\n<p>This leaves the original instance of the class in its initialised state which may be required by other functionality in the application that does not want the updated values or can be used by a different bridge class.</p>\n",
    "sanitized": "In one of my previous posts, Hiding Secrets in appsettings.json – Using a Bridge in your ASP.Net Core Configuration (Part 4) I had a comment from Anderson asking if I had considered IConfigureOptions as a way of injecting dependencies into the TOptions class that had been bound to a configuration section.\nI had not come across the interface, so with an example provided by Anderson, I started to look further into it. Andrew Lock has post on his blog that describes how an implementation works which was a starting point for me.\nIConfigureOptions\nThe IConfigureOptions interface has a single method (Configure) that takes an instance of TOptions where TOptions is a class that you will use for binding configuration values. If unfamiliar with this, please read my previous posts starting with Creating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 1).\n\nIn your implementation of IConfigureOptions, you are free to manipulate the options instance by changing property values. Your implementation can have a constructor that will take any dependencies you may need into order to set the properties. These constructor parameters will be resolved by the DI container. However, be aware of different scopes that may apply and how these are resolved. Read Andrew's post for more info.\nIn the example below, I have:\n\na class MySettings which I will bind the configuration section to.\nthree implementations of IConfigureOptions which change the Setting1 string in different ways.\nan interface IDateTimeResolver and class implementation to demonstrate the DI container injecting a resolved instance into the last of the IConfigureOptions implementations (and also serves as a  reminder that using DateTime.Now directly in your code is evil and should be wrapped in another class so it can be substituted when unit testing)\n\n\nThe interface/implementation mappings are registered with the DI container in the Startup class. Note the Confgure() and AddOptions() extension methods have been registered first.\n\nWhat is interesting, is that multiple classes can implement the interface and, when registered with the DI container, be applied in the order they have been registered.\nWhen I first saw this, I was confused as my understanding of DI registration was that the last registration is the one that is used to return the resolved instance by the DI container, so how could multiple registrations be applied? Of course, the penny dropped in that whatever is using these instances is not asking for the DI container to resolve a single instance, but is somehow iterating over the relevant registrations and resolving each registration in turn then calling the .Configure method.\nA quick delve in Microsoft's ASP.Net repository on GitHub, revealed the OptionsFactory class.\n\nThis pretty much confirmed the theory that the services collection is being iterated over, except you will notice that the constructor takes a parameter of IEnumerable<IConfigureOptions<TOptions>>. So what is resolving these multiple instances?\nWell it turns out to be the DI Container itself! I will go into more detail in a future post (as I don't want to deviate from the main topic of this post)*, but in short, if your constructor takes an IEnumerable as a parameter, then the DI container will use all registered mappings of T to resolve and inject multiple instances as an enumeration.\n*Update - Since the post was written, Steve Gordon has written a great post that goes into more detail about the DI container and resolving multiple implementations  - see ASP.NET Core Dependency Injection – Registering Multiple Implementations of an Interface.\nSo in the OptionsFactory instance that will be created by ASP.Net using the DI container (where TOptions is MySettings, the class I have set up for configuration binding), the DI container will resolve the parameter of IEnumerable<IConfigureOptions<MySettings>> to our three registered implementations of IConfigureOptions. These will be injected and then the Create method in the factory will call the Configure method on each instance in order of registration in the Create method.\nTherefore to demonstrate this, we will start with an appsettings.json that looks like this:\n\nWe can step through the three implementations as they are called by the factory Create method to see how the Settings1 value in MySettings is updated:\n\nIConfigureNamedOptions and IPostConfigureOptions\nThe eagle eyed will have spotted that in the OptionsFactory, there is a check on each instance of IConfigureOption to see if it is an implementation of IConfigureNamedOptions.\nThis is very similar to IConfigureOption, but the signature of the Configure method has an extra string parameter called name to point to a named configuration instance.\nOnce each of the registered implementations of IConfigureOption has been processed, the factory then looks for implementations of the IPostConfigureOptions interface.\nI plan on covering these two interfaces in a future post as I don't want to make this post too long.\nSummary\nAt first glance, this approach is similar to the bridge pattern approach I have described in my earlier posts in that the responsibility for changing the state values is delegated to an outside class.\nHowever it is not a bridge as the properties of the underlying configuration instance are being changed. A bridge will intercept requests and then change the return value, leaving the underlying configuration instance unchanged.\nWhen using the IConfigureOptions approach, the TOptions instance that is passed in through the parameter in the Configure method is being mutated. The changes will live on in the instance.\nMy personal opinion is that this not as clean as the bridge approach (but I guess I am biased).\nI prefer to try to avoid potentially unexpected mutation of objects and instead return either a new instance of the object with property values copied from the existing into the new one or a substitute (e.g. the bridge). In other words, a functional approach where the input object is treated as immutable.\nThis leaves the original instance of the class in its initialised state which may be required by other functionality in the application that does not want the updated values or can be used by a different bridge class."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/istartup-gotcha",
    "raw": "<p>In this post I look at a problem I had with injecting values into the constructor of a class that implemented IStartup and why doing this does not work as expected.</p>\n<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>When learning a new technology, I like to get to know the inner workings of it to try and understand how my code will work and develop a set of my own best practices.</p>\n<p>Two areas of ASP.Net Core that I have a lot of interest in are the startup pipeline and configuration. The latter of these has been the focus of my previous posts about how to set up configuration using a SOLID approach.</p>\n<p>For the former, I have been reading several blog posts, the most illuminating of which has been <a href=\"https://www.stevejgordon.co.uk/\" target=\"_blank\" class=\"externallink\">Steve Gordon's blog</a>&nbsp;which I highly recommend a read of. (Full disclosure - Steve runs the local <a href=\"http://www.dotnetsoutheast.co.uk/\">.Net South East user group</a> that I regularly attend).</p>\n<p>In particular, I found his <a href=\"https://www.stevejgordon.co.uk/aspnet-core-anatomy-how-does-usestartup-work\" target=\"_blank\" class=\"externallink\">post on the Startup process</a> illuminating as it reveals a whole load of reflection work being done by the ConventionBasedStartup class when using a convention based Startup class.</p>\n<p>Now whilst I appreciate that Microsoft wants to make the Startup class as simple as possible to implement out-of-the-box in a new project, I am not a massive fan of this approach once you know what you are doing. After reading Steve's blog post, it seemed logical to refactor my Startup classes to implement the IStartup interface as this is the first thing that is checked when instantiating the class (as shown in Steve G's gist here)</p>\n<!--?# Gist e3735eb251ab9374de41c1ef0953da68 /?-->\n<p>On reading through how the ConventionBasedStartup class constructs an implementation of IStartup, I came to the following conclusions:</p>\n<ul>\n<li>There is a lot of reflection going on that, if IStartup was used, would not be required</li>\n<li>It is not statically typed, so if signatures are incorrect in some way such as a typo, it will fail at runtime but not be caught at compile time</li>\n<li>The ability to have multiple versions of the same method for different environments in the same class differentiated by name E.g. ConfigureServicesDevelopment, ConfigureServicesStaging and relying on the ASPNETCORE_ENVIRONMENT environmental variable feels clunky and doesn't feel like a SOLID approach.</li>\n</ul>\n<p>I can see why Microsoft has taken this approach as it is easy to provide boilerplate code that is easy to extend, but for me, the convention approach does not feel right for the Startup process (though I reserve the right to be hypocritical in that I have been using the convention based approach for years in letting ASP.Net MVC work out which controller is being used in routing, but hey, I'm not perfect! ).</p>\n<p>Therefore, in my ASP.Net Core projects, I started to refactor the Startup classes to implement the IStartup interface.</p>\n<p>This was the approach I took on a project I was working on for a client. I had recently added NLog and this was working nicely using the convention based Startup, but in an effort to clean up the code, I wanted to refactor to IStartup. However, for some strange reason, the NLog logger stopped working after refactoring.</p>\n<p>Time to get the deerstalker and magnifying glass out.</p>\n<h2 id=\"how-using-istartup-differs-from-convention-based-startup\"><a name=\"how-using-istartup differs-from-convention-based-startup\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#how-using-istartup%20differs-from-convention-based-startup\">How Using IStartup Differs from Convention Based Startup</a></h2>\n<p>At first glance, the Startup classes used in both approaches appear to be the same, but there is one significant difference.</p>\n<p>In the convention based approach, you are free to add as many parameters as you want to the Configure method, safe in the knowledge that as long as you (or rather in most cases, the WebHost.CreateDefaultBuilder) have registered the interfaces or classes you require with the Dependency Injection services, then they will be injected into the method.</p>\n<!--?# Gist f3cf0982bf92106ddf46927fdab46c88 /?-->\n<p>This is thanks to the 'magic' inside the ConventionBasedStartup. When implementing IStartup yourself, the signatures are set in the contract</p>\n<!--?# Gist 7aae4c738628c17d8dd65206854b4ab9 /?-->\n<p>Of particular note is the signature for the Configure method which only accepts a single parameter of type IApplicationBuilder.</p>\n<p>Therefore, if you want to inject other types, you can't just add them to the method parameters, so you need to find another way.</p>\n<h2 id=\"adding-iloggerfactory-to-the-startup-constructor-dont-do-this-at-home\"><a name=\"adding-iloggerfactory-to-the-startup-constructor\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#adding-iloggerfactory-to-the-startup-constructor\">Adding ILoggerFactory to the Startup Constructor - Don't Do This At Home</a></h2>\n<p><strong>TL;DR You can skip this bit if you want as it describes how I got to the bottom of the problem. If you just want to know how to correctly access the ILoggerFactory, go to <a href=\"https://stevetalkscode.co.uk/#Solution\">here</a></strong></p>\n<p>When writing code that implements an interface, if I need extra class instances that are not provided for in the method signature, my usual approach is to make an instance variable that is assigned from a constructor parameter.</p>\n<p>So when I no longer had access to ILoggerFactory in the Configure method, I added it to the Startup constructor and created an instance variable that I could then accessed from inside the Configure method. So far, so normal. I did the NLog configuration in the Configure method and just expected things to work as they had before.</p>\n<p>Alas, twas not to be ...</p>\n<p>At first I thought that I must have messed up something in the NLog configuration, but couldn't see anything different. So I stepped through the code. I looked at the ILoggerFactory that had been injected into the constructor of the Startup.</p>\n<p>There were two providers present, Console and Debug. I then ran through the code to where NLog had been added, and looked at the providers collection again. Yup, three providers with NLog being the last. All good.</p>\n<p>I then ran though to the constructor of the controller where I was doing some logging where I had declared a parameter of ILogger. I looked at the Loggers collection on the logger and there were four loggers, but no sign of the NLog logger.</p>\n<p>Somewhere along the pipeline, the ILoggerFactory had lost NLog and added two loggers, both of which were the ApplicationInsights logger.</p>\n<p>For the client's project I reverted back to the convention based approach so as not to hold up the development, but my curiosity got the better of me and decided to dig a bit more.</p>\n<h2 id=\"compare-and-contrast\"><a name=\"compare-and-contrast\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#compare-and-contrast\">Compare and Contrast</a></h2>\n<p>At home, I created a simple project where I created two copies of the Startup class. One using the convention approach where ILoggerFactory is injected into the Configure method and one where I have the ILoggerFactory injected into the constructor of the Startup class and saved in an instance variable. I then used a compilation symbol to run with one or the other.</p>\n<p>I created a dummy class that implemented ILoggerProvider rather than muddy the waters by using NLog and having to worry about its configuration. I then implemented the constructor based code as I had previously done, as shown below:</p>\n<!--?# Gist 2b9478ccfb85aa996e9a506b7138be8b /?-->\n<p>I ran through both scenarios using the Visual Studio debugger and got the same result.</p>\n<ul>\n<li>When using convention based Startup class, there were five loggers, Debug, Console, 2 x Application Insights and my DummyProvider.</li>\n<li>When using the IStartup implementation, there were just the four loggers - DummyProvider was missing.</li>\n</ul>\n<p>So somehow, the logging factory instance in my Startup class is not the one that makes it to the controller.</p>\n<h2 id=\"the-correct-way-to-access-the-logger-factory\"><a name=\"correct-way-to-access-the-logger-factory\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#correct-way-to-access-the-logger-factory\">The Correct Way to Access the Logger Factory</a></h2>\n<p>OK, so the constructor approach does not work. It then dawned on me - is there another way to get to the ILoggerFactory instance through the IApplicationBuilder that is provided via the IStartup.Configure(IApplicationBuilder app) method?</p>\n<p>In short, yes! The app.ApplicationServices provides access the the IServiceProvider container where we can resolve services.</p>\n<!--?# Gist 02dcf4d6d58f7bfa0e061053df4ca9f8 /?-->\n<p>I ran the test again, and there was the DummyLogProvider.</p>\n<!--?# LinkSizedImage \"curious_provider_with_convension_startup.png\" Alt=\"The Logger Is There!\" /?-->\n<p>Problem solved.!</p>\n",
    "sanitized": "In this post I look at a problem I had with injecting values into the constructor of a class that implemented IStartup and why doing this does not work as expected.\nBackground\nWhen learning a new technology, I like to get to know the inner workings of it to try and understand how my code will work and develop a set of my own best practices.\nTwo areas of ASP.Net Core that I have a lot of interest in are the startup pipeline and configuration. The latter of these has been the focus of my previous posts about how to set up configuration using a SOLID approach.\nFor the former, I have been reading several blog posts, the most illuminating of which has been Steve Gordon's blog which I highly recommend a read of. (Full disclosure - Steve runs the local .Net South East user group that I regularly attend).\nIn particular, I found his post on the Startup process illuminating as it reveals a whole load of reflection work being done by the ConventionBasedStartup class when using a convention based Startup class.\nNow whilst I appreciate that Microsoft wants to make the Startup class as simple as possible to implement out-of-the-box in a new project, I am not a massive fan of this approach once you know what you are doing. After reading Steve's blog post, it seemed logical to refactor my Startup classes to implement the IStartup interface as this is the first thing that is checked when instantiating the class (as shown in Steve G's gist here)\n\nOn reading through how the ConventionBasedStartup class constructs an implementation of IStartup, I came to the following conclusions:\n\nThere is a lot of reflection going on that, if IStartup was used, would not be required\nIt is not statically typed, so if signatures are incorrect in some way such as a typo, it will fail at runtime but not be caught at compile time\nThe ability to have multiple versions of the same method for different environments in the same class differentiated by name E.g. ConfigureServicesDevelopment, ConfigureServicesStaging and relying on the ASPNETCORE_ENVIRONMENT environmental variable feels clunky and doesn't feel like a SOLID approach.\n\nI can see why Microsoft has taken this approach as it is easy to provide boilerplate code that is easy to extend, but for me, the convention approach does not feel right for the Startup process (though I reserve the right to be hypocritical in that I have been using the convention based approach for years in letting ASP.Net MVC work out which controller is being used in routing, but hey, I'm not perfect! ).\nTherefore, in my ASP.Net Core projects, I started to refactor the Startup classes to implement the IStartup interface.\nThis was the approach I took on a project I was working on for a client. I had recently added NLog and this was working nicely using the convention based Startup, but in an effort to clean up the code, I wanted to refactor to IStartup. However, for some strange reason, the NLog logger stopped working after refactoring.\nTime to get the deerstalker and magnifying glass out.\nHow Using IStartup Differs from Convention Based Startup\nAt first glance, the Startup classes used in both approaches appear to be the same, but there is one significant difference.\nIn the convention based approach, you are free to add as many parameters as you want to the Configure method, safe in the knowledge that as long as you (or rather in most cases, the WebHost.CreateDefaultBuilder) have registered the interfaces or classes you require with the Dependency Injection services, then they will be injected into the method.\n\nThis is thanks to the 'magic' inside the ConventionBasedStartup. When implementing IStartup yourself, the signatures are set in the contract\n\nOf particular note is the signature for the Configure method which only accepts a single parameter of type IApplicationBuilder.\nTherefore, if you want to inject other types, you can't just add them to the method parameters, so you need to find another way.\nAdding ILoggerFactory to the Startup Constructor - Don't Do This At Home\nTL;DR You can skip this bit if you want as it describes how I got to the bottom of the problem. If you just want to know how to correctly access the ILoggerFactory, go to here\nWhen writing code that implements an interface, if I need extra class instances that are not provided for in the method signature, my usual approach is to make an instance variable that is assigned from a constructor parameter.\nSo when I no longer had access to ILoggerFactory in the Configure method, I added it to the Startup constructor and created an instance variable that I could then accessed from inside the Configure method. So far, so normal. I did the NLog configuration in the Configure method and just expected things to work as they had before.\nAlas, twas not to be ...\nAt first I thought that I must have messed up something in the NLog configuration, but couldn't see anything different. So I stepped through the code. I looked at the ILoggerFactory that had been injected into the constructor of the Startup.\nThere were two providers present, Console and Debug. I then ran through the code to where NLog had been added, and looked at the providers collection again. Yup, three providers with NLog being the last. All good.\nI then ran though to the constructor of the controller where I was doing some logging where I had declared a parameter of ILogger. I looked at the Loggers collection on the logger and there were four loggers, but no sign of the NLog logger.\nSomewhere along the pipeline, the ILoggerFactory had lost NLog and added two loggers, both of which were the ApplicationInsights logger.\nFor the client's project I reverted back to the convention based approach so as not to hold up the development, but my curiosity got the better of me and decided to dig a bit more.\nCompare and Contrast\nAt home, I created a simple project where I created two copies of the Startup class. One using the convention approach where ILoggerFactory is injected into the Configure method and one where I have the ILoggerFactory injected into the constructor of the Startup class and saved in an instance variable. I then used a compilation symbol to run with one or the other.\nI created a dummy class that implemented ILoggerProvider rather than muddy the waters by using NLog and having to worry about its configuration. I then implemented the constructor based code as I had previously done, as shown below:\n\nI ran through both scenarios using the Visual Studio debugger and got the same result.\n\nWhen using convention based Startup class, there were five loggers, Debug, Console, 2 x Application Insights and my DummyProvider.\nWhen using the IStartup implementation, there were just the four loggers - DummyProvider was missing.\n\nSo somehow, the logging factory instance in my Startup class is not the one that makes it to the controller.\nThe Correct Way to Access the Logger Factory\nOK, so the constructor approach does not work. It then dawned on me - is there another way to get to the ILoggerFactory instance through the IApplicationBuilder that is provided via the IStartup.Configure(IApplicationBuilder app) method?\nIn short, yes! The app.ApplicationServices provides access the the IServiceProvider container where we can resolve services.\n\nI ran the test again, and there was the DummyLogProvider.\n\nProblem solved.!"
  },
  {
    "itemId": "https://stevetalkscode.co.uk/configuration-bridging-part-4",
    "raw": "<p>This is part 4 of a series where I have been looking at&nbsp;moving to a SOLID approach of implementing configuration binding in ASP.Net Core using a bridging class to remove the need for consumers of the configuration object to use IOptions<t>&nbsp; or IOptionsSnapshot<t>.&nbsp;If you have arrived at this page from a search engine, I recommend looking at the previous posts&nbsp;<a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-1\">Part 1</a>,&nbsp;<a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-2\">Part 2</a>&nbsp;and&nbsp;<a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-3\">Part 3</a>&nbsp;before moving onto this one.</t></t></p>\n<p>In this post I move onto looking at injecting some functionality into the bridge class to decrypt settings and validate the settings read. Lastly I show registering the bridge class via multiple fine grained interfaces.</p>\n<p>To follow along with this post, I suggest you download the full solution source code from the Github repo at&nbsp;<a href=\"https://github.com/configureappio/ConfiguarationBridgeCrypto\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/ConfiguarationBridgeCrypto</a>&nbsp;as there is far too much code to display in this post.</p>\n<h2 id=\"adding-more-di-services\"><a name=\"adding-more-di-services\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#adding-more-di-services\">Adding More DI Services</a></h2>\n<p>I will start with the changes to the Startup.cs ConfigureServices method which gives a structure to the changes we will be making.</p>\n<!--?# Gist 6de2917887f97e86edf5189ae24af701 /?-->\n<p>The main highlights are:</p>\n<ul>\n<li>A factory class is registered via its interface to decrypt values read from the settings</li>\n<li>A class via its interface is registered to validate the settings</li>\n<li>The bridge class is registered via an aggregate interface</li>\n<li>A resolution lambda is registered for each of the component interfaces that make up the aggregate interface.</li>\n</ul>\n<p>The classes and interfaces are now looked at in more detail below.</p>\n<h2 id=\"encrypted-settings\"><a name=\"encrypted-settings\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#encrypted-settings\">Encrypted Settings</a></h2>\n<p>For the purpose of this demonstration, I am assuming that for one reason or another,&nbsp; the standard secure configuration providers such as Azure Key Vault cannot be used for one reason or another, so we are having to deal with encrypting the settings ourselves.</p>\n<p><em>Heath Warning !!!</em></p>\n<p>In this demo, the encrypted settings are in the main appsettings.json.&nbsp;<strong>DO NOT DO THIS IN THE REAL WORLD!</strong>&nbsp; Stick to the mantra that you should not put any secrets in your code source control. Always think, \"Would I be OK with the source code repo going open source?\"</p>\n<p>In the source code, I have included code to read from an external file outside of the web code location so that the secrets are maintained outside of source code, but the settings could come from environmental variables or the command line. If copying the source code that accompanies this post, I suggest copying the appsettings.json to the location shown and removing outside of source code. It is up to you where to store it.</p>\n<!--?# Gist 5ca8343417d37c1cd5ef7987a1b521c2 /?-->\n<p>To keep things clean, all encrypted values are held in a dictionary within the AppSettings class in a property called Secrets. It will be the responsibility of the bridge class to decrypt the secrets and inject them into the properties exposed via the interfaces (more on this later).</p>\n<p>The appsettings.json and&nbsp;matching MyAppSettings class will therefore look like this:</p>\n<!--?# Gist 61f3f0a06d4370c6016af7efcbf98d48 /?-->\n<!--?# Gist cfb6c311bfe333ce8d47c757f23928aa /?-->\n<p>At this point, the DI container has been configured to return IOptionsSnapshot<myappsettings> by the services.Configure<myappsettings> line in the startup code.</myappsettings></myappsettings></p>\n<p>The secrets dictionary key/value pairs have been encrypted using a hash for the key and AESManaged for the value. Both have then been Base64 encoded so that they can be cleanly represented in the JSON.</p>\n<h2 id=\"decrypting-the-settings\"><a name=\"decrypting-the-settings\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#decrypting-the-settings\">Decrypting the Settings</a></h2>\n<p>In order for the bridge class to decrypt the dictionary, we will need a class that will get injected into the bridge class via an ICryptoAlgorithm interface. To keep things flexible, we will use a factory pattern to create the decryptor instance.</p>\n<!--?# Gist 51a3bfcf3f332f1ab167ba6a34f6b923 /?-->\n<p>In the example code, I register the results of calling the factory as a singleton that. However, you may want to register the factory and inject that into classes if you want to use multiple cryptographic algorithms or salt/password combinations.</p>\n<p>Once we have the decryptor registered, we need to apply it to the settings. For this, we will have a SettingsDecryptor class that implements an ISettingsDecrypt interface.</p>\n<!--?# Gist efc0bc4b4db24b749c4424c90f33078f /?-->\n<p>This is registered with the DI container services ready to be used by the bridge class. The Decrypt method in the example takes a plain text version of the dictionary key then</p>\n<ul>\n<li>hashes it with the method exposed by the injected decryptor,</li>\n<li>looks up the hashed key (that is in the appsettings.json)</li>\n<li>then decrypts the value for that key.</li>\n</ul>\n<p>So with these pieces in place, we have the components for decrypting the settings in the bridge class.</p>\n<p>Before looking at the bridge, we will look at injecting functionality to validate the settings.</p>\n<h2 id=\"validating-settings\"><a name=\"validating-settings\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#validating-settings\">Validating Settings</a></h2>\n<p>By injecting a settings validator into the bridge class, we have the ability to catch any problems before the rest of our code tries to use the values (encrypted or not).</p>\n<!--?# Gist 826c6cb896f93b3cd2c1cee11d83aa35 /?-->\n<p>The class implements an interface that validates the settings and returns a boolean to indicate success or failure. If validation has failed, an AggregateException instance is available that holds one or more validation exceptions.</p>\n<p>There are more elegant ways in which this can be approached, but I used this approach for simplicity to illustrate the principle of injecting a validator into the bridge.</p>\n<p>Once the validator is registered as a DI service, we are now ready to register the bridge class that takes both the decryptor and validator.</p>\n<h2 id=\"take-it-to-the-bridge\"><a name=\"take-it-to-the-bridge\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#take-it-to-the-bridge\">Take It To The Bridge</a></h2>\n<!--?# Gist 893f55c1220bb60758a32b61caf7280d /?-->\n<p>The example above is fairly self-explanatory at a high level. The constructor takes the IOptionsShapshot<myappsettings> to get the settings class that has been constructed by the DI service using the Options pattern. This gives us access to the bound object.</myappsettings></p>\n<p>We then have a decryptor which is stored as an instance field for use by the property getters to decrypt values read from the settings object.</p>\n<p>We then have a validator instance which we call immediately to validate the settings and throw an exception if there is a problem.</p>\n<p>The three properties exposed are proxies to the underlying settings class, with decryption taking place where necessary.</p>\n<h2 id=\"interface-segregation\"><a name=\"interface-segregation\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#interface-segregation\">Interface Segregation</a></h2>\n<!--?# Gist f8ca50e7a93b913a18697fab5c8f8e5a /?-->\n<p>The bridge class implements the IAppSettingsResolved interface which is an aggregate of three other interfaces.</p>\n<p>This has been done to illustrate that the settings can be registered as multiple interfaces to allow for interface segregation as part of the SOLID approach. E.g. if a controller is only interested in the SQL Server connection string, it can just ask for that rather than the full&nbsp;&nbsp;IAppSettingsResolved. This makes it easier to implement just the required functionality in any mocks when unit testing or any other implementation you may want to register.</p>\n<!--?# Gist e2afd62b066f48196b94892c4f2fe8e2 /?-->\n<p>The registration above uses the service for IAppSettingsResolved to resolve the three other interfaces.</p>\n<h2 id=\"conclusion\"><a name=\"conclusion\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#conclusion\">Conclusion</a></h2>\n<p>Having described the working parts above, we can come back to the ConfigureSevices method to tie it together.</p>\n<!--?# Gist 6de2917887f97e86edf5189ae24af701 /?-->\n<p>Here we have done the following</p>\n<ul>\n<li>Registered an IOptionsSnapshot<myappsettings> using the Configure method to bind the \"MyAppSettings\" configuration section to an object instance</myappsettings></li>\n<li>Registered a decryption algorithm</li>\n<li>Registered&nbsp;a class instance to decrypt the key/value pairs in the Secrets dictionary using the algorithm</li>\n<li>Registered&nbsp;a class instance to validate the settingsop</li>\n<li>Registered a class instance to act as the bridge/proxy to the&nbsp;IOptionsSnapshot<myappsettings> and decrypt the values</myappsettings></li>\n<li>Registered the resolved bridge class using its multiple exposed interfaces for finer grained use</li>\n</ul>\n<p>With the last of these in place, our controllers and any other dependant classes can choose whether to get the settings as a whole via IAppSettingsResolved or one of the finer grained interfaces</p>\n<ul>\n<li>IAppSettings - the non-encrypted values</li>\n<li>ISqlConnectionString - the decrypted SQL connection string</li>\n<li>IOracleConnectionString - the decrypted Oracle connection string</li>\n</ul>\n<h2 id=\"taking-things-further\"><a name=\"taking-things-further\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#taking-things-further\">Taking Things Further</a></h2>\n<p>That wraps up this series of posts for now, but you may want to take things further now that the settings class can have functionality injected into it.&nbsp; Possibilities include</p>\n<ul>\n<li>Using connection string builders to create connection strings using multiple properties (some encrypted, some not) from the bound object</li>\n<li>Using the ICryptoFactory instead of ICryptoAlgorithm to use multiple algorithms for different properties</li>\n</ul>\n<p>Don't forget, the full source code including a WPF app to encrypt the settings dictionary can the downloaded from the Github repo at&nbsp;<a href=\"https://github.com/configureappio/ConfiguarationBridgeCrypto\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/ConfiguarationBridgeCrypto</a></p>\n<p>Thanks for reading.</p>\n",
    "sanitized": "This is part 4 of a series where I have been looking at moving to a SOLID approach of implementing configuration binding in ASP.Net Core using a bridging class to remove the need for consumers of the configuration object to use IOptions  or IOptionsSnapshot. If you have arrived at this page from a search engine, I recommend looking at the previous posts Part 1, Part 2 and Part 3 before moving onto this one.\nIn this post I move onto looking at injecting some functionality into the bridge class to decrypt settings and validate the settings read. Lastly I show registering the bridge class via multiple fine grained interfaces.\nTo follow along with this post, I suggest you download the full solution source code from the Github repo at https://github.com/configureappio/ConfiguarationBridgeCrypto as there is far too much code to display in this post.\nAdding More DI Services\nI will start with the changes to the Startup.cs ConfigureServices method which gives a structure to the changes we will be making.\n\nThe main highlights are:\n\nA factory class is registered via its interface to decrypt values read from the settings\nA class via its interface is registered to validate the settings\nThe bridge class is registered via an aggregate interface\nA resolution lambda is registered for each of the component interfaces that make up the aggregate interface.\n\nThe classes and interfaces are now looked at in more detail below.\nEncrypted Settings\nFor the purpose of this demonstration, I am assuming that for one reason or another,  the standard secure configuration providers such as Azure Key Vault cannot be used for one reason or another, so we are having to deal with encrypting the settings ourselves.\nHeath Warning !!!\nIn this demo, the encrypted settings are in the main appsettings.json. DO NOT DO THIS IN THE REAL WORLD!  Stick to the mantra that you should not put any secrets in your code source control. Always think, \"Would I be OK with the source code repo going open source?\"\nIn the source code, I have included code to read from an external file outside of the web code location so that the secrets are maintained outside of source code, but the settings could come from environmental variables or the command line. If copying the source code that accompanies this post, I suggest copying the appsettings.json to the location shown and removing outside of source code. It is up to you where to store it.\n\nTo keep things clean, all encrypted values are held in a dictionary within the AppSettings class in a property called Secrets. It will be the responsibility of the bridge class to decrypt the secrets and inject them into the properties exposed via the interfaces (more on this later).\nThe appsettings.json and matching MyAppSettings class will therefore look like this:\n\n\nAt this point, the DI container has been configured to return IOptionsSnapshot by the services.Configure line in the startup code.\nThe secrets dictionary key/value pairs have been encrypted using a hash for the key and AESManaged for the value. Both have then been Base64 encoded so that they can be cleanly represented in the JSON.\nDecrypting the Settings\nIn order for the bridge class to decrypt the dictionary, we will need a class that will get injected into the bridge class via an ICryptoAlgorithm interface. To keep things flexible, we will use a factory pattern to create the decryptor instance.\n\nIn the example code, I register the results of calling the factory as a singleton that. However, you may want to register the factory and inject that into classes if you want to use multiple cryptographic algorithms or salt/password combinations.\nOnce we have the decryptor registered, we need to apply it to the settings. For this, we will have a SettingsDecryptor class that implements an ISettingsDecrypt interface.\n\nThis is registered with the DI container services ready to be used by the bridge class. The Decrypt method in the example takes a plain text version of the dictionary key then\n\nhashes it with the method exposed by the injected decryptor,\nlooks up the hashed key (that is in the appsettings.json)\nthen decrypts the value for that key.\n\nSo with these pieces in place, we have the components for decrypting the settings in the bridge class.\nBefore looking at the bridge, we will look at injecting functionality to validate the settings.\nValidating Settings\nBy injecting a settings validator into the bridge class, we have the ability to catch any problems before the rest of our code tries to use the values (encrypted or not).\n\nThe class implements an interface that validates the settings and returns a boolean to indicate success or failure. If validation has failed, an AggregateException instance is available that holds one or more validation exceptions.\nThere are more elegant ways in which this can be approached, but I used this approach for simplicity to illustrate the principle of injecting a validator into the bridge.\nOnce the validator is registered as a DI service, we are now ready to register the bridge class that takes both the decryptor and validator.\nTake It To The Bridge\n\nThe example above is fairly self-explanatory at a high level. The constructor takes the IOptionsShapshot to get the settings class that has been constructed by the DI service using the Options pattern. This gives us access to the bound object.\nWe then have a decryptor which is stored as an instance field for use by the property getters to decrypt values read from the settings object.\nWe then have a validator instance which we call immediately to validate the settings and throw an exception if there is a problem.\nThe three properties exposed are proxies to the underlying settings class, with decryption taking place where necessary.\nInterface Segregation\n\nThe bridge class implements the IAppSettingsResolved interface which is an aggregate of three other interfaces.\nThis has been done to illustrate that the settings can be registered as multiple interfaces to allow for interface segregation as part of the SOLID approach. E.g. if a controller is only interested in the SQL Server connection string, it can just ask for that rather than the full  IAppSettingsResolved. This makes it easier to implement just the required functionality in any mocks when unit testing or any other implementation you may want to register.\n\nThe registration above uses the service for IAppSettingsResolved to resolve the three other interfaces.\nConclusion\nHaving described the working parts above, we can come back to the ConfigureSevices method to tie it together.\n\nHere we have done the following\n\nRegistered an IOptionsSnapshot using the Configure method to bind the \"MyAppSettings\" configuration section to an object instance\nRegistered a decryption algorithm\nRegistered a class instance to decrypt the key/value pairs in the Secrets dictionary using the algorithm\nRegistered a class instance to validate the settingsop\nRegistered a class instance to act as the bridge/proxy to the IOptionsSnapshot and decrypt the values\nRegistered the resolved bridge class using its multiple exposed interfaces for finer grained use\n\nWith the last of these in place, our controllers and any other dependant classes can choose whether to get the settings as a whole via IAppSettingsResolved or one of the finer grained interfaces\n\nIAppSettings - the non-encrypted values\nISqlConnectionString - the decrypted SQL connection string\nIOracleConnectionString - the decrypted Oracle connection string\n\nTaking Things Further\nThat wraps up this series of posts for now, but you may want to take things further now that the settings class can have functionality injected into it.  Possibilities include\n\nUsing connection string builders to create connection strings using multiple properties (some encrypted, some not) from the bound object\nUsing the ICryptoFactory instead of ICryptoAlgorithm to use multiple algorithms for different properties\n\nDon't forget, the full source code including a WPF app to encrypt the settings dictionary can the downloaded from the Github repo at https://github.com/configureappio/ConfiguarationBridgeCrypto\nThanks for reading."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/configuration-bridging-part-3",
    "raw": "<p>This is the third in my series of posts looking at how to remove the need for controllers and razor pages to have knowledge of the options pattern in ASP.Net Core.</p>\n<p>If you have arrived here from a search engine or link, I would recommend reading the previous two posts which set the background before coming back to this post.</p>\n<p><a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-1\">Creating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 1)</a></p>\n<p><a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-2\">Creating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 2)</a></p>\n<p>In Part 2, I looked at how a lambda expression could be used to act as a bridge between the IOptionsSnapshot and T by creating an additional DI service:</p>\n<!--?# Gist 9730b44a163f68f6aeb1e99020ea560b /?-->\n<p>If all you are concerned with is ensuring that your controller or razor page does not need to refer to the options pattern, then this is a suitable solution.</p>\n<p>However, you may have the need to do something more exotic with the configuration settings such as perform some transformation such as decrypting some data or would like to perform&nbsp; a validation of the settings before invalid values get injected into a class.&nbsp;This is achievable using the lambda, but it becomes somewhat messy.</p>\n<p>Instead, the approach I will describe in this post will be to split the MyAppSettings class from the previous post out into three parts:</p>\n<ul>\n<li>An interface that defines the properties as read only values IMyAppSettings</li>\n<li>A settings reader class,&nbsp;MyAppSettingsReader,&nbsp; that implements the interface but also implements setters so that the values can be mapped into an instance by the Configure extension method</li>\n<li>A bridge class,&nbsp;MyAppSettingsBridge, that takes the IOptionsSnapshot in the constructor and then presents itself as the interface by using the Value method to get the value that has been read from configuration.</li>\n</ul>\n<p>The last part of the jigsaw is then to register the bridge as a transient service with the DI container. From therein, the controllers, razor pages and any other class that needs the settings will just need a parameter of type IMyAppSettings.</p>\n<h2 id=\"splitting-the-class-up\"><a name=\"splitting-the-class-up\" href=\"https://stevetalkscode.co.uk/#splitting-the-class-up\" class=\"pageLink\">Splitting the Class Up</a></h2>\n<p>In the previous post, the MyAppSettings class looked like this:</p>\n<!--?# Gist eb92f20067fce516f52cc45e5ae26e60 /?-->\n<p>We will now divide it up, starting with the interface:</p>\n<!--?# Gist b885cb106f096493bdbb4ea214a27005 /?-->\n<p>Note that the properties have been defined as read-only. Given that the configuration source(s) are read only as far as the code is concerned ( JSON files, XML files, environmental variables etc.), it is unlikely you will have code that would change the values.</p>\n<p>Then comes the two implementation of the interface:</p>\n<!--?# Gist 4eb5b0e2fa1f481a8a6030632db230ee /?-->\n<p>The MyAppSettingsReader is a simple DTO that the Configure extension method can map the configuration settings to - and therefore does need setters as well as the getters.</p>\n<p>This class does not strictly need to implement the interface as it is there simply to map settings from the configuration into an object.&nbsp;You could include other properties that may be components that will be combined as a value returned in a property exposed in the interface. E.g. say you have a property that is a database connection string.</p>\n<p>The second class is the bridge itself, MyAppSettingsBridge which <strong>must</strong> implement the interface as it will be used in the DI container. Note the constructor takes IOptionsSnapshot as the parameter and then acts as the go-between for the properties for the interface.</p>\n<p>In the example class above, the class is effectively doing the same as the lambda expression from the previous post by calling the Value property on the options object.</p>\n<p>However, by using a class, you can add more functionality. Taking the database connection example again, you could have individual properties for the server name, the database name, etc. in the reader class which can be then passed as parameters into a connection string builder inside the bridge class whose result is then exposed as a ConnectionString property in the interface.</p>\n<p>In my plans for another post, I will be showing how values could be encrypted in the configuration settings source and then decrypted by the bridge class.</p>\n<h2 id=\"wiring-it-all-up\"><a name=\"wiring-it-all-up\" href=\"https://stevetalkscode.co.uk/#wiring-it-all-up\" class=\"pageLink\">Wiring It All Up</a></h2>\n<p>Now we have the interface, reader and bridge, it is time to wire it all up.</p>\n<p>Firstly, we will set up the DI container to read the configuration into the reader. This will automatically create a DI service for IOptionsSnapshot. Next we register the bridge as a transient instance of the IMyAppSettings interface.</p>\n<!--?# Gist 6558f9776ae60aa3f1d888072a37cc12 /?-->\n<p>_Why register as Transient?_<em>It is up to you really. If registered as a transient, then every call will get the latest version of the configuration injected into it from IOptionsSnapshot.</em></p>\n<p><em>If registered as a singleton, the IOptionsSnapshot does not reload the configuration on each request.</em></p>\n<p><em>If you are not worried about having the ability to read the configuration without restarting the application, use the singleton and also change IOptionsSnapshot to IOptions so that the configuration monitor is not required.</em></p>\n<p>Now we are all set with the DI container, so we can change our controller to take IMyAppSettings as the parameter to the controller.</p>\n<!--?# Gist e87e31a1bacb6268b12227669f75cf1c /?-->\n<p>The code for this post and the previous post is available on GitHub at <a href=\"https://github.com/configureappio/configurebridgedemo1\" target=\"_blank\" class=\"externallink\">https://github.com/configureappio/configurebridgedemo1</a></p>\n<p>In the next post, I will look at injecting more functionality into the bridge to decrypt some settings before they get injected into the controller.</p>\n",
    "sanitized": "This is the third in my series of posts looking at how to remove the need for controllers and razor pages to have knowledge of the options pattern in ASP.Net Core.\nIf you have arrived here from a search engine or link, I would recommend reading the previous two posts which set the background before coming back to this post.\nCreating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 1)\nCreating a Bridge to your ASP.Net Core Configuration from your Controller or Razor Page (Part 2)\nIn Part 2, I looked at how a lambda expression could be used to act as a bridge between the IOptionsSnapshot and T by creating an additional DI service:\n\nIf all you are concerned with is ensuring that your controller or razor page does not need to refer to the options pattern, then this is a suitable solution.\nHowever, you may have the need to do something more exotic with the configuration settings such as perform some transformation such as decrypting some data or would like to perform  a validation of the settings before invalid values get injected into a class. This is achievable using the lambda, but it becomes somewhat messy.\nInstead, the approach I will describe in this post will be to split the MyAppSettings class from the previous post out into three parts:\n\nAn interface that defines the properties as read only values IMyAppSettings\nA settings reader class, MyAppSettingsReader,  that implements the interface but also implements setters so that the values can be mapped into an instance by the Configure extension method\nA bridge class, MyAppSettingsBridge, that takes the IOptionsSnapshot in the constructor and then presents itself as the interface by using the Value method to get the value that has been read from configuration.\n\nThe last part of the jigsaw is then to register the bridge as a transient service with the DI container. From therein, the controllers, razor pages and any other class that needs the settings will just need a parameter of type IMyAppSettings.\nSplitting the Class Up\nIn the previous post, the MyAppSettings class looked like this:\n\nWe will now divide it up, starting with the interface:\n\nNote that the properties have been defined as read-only. Given that the configuration source(s) are read only as far as the code is concerned ( JSON files, XML files, environmental variables etc.), it is unlikely you will have code that would change the values.\nThen comes the two implementation of the interface:\n\nThe MyAppSettingsReader is a simple DTO that the Configure extension method can map the configuration settings to - and therefore does need setters as well as the getters.\nThis class does not strictly need to implement the interface as it is there simply to map settings from the configuration into an object. You could include other properties that may be components that will be combined as a value returned in a property exposed in the interface. E.g. say you have a property that is a database connection string.\nThe second class is the bridge itself, MyAppSettingsBridge which must implement the interface as it will be used in the DI container. Note the constructor takes IOptionsSnapshot as the parameter and then acts as the go-between for the properties for the interface.\nIn the example class above, the class is effectively doing the same as the lambda expression from the previous post by calling the Value property on the options object.\nHowever, by using a class, you can add more functionality. Taking the database connection example again, you could have individual properties for the server name, the database name, etc. in the reader class which can be then passed as parameters into a connection string builder inside the bridge class whose result is then exposed as a ConnectionString property in the interface.\nIn my plans for another post, I will be showing how values could be encrypted in the configuration settings source and then decrypted by the bridge class.\nWiring It All Up\nNow we have the interface, reader and bridge, it is time to wire it all up.\nFirstly, we will set up the DI container to read the configuration into the reader. This will automatically create a DI service for IOptionsSnapshot. Next we register the bridge as a transient instance of the IMyAppSettings interface.\n\n_Why register as Transient?_It is up to you really. If registered as a transient, then every call will get the latest version of the configuration injected into it from IOptionsSnapshot.\nIf registered as a singleton, the IOptionsSnapshot does not reload the configuration on each request.\nIf you are not worried about having the ability to read the configuration without restarting the application, use the singleton and also change IOptionsSnapshot to IOptions so that the configuration monitor is not required.\nNow we are all set with the DI container, so we can change our controller to take IMyAppSettings as the parameter to the controller.\n\nThe code for this post and the previous post is available on GitHub at https://github.com/configureappio/configurebridgedemo1\nIn the next post, I will look at injecting more functionality into the bridge to decrypt some settings before they get injected into the controller."
  },
  {
    "itemId": "https://stevetalkscode.co.uk/configuration-bridging-part-2",
    "raw": "<p>In my <a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-1\">previous blog post</a>, I set the scene to give some background as to the relative merits of using the Options pattern vs binding a configuration object to a singleton in the DI container.</p>\n<p>To summarise</p>\n<ul>\n<li>IOptionsSnaphot allows the configuration to be changed without having to restart the application in order for new requests to make use of the changed values. However, the DI container will present the configuration as IOptions or&nbsp;IOptionsSnaphot which means that any assembly that makes using of the configuration will need to refer to the Microsoft.Extensions.Options Nuget package. Some feel that this is an overhead and is harder to test than a reference to just T (where T is the bound configuration object)</li>\n<li>Binding the configuration object T in the service setup and then storing it in a singleton allows method signatures to just use T as a parameter which is cleaner, but loses the ability to dynamically react to changes in the configuration (E.g. the appsettings.json being changed)</li>\n</ul>\n<p>For this post, I originally planned to launch into a full blown discussion of creating a bridging class between the controller and the configuration setting object to mask the use of IOptions.</p>\n<p>However, before doing that, I noticed a comment at the bottom of Rick Strahl's blog post from&nbsp;Todd Menier pointing out that a very simple bridge could be created by using an anonymous function, which seemed a good way to start to describe the basics of creating a bridge between the controller and the configuration before going into the more complex implementation of using abstractions and classes.</p>\n<p>Before getting into creating the bridge, a quick recap of why the bridge is required.</p>\n<h2 id=\"an-example-of-out-of-the-box-ioptions-implementation\"><a name=\"out-the-box-ioptions-example\" href=\"https://stevetalkscode.co.uk/#out-the-box-ioptions-example\" class=\"pageLink\">An Example of Out-of-the-Box IOptions implementation</a></h2>\n<p>Say you have a class called MyAppSettings that you want to bind configuration data to, you will have a class that looks like this:</p>\n<!--?# Gist eb92f20067fce516f52cc45e5ae26e60 /?-->\n<p>Which in turn you add to your appsettings.json as this:</p>\n<!--?# Gist a1967450acb923286654e4aa41f27688 /?-->\n<p>In order to wire up the two, you will have a Startup.cs class that looks like this:</p>\n<!--?# Gist 2e3239e5107bb2226d1537a4d192725e /?-->\n<p>The first important line is <strong>services.<a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.optionsservicecollectionextensions.addoptions?view=aspnetcore-2.0\" target=\"_blank\" class=\"externallink\">AddOptions()</a></strong> which is required for IOptions to work.</p>\n<p>The second is <strong>services.Configure(Configuration.GetSection(\"MyAppSettings\"))</strong> which takes the MyAppSettings section that has been read from the configuration sources (in our case appsettings.json, but this may have been overridden by other sources) and binds it to an instance of MyAppSettings.</p>\n<p>Looking at the <a href=\"https://github.com/aspnet/Options/blob/dev/src/Microsoft.Extensions.Options.ConfigurationExtensions/OptionsConfigurationServiceCollectionExtensions.cs\" target=\"_blank\" class=\"externallink\">source code for Configure(IConfiguration config)</a> shows that behind the scenes, two singletons are registered: ConfigurationChangeTokenSource and&nbsp;NamedConfigureFromConfigurationOptions.</p>\n<p>The first of these monitors the configuration for changes and is used when a parameter is defined as being of type IOptionsSnaphot. This allows changes to be sent to the controller without the need to restart the application.&nbsp;The second handles binding the object that holds the configuration values as the class instance.</p>\n<p>With the binding set up, in your controller, you can have a constructor that looks like this:</p>\n<!--?# Gist 324d635d52feb99dd3aa2013d025d294 /?-->\n<p>Note, that in order to receive the MyAppSettings instance, the constructor parameter must be either of type IOptions or IOptionsSnapshot. The difference being that the former only gets the settings as they were when the application started, but the latter reads the latest version of the settings even if they changed after the application has started.</p>\n<p>It is this extra bit of orchestration that some people don't like as it means that any unit tests must mock the IOptions interface and provide an implementation of the Value method to get to the MyAppSettings object that is actually of interest.</p>\n<p>Now, if it was purely just about MVC controllers and razor pages, I am personally not too hung up on this. However, it becomes a bit more complicated if the configuration settings are required for the constructor of a class in some other assembly as it then means that the other assembly needs to have a reference to the Microsoft.Extensions.Options Nuget package. This in turn could start a dependency sprawl across multiple assemblies.</p>\n<h2 id=\"intercepting-with-a-bridge\"><a name=\"intercepting-with-a-bridge\" href=\"https://stevetalkscode.co.uk/#intercepting-with-a-bridge\" class=\"pageLink\">Intercepting with a Bridge</a></h2>\n<p>To avoid this, a bridge is required that can accept IOptions or IOptionsSnapshot as a parameter, but presents itself as MyAppSettings (or an abstraction of it).</p>\n<p>In my next post, I intend to present a solution using a bridging class that can accept other parameters to do more than just abstract away from the options pattern, by adding functionality such as decryption.</p>\n<p>In the meantime, a way of simply getting the options pattern out of the way is to add another service to the DI container, this time a transient that uses an anonymous lambda function to get the value out of the options pattern and present that (as suggested by Todd).</p>\n<!--?# Gist 9730b44a163f68f6aeb1e99020ea560b /?-->\n<p>This then allows the controller (or any other class that needs access to the configuration settings) to just need the MyAppSettings class as the parameter:</p>\n<!--?# Gist 963568bb44b917d063c2d7132c752a20 /?-->\n<p>This lets the DI container deal with the bridging and keeps the clients ignorant of the options pattern.</p>\n<p>In the [next post in the series]/configuration-bridging-part-3), I look at moving from using the lambda to using a full bridging pattern</p>\n",
    "sanitized": "In my previous blog post, I set the scene to give some background as to the relative merits of using the Options pattern vs binding a configuration object to a singleton in the DI container.\nTo summarise\n\nIOptionsSnaphot allows the configuration to be changed without having to restart the application in order for new requests to make use of the changed values. However, the DI container will present the configuration as IOptions or IOptionsSnaphot which means that any assembly that makes using of the configuration will need to refer to the Microsoft.Extensions.Options Nuget package. Some feel that this is an overhead and is harder to test than a reference to just T (where T is the bound configuration object)\nBinding the configuration object T in the service setup and then storing it in a singleton allows method signatures to just use T as a parameter which is cleaner, but loses the ability to dynamically react to changes in the configuration (E.g. the appsettings.json being changed)\n\nFor this post, I originally planned to launch into a full blown discussion of creating a bridging class between the controller and the configuration setting object to mask the use of IOptions.\nHowever, before doing that, I noticed a comment at the bottom of Rick Strahl's blog post from Todd Menier pointing out that a very simple bridge could be created by using an anonymous function, which seemed a good way to start to describe the basics of creating a bridge between the controller and the configuration before going into the more complex implementation of using abstractions and classes.\nBefore getting into creating the bridge, a quick recap of why the bridge is required.\nAn Example of Out-of-the-Box IOptions implementation\nSay you have a class called MyAppSettings that you want to bind configuration data to, you will have a class that looks like this:\n\nWhich in turn you add to your appsettings.json as this:\n\nIn order to wire up the two, you will have a Startup.cs class that looks like this:\n\nThe first important line is services.AddOptions() which is required for IOptions to work.\nThe second is services.Configure(Configuration.GetSection(\"MyAppSettings\")) which takes the MyAppSettings section that has been read from the configuration sources (in our case appsettings.json, but this may have been overridden by other sources) and binds it to an instance of MyAppSettings.\nLooking at the source code for Configure(IConfiguration config) shows that behind the scenes, two singletons are registered: ConfigurationChangeTokenSource and NamedConfigureFromConfigurationOptions.\nThe first of these monitors the configuration for changes and is used when a parameter is defined as being of type IOptionsSnaphot. This allows changes to be sent to the controller without the need to restart the application. The second handles binding the object that holds the configuration values as the class instance.\nWith the binding set up, in your controller, you can have a constructor that looks like this:\n\nNote, that in order to receive the MyAppSettings instance, the constructor parameter must be either of type IOptions or IOptionsSnapshot. The difference being that the former only gets the settings as they were when the application started, but the latter reads the latest version of the settings even if they changed after the application has started.\nIt is this extra bit of orchestration that some people don't like as it means that any unit tests must mock the IOptions interface and provide an implementation of the Value method to get to the MyAppSettings object that is actually of interest.\nNow, if it was purely just about MVC controllers and razor pages, I am personally not too hung up on this. However, it becomes a bit more complicated if the configuration settings are required for the constructor of a class in some other assembly as it then means that the other assembly needs to have a reference to the Microsoft.Extensions.Options Nuget package. This in turn could start a dependency sprawl across multiple assemblies.\nIntercepting with a Bridge\nTo avoid this, a bridge is required that can accept IOptions or IOptionsSnapshot as a parameter, but presents itself as MyAppSettings (or an abstraction of it).\nIn my next post, I intend to present a solution using a bridging class that can accept other parameters to do more than just abstract away from the options pattern, by adding functionality such as decryption.\nIn the meantime, a way of simply getting the options pattern out of the way is to add another service to the DI container, this time a transient that uses an anonymous lambda function to get the value out of the options pattern and present that (as suggested by Todd).\n\nThis then allows the controller (or any other class that needs access to the configuration settings) to just need the MyAppSettings class as the parameter:\n\nThis lets the DI container deal with the bridging and keeps the clients ignorant of the options pattern.\nIn the [next post in the series]/configuration-bridging-part-3), I look at moving from using the lambda to using a full bridging pattern"
  },
  {
    "itemId": "https://stevetalkscode.co.uk/configuration-bridging-part-1",
    "raw": "<h2 id=\"background\"><a name=\"background\" class=\"pageLink\" href=\"https://stevetalkscode.co.uk/#background\">Background</a></h2>\n<p>This post is intended to set the stage for a later post (though may become a series) I have planned in which I look at using the Bridge design pattern to break the immediate dependency on <a href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/options\" target=\"_blank\" class=\"externallink\">the Options&nbsp;pattern</a> in a .Net Core application.</p>\n<p>The pattern is used to bind configuration settings to an object rather than pollute code with references to the configuration directly. For those not familiar with the pattern, I recommend following the link above.</p>\n<p>This topic became of interest to me after reading a couple of Rick Strahl's blog posts in which he discusses binding configuration settings to objects.</p>\n<ul>\n<li><a href=\"https://weblog.west-wind.com/posts/2016/may/23/strongly-typed-configuration-settings-in-aspnet-core\" target=\"_blank\" class=\"externallink\">Strongly Typed Configuration Settings in ASP.NET Core</a></li>\n<li><a href=\"https://weblog.west-wind.com/posts/2017/Dec/12/Easy-Configuration-Binding-in-ASPNET-Core-revisited?page=1\" target=\"_blank\" class=\"externallink\">Easy Configuration Binding in ASP.NET Core - revisited</a></li>\n</ul>\n<p>In these posts, Rick looks at approaches to binding configuration sections to objects, initially by binding directly to a class and storing that class as a singleton which can then be injected into the controller (or Razor page), and then later comparing that approach to using IOptions<t> or IOptionsSnapshot<t> as advocated by the .Net team.</t></t></p>\n<p>A comparison between the two is also looked at by Filip W in the post <a href=\"https://www.strathweb.com/2016/09/strongly-typed-configuration-in-asp-net-core-without-ioptionst/\" target=\"_blank\" class=\"externallink\">Strongly typed configuration in ASP.NET Core without IOptions</a>.</p>\n<p>In both these posts, the authors appreciate the benefits of IOptions allowing the configuration to be changed on-the-fly without having to restart the application (when using IOptionsSnapshot<t>) whereas, an application restart is necessary with a singleton, but are not keen on having to drag along the&nbsp;Microsoft.Extensions.Options package everywhere that the configuration object will be used and having to use IOptions in parameter definitions when what is really of interest is the object T. This is a theme that runs through several blog posts and StackOverflow questions.</t></p>\n<p>Rather than repeat the same discussions here, the links above are provided as background reading to give a foundation for the content I intend to provide in my next post where I will look at creating an intermediary (or bridge) so that projects that have in interest in the configuration object do not have to have a direct dependency on taking an IOptions<t> parameter, but instead take an abstraction representing the configuration.</t></p>\n<p>By bringing an intermediary into the mix, it will also allow for adding functionality such as early validation of the configuration before it is used and decryption of sensitive data.</p>\n<p>This will be the focus of the <a href=\"https://stevetalkscode.co.uk/configuration-bridging-part-2\">next post</a>.</p>\n",
    "sanitized": "Background\nThis post is intended to set the stage for a later post (though may become a series) I have planned in which I look at using the Bridge design pattern to break the immediate dependency on the Options pattern in a .Net Core application.\nThe pattern is used to bind configuration settings to an object rather than pollute code with references to the configuration directly. For those not familiar with the pattern, I recommend following the link above.\nThis topic became of interest to me after reading a couple of Rick Strahl's blog posts in which he discusses binding configuration settings to objects.\n\nStrongly Typed Configuration Settings in ASP.NET Core\nEasy Configuration Binding in ASP.NET Core - revisited\n\nIn these posts, Rick looks at approaches to binding configuration sections to objects, initially by binding directly to a class and storing that class as a singleton which can then be injected into the controller (or Razor page), and then later comparing that approach to using IOptions or IOptionsSnapshot as advocated by the .Net team.\nA comparison between the two is also looked at by Filip W in the post Strongly typed configuration in ASP.NET Core without IOptions.\nIn both these posts, the authors appreciate the benefits of IOptions allowing the configuration to be changed on-the-fly without having to restart the application (when using IOptionsSnapshot) whereas, an application restart is necessary with a singleton, but are not keen on having to drag along the Microsoft.Extensions.Options package everywhere that the configuration object will be used and having to use IOptions in parameter definitions when what is really of interest is the object T. This is a theme that runs through several blog posts and StackOverflow questions.\nRather than repeat the same discussions here, the links above are provided as background reading to give a foundation for the content I intend to provide in my next post where I will look at creating an intermediary (or bridge) so that projects that have in interest in the configuration object do not have to have a direct dependency on taking an IOptions parameter, but instead take an abstraction representing the configuration.\nBy bringing an intermediary into the mix, it will also allow for adding functionality such as early validation of the configuration before it is used and decryption of sensitive data.\nThis will be the focus of the next post."
  }
]
